{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yue-zhongqi/cartpole_colab/blob/main/cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauhjPSfX7pI"
      },
      "source": [
        "# Tutorial and Sample Code for Balancing a Pole on a Cart"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Puk2HpEr81jv"
      },
      "source": [
        "# Assignment 1 is done by:\n",
        "\n",
        "John\n",
        "<br>\n",
        "Ken Ho \n",
        "<br>\n",
        "Hisham "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UBiYOoesYMvr"
      },
      "source": [
        "## Installing dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbgnVwZmX5uW",
        "outputId": "495ebbb2-4d68-4d57-cd33-c185597c949c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.1.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[classic_control]\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rn3BLPfzj1z",
        "outputId": "e4eb3207-bb8a-44e5-c96b-c36f95ab3ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.22.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RwKbYeTgbaTA"
      },
      "source": [
        "## Importing dependencies and define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6KpgCLGYWmj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ehbqP9CXbmo7"
      },
      "source": [
        "## Tutorial: Loading CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go12dH4qbwBy"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZ9g3xrcAXE"
      },
      "source": [
        "We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytxvVmLdcRyw",
        "outputId": "95a42567-f957-4e38-c181-5169c394f95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXGWi_Ncfg-"
      },
      "source": [
        "The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyqHr9I5cdkX",
        "outputId": "97011cf7-2da2-4c6e-a92f-3352d3d167f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HFOdaU2Gdyg0"
      },
      "source": [
        "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMr6qAqxdOsm",
        "outputId": "b2f3d43d-ce98-418d-a607-8b002556b94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial observations: [ 0.04110717 -0.02655157 -0.02932773  0.04078307]\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "print(\"Initial observations:\", observation)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qnG2QdfbeZrI"
      },
      "source": [
        "For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmfMDvyYdWGk",
        "outputId": "1f291de8-47f6-4747-cab0-c1ac0a281e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observations after choosing action 0: [ 0.04057614 -0.22124097 -0.02851207  0.32407045]\n",
            "Reward for this step: 1.0\n",
            "Is this round done? False\n"
          ]
        }
      ],
      "source": [
        "observation, reward, done, info = env.step(0)\n",
        "print(\"New observations after choosing action 0:\", observation)\n",
        "print(\"Reward for this step:\", reward)\n",
        "print(\"Is this round done?\", done)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tj0zCh59fhBb"
      },
      "source": [
        "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVucQVRwf6Jm",
        "outputId": "5810deb6-39a6-403f-fb6d-c349fa3a73b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative reward for this round: 9.0\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    observation, reward, done, info = env.step(0)\n",
        "    cumulative_reward += reward\n",
        "print(\"Cumulative reward for this round:\", cumulative_reward)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIzK9SzhlWN"
      },
      "source": [
        "## Task 1: Development of an RL agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sDiCxLcXv6aI"
      },
      "source": [
        "### Task 1: Deep Q Network RL Agent ###\n",
        "\n",
        "(referenced from https://github.com/ChaithanyaVamshi/CartPole-DQN-Reinforcementlearning/blob/main/CartPole_DQN.ipynb)\n",
        "\n",
        "\n",
        "---\n",
        "Reference for linear annealing policy with epsilon greedy policy can be found at: https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py\n",
        "\n",
        "---\n",
        "Importing required libraries for agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfAEpky2ycH-",
        "outputId": "9dbeb5ac-65b1-41fc-8430-4b3ee3764697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.9/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (67.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.22.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->keras-rl2) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow->keras-rl2) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl2 \n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlKxPX1sxc43",
        "outputId": "e1a5045f-1011-48a6-accd-d04949ddcc1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------------\n",
            "absl-py                       1.4.0\n",
            "alabaster                     0.7.13\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.2\n",
            "appdirs                       1.4.4\n",
            "argon2-cffi                   21.3.0\n",
            "argon2-cffi-bindings          21.2.0\n",
            "arviz                         0.15.1\n",
            "astropy                       5.2.1\n",
            "astunparse                    1.6.3\n",
            "attrs                         22.2.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.12.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.11.2\n",
            "bleach                        6.0.0\n",
            "blis                          0.7.9\n",
            "bokeh                         2.4.3\n",
            "branca                        0.6.0\n",
            "CacheControl                  0.12.11\n",
            "cached-property               1.5.2\n",
            "cachetools                    5.3.0\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.12.7\n",
            "cffi                          1.15.1\n",
            "chardet                       3.0.4\n",
            "charset-normalizer            2.0.12\n",
            "chex                          0.1.6\n",
            "click                         8.1.3\n",
            "cloudpickle                   2.2.1\n",
            "cmake                         3.25.2\n",
            "cmdstanpy                     1.1.0\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.4\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.6.0.post1\n",
            "contourpy                     1.0.7\n",
            "convertdate                   2.4.0\n",
            "cryptography                  39.0.2\n",
            "cufflinks                     0.17.3\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.3.1\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.33\n",
            "dask                          2022.12.1\n",
            "datascience                   0.17.6\n",
            "db-dtypes                     1.0.5\n",
            "dbus-python                   1.2.16\n",
            "debugpy                       1.6.6\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "distributed                   2022.12.1\n",
            "dlib                          19.24.0\n",
            "dm-tree                       0.1.8\n",
            "docutils                      0.16\n",
            "dopamine-rl                   4.0.6\n",
            "earthengine-api               0.1.346\n",
            "easydict                      1.10\n",
            "ecos                          2.0.12\n",
            "editdistance                  0.5.3\n",
            "en-core-web-sm                3.5.0\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.4\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         1.1.1\n",
            "etuples                       0.3.8\n",
            "exceptiongroup                1.1.1\n",
            "ez-setup                      0.9\n",
            "fastai                        2.7.11\n",
            "fastcore                      1.5.28\n",
            "fastdownload                  0.0.7\n",
            "fastjsonschema                2.16.3\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "filelock                      3.10.1\n",
            "firebase-admin                5.3.0\n",
            "Flask                         2.2.3\n",
            "flatbuffers                   23.3.3\n",
            "flax                          0.6.7\n",
            "folium                        0.14.0\n",
            "fonttools                     4.39.2\n",
            "frozendict                    2.3.6\n",
            "fsspec                        2023.3.0\n",
            "future                        0.18.3\n",
            "gast                          0.4.0\n",
            "GDAL                          3.3.2\n",
            "gdown                         4.6.4\n",
            "gensim                        4.3.1\n",
            "geographiclib                 2.0\n",
            "geopy                         2.3.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               2.11.0\n",
            "google-api-python-client      2.70.0\n",
            "google-auth                   2.16.2\n",
            "google-auth-httplib2          0.1.0\n",
            "google-auth-oauthlib          0.4.6\n",
            "google-cloud-bigquery         3.4.2\n",
            "google-cloud-bigquery-storage 2.19.0\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.11.1\n",
            "google-cloud-firestore        2.7.3\n",
            "google-cloud-language         2.6.1\n",
            "google-cloud-storage          2.7.0\n",
            "google-cloud-translate        3.8.4\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.4.1\n",
            "googleapis-common-protos      1.59.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.20.1\n",
            "greenlet                      2.0.2\n",
            "grpcio                        1.51.3\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h5netcdf                      1.1.0\n",
            "h5py                          3.8.0\n",
            "HeapDict                      1.0.1\n",
            "hijri-converter               2.2.4\n",
            "holidays                      0.21.13\n",
            "holoviews                     1.15.4\n",
            "html5lib                      1.1\n",
            "htmlmin                       0.1.12\n",
            "httpimport                    1.3.0\n",
            "httplib2                      0.21.0\n",
            "humanize                      4.6.0\n",
            "hyperopt                      0.2.7\n",
            "idna                          3.4\n",
            "ImageHash                     4.3.1\n",
            "imageio                       2.25.1\n",
            "imageio-ffmpeg                0.4.8\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.10.1\n",
            "imgaug                        0.4.0\n",
            "importlib-metadata            6.1.0\n",
            "importlib-resources           5.12.0\n",
            "imutils                       0.5.4\n",
            "inflect                       6.0.2\n",
            "iniconfig                     2.0.0\n",
            "intel-openmp                  2023.0.0\n",
            "ipykernel                     5.3.4\n",
            "ipython                       7.9.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.4.1\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  2.1.2\n",
            "jax                           0.4.6\n",
            "jaxlib                        0.4.6+cuda11.cudnn86\n",
            "jieba                         0.42.1\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.1.1\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter_core                  5.3.0\n",
            "jupyterlab-pygments           0.2.2\n",
            "jupyterlab-widgets            3.0.6\n",
            "kaggle                        1.5.13\n",
            "keras                         2.12.0\n",
            "keras-rl2                     1.0.5\n",
            "keras-vis                     0.4.1\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "lazy_loader                   0.2\n",
            "libclang                      15.0.6.1\n",
            "librosa                       0.10.0.post2\n",
            "lightgbm                      3.3.5\n",
            "llvmlite                      0.39.1\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.2\n",
            "Markdown                      3.4.2\n",
            "markdown-it-py                2.2.0\n",
            "MarkupSafe                    2.1.2\n",
            "matplotlib                    3.7.1\n",
            "matplotlib-venn               0.11.9\n",
            "mdurl                         0.1.2\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.2\n",
            "mistune                       0.8.4\n",
            "mizani                        0.8.1\n",
            "mkl                           2019.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.1.0\n",
            "moviepy                       1.0.3\n",
            "mpmath                        1.3.0\n",
            "msgpack                       1.0.5\n",
            "multimethod                   1.9.1\n",
            "multipledispatch              0.6.0\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       5.5.0\n",
            "natsort                       5.5.0\n",
            "nbclient                      0.7.2\n",
            "nbconvert                     6.5.4\n",
            "nbformat                      5.8.0\n",
            "nest-asyncio                  1.5.6\n",
            "networkx                      3.0\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.8.1\n",
            "notebook                      6.3.0\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.22.4\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "opencv-contrib-python         4.7.0.72\n",
            "opencv-python                 4.7.0.72\n",
            "opencv-python-headless        4.7.0.72\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "optax                         0.1.4\n",
            "orbax                         0.1.6\n",
            "osqp                          0.6.2.post0\n",
            "packaging                     23.0\n",
            "palettable                    3.3.0\n",
            "pandas                        1.4.4\n",
            "pandas-datareader             0.10.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandas-profiling              3.2.0\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.14.4\n",
            "param                         1.13.0\n",
            "parso                         0.8.3\n",
            "partd                         1.3.0\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.1\n",
            "patsy                         0.5.3\n",
            "pep517                        0.13.0\n",
            "pexpect                       4.8.0\n",
            "phik                          0.12.3\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        8.4.0\n",
            "pip                           22.0.4\n",
            "pip-tools                     6.6.2\n",
            "platformdirs                  3.1.1\n",
            "plotly                        5.13.1\n",
            "plotnine                      0.10.1\n",
            "pluggy                        1.0.0\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   0.7.2\n",
            "proglog                       0.1.10\n",
            "progressbar2                  3.38.0\n",
            "prometheus-client             0.16.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                2.0.10\n",
            "prophet                       1.1.2\n",
            "proto-plus                    1.22.2\n",
            "protobuf                      4.22.1\n",
            "psutil                        5.9.4\n",
            "psycopg2                      2.9.5\n",
            "ptyprocess                    0.7.0\n",
            "py4j                          0.10.9.7\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.4.8\n",
            "pyasn1-modules                0.2.8\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.5.0\n",
            "pydantic                      1.10.7\n",
            "pydata-google-auth            1.7.0\n",
            "pydot                         1.4.2\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyerfa                        2.0.0.2\n",
            "pygame                        2.1.0\n",
            "Pygments                      2.14.0\n",
            "PyGObject                     3.36.0\n",
            "pymc                          5.1.2\n",
            "PyMeeus                       0.5.12\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyrsistent                    0.19.3\n",
            "PySocks                       1.7.1\n",
            "pytensor                      2.10.1\n",
            "pytest                        7.2.2\n",
            "python-apt                    0.0.0\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                8.0.1\n",
            "python-utils                  3.5.2\n",
            "pytz                          2022.7.1\n",
            "pytz-deprecation-shim         0.1.0.post0\n",
            "PyVirtualDisplay              3.0\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.5.post3\n",
            "qudida                        0.0.4\n",
            "regex                         2022.10.31\n",
            "requests                      2.27.1\n",
            "requests-oauthlib             1.3.1\n",
            "requests-unixsocket           0.2.0\n",
            "rich                          13.3.2\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.19.3\n",
            "scikit-learn                  1.2.2\n",
            "scipy                         1.10.1\n",
            "screen-resolution-extra       0.0.0\n",
            "scs                           3.2.2\n",
            "seaborn                       0.12.2\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    67.6.0\n",
            "shapely                       2.0.1\n",
            "six                           1.16.0\n",
            "sklearn-pandas                2.2.0\n",
            "smart-open                    6.3.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.12.1\n",
            "soupsieve                     2.4\n",
            "soxr                          0.3.4\n",
            "spacy                         3.5.1\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.4\n",
            "Sphinx                        3.5.4\n",
            "sphinxcontrib-applehelp       1.0.4\n",
            "sphinxcontrib-devhelp         1.0.2\n",
            "sphinxcontrib-htmlhelp        2.0.1\n",
            "sphinxcontrib-jsmath          1.0.1\n",
            "sphinxcontrib-qthelp          1.0.3\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "SQLAlchemy                    1.4.47\n",
            "sqlparse                      0.4.3\n",
            "srsly                         2.4.6\n",
            "statsmodels                   0.13.5\n",
            "sympy                         1.11.1\n",
            "tables                        3.7.0\n",
            "tabulate                      0.8.10\n",
            "tangled-up-in-unicode         0.2.0\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.2.2\n",
            "tensorboard                   2.12.0\n",
            "tensorboard-data-server       0.7.0\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.12.0\n",
            "tensorflow-datasets           4.8.3\n",
            "tensorflow-estimator          2.12.0\n",
            "tensorflow-gcs-config         2.11.0\n",
            "tensorflow-hub                0.13.0\n",
            "tensorflow-io-gcs-filesystem  0.31.0\n",
            "tensorflow-metadata           1.12.0\n",
            "tensorflow-probability        0.19.0\n",
            "tensorstore                   0.1.33\n",
            "termcolor                     2.2.0\n",
            "terminado                     0.17.1\n",
            "text-unidecode                1.3\n",
            "textblob                      0.17.1\n",
            "tf-slim                       1.1.0\n",
            "thinc                         8.1.9\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2023.3.21\n",
            "tinycss2                      1.2.1\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         1.13.1+cu116\n",
            "torchaudio                    0.13.1+cu116\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.14.1\n",
            "torchvision                   0.14.1+cu116\n",
            "tornado                       6.2\n",
            "tqdm                          4.65.0\n",
            "traitlets                     5.7.1\n",
            "tweepy                        4.13.0\n",
            "typer                         0.7.0\n",
            "typing_extensions             4.5.0\n",
            "tzdata                        2022.7\n",
            "tzlocal                       4.3\n",
            "uritemplate                   4.1.1\n",
            "urllib3                       1.26.15\n",
            "vega-datasets                 0.9.0\n",
            "visions                       0.7.4\n",
            "wasabi                        1.1.1\n",
            "wcwidth                       0.2.6\n",
            "webencodings                  0.5.1\n",
            "Werkzeug                      2.2.3\n",
            "wheel                         0.40.0\n",
            "widgetsnbextension            3.6.3\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        2022.12.0\n",
            "xarray-einstats               0.5.1\n",
            "xgboost                       1.7.4\n",
            "xkit                          0.0.0\n",
            "xlrd                          2.0.1\n",
            "yellowbrick                   1.5\n",
            "yfinance                      0.2.13\n",
            "zict                          2.2.0\n",
            "zipp                          3.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip list #list of dependencies used, and their corresponding versions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1EEjRlHA3a1"
      },
      "source": [
        "\n",
        "In this case, Greedy Policy w/ epsilon is used. As covered in Monte Carlo slides, this policy starts by favouring exploration (random choice) with a large epsilon value, then increasingly favours a greedy policy by decreasing epsilon, as Q-values are refined.\n",
        "\n",
        "We increased nb_steps_warmup for improved performance. Warmup steps are recorded as an \"experience replay\" for training the agent off-policy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BGd9fDa9jX6",
        "outputId": "fd2e5dd4-bcdb-4343-c853-c96b23b95177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                120       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                600       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 100000 steps ...\n",
            "    11/100000: episode: 1, duration: 0.129s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: --, mae: --, accuracy: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    34/100000: episode: 2, duration: 0.690s, episode steps:  23, steps per second:  33, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.632392, mae: 0.639947, accuracy: 0.489583, mean_q: 0.053453, mean_eps: 0.997120\n",
            "    46/100000: episode: 3, duration: 0.106s, episode steps:  12, steps per second: 113, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.577425, mae: 0.604982, accuracy: 0.489583, mean_q: 0.119692, mean_eps: 0.996445\n",
            "    75/100000: episode: 4, duration: 0.276s, episode steps:  29, steps per second: 105, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 0.395060, mae: 0.553515, accuracy: 0.529095, mean_q: 0.330072, mean_eps: 0.994600\n",
            "    90/100000: episode: 5, duration: 0.132s, episode steps:  15, steps per second: 113, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.236768, mae: 0.573459, accuracy: 0.579167, mean_q: 0.567335, mean_eps: 0.992620\n",
            "   111/100000: episode: 6, duration: 0.196s, episode steps:  21, steps per second: 107, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.126857, mae: 0.618066, accuracy: 0.558036, mean_q: 0.880502, mean_eps: 0.991000\n",
            "   134/100000: episode: 7, duration: 0.218s, episode steps:  23, steps per second: 105, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.060018, mae: 0.698133, accuracy: 0.554348, mean_q: 1.227832, mean_eps: 0.989020\n",
            "   162/100000: episode: 8, duration: 0.422s, episode steps:  28, steps per second:  66, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.033090, mae: 0.777644, accuracy: 0.485491, mean_q: 1.469316, mean_eps: 0.986725\n",
            "   178/100000: episode: 9, duration: 0.291s, episode steps:  16, steps per second:  55, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.031157, mae: 0.851115, accuracy: 0.451172, mean_q: 1.606669, mean_eps: 0.984745\n",
            "   191/100000: episode: 10, duration: 0.260s, episode steps:  13, steps per second:  50, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.029929, mae: 0.885236, accuracy: 0.509615, mean_q: 1.697131, mean_eps: 0.983440\n",
            "   204/100000: episode: 11, duration: 0.236s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.026651, mae: 0.935186, accuracy: 0.497596, mean_q: 1.828476, mean_eps: 0.982270\n",
            "   228/100000: episode: 12, duration: 0.391s, episode steps:  24, steps per second:  61, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.047334, mae: 1.023775, accuracy: 0.510417, mean_q: 1.962462, mean_eps: 0.980605\n",
            "   250/100000: episode: 13, duration: 0.363s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.043850, mae: 1.105613, accuracy: 0.504261, mean_q: 2.166266, mean_eps: 0.978535\n",
            "   269/100000: episode: 14, duration: 0.380s, episode steps:  19, steps per second:  50, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.047683, mae: 1.183056, accuracy: 0.503289, mean_q: 2.322333, mean_eps: 0.976690\n",
            "   281/100000: episode: 15, duration: 0.230s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.054512, mae: 1.238450, accuracy: 0.531250, mean_q: 2.426515, mean_eps: 0.975295\n",
            "   314/100000: episode: 16, duration: 0.644s, episode steps:  33, steps per second:  51, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.081913, mae: 1.340745, accuracy: 0.542614, mean_q: 2.563455, mean_eps: 0.973270\n",
            "   328/100000: episode: 17, duration: 0.252s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.069345, mae: 1.419575, accuracy: 0.540179, mean_q: 2.802996, mean_eps: 0.971155\n",
            "   349/100000: episode: 18, duration: 0.299s, episode steps:  21, steps per second:  70, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.115903, mae: 1.511953, accuracy: 0.511905, mean_q: 2.912684, mean_eps: 0.969580\n",
            "   364/100000: episode: 19, duration: 0.241s, episode steps:  15, steps per second:  62, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.118020, mae: 1.577433, accuracy: 0.508333, mean_q: 3.046227, mean_eps: 0.967960\n",
            "   377/100000: episode: 20, duration: 0.185s, episode steps:  13, steps per second:  70, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.144021, mae: 1.637290, accuracy: 0.512019, mean_q: 3.105838, mean_eps: 0.966700\n",
            "   398/100000: episode: 21, duration: 0.282s, episode steps:  21, steps per second:  75, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.135199, mae: 1.703178, accuracy: 0.553571, mean_q: 3.256847, mean_eps: 0.965170\n",
            "   421/100000: episode: 22, duration: 0.352s, episode steps:  23, steps per second:  65, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.162652, mae: 1.803175, accuracy: 0.509511, mean_q: 3.441542, mean_eps: 0.963190\n",
            "   433/100000: episode: 23, duration: 0.173s, episode steps:  12, steps per second:  69, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.198119, mae: 1.875366, accuracy: 0.497396, mean_q: 3.504011, mean_eps: 0.961615\n",
            "   446/100000: episode: 24, duration: 0.192s, episode steps:  13, steps per second:  68, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.145556, mae: 1.933557, accuracy: 0.500000, mean_q: 3.713778, mean_eps: 0.960490\n",
            "   461/100000: episode: 25, duration: 0.221s, episode steps:  15, steps per second:  68, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.171991, mae: 1.975393, accuracy: 0.495833, mean_q: 3.767298, mean_eps: 0.959230\n",
            "   490/100000: episode: 26, duration: 0.452s, episode steps:  29, steps per second:  64, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.192018, mae: 2.061660, accuracy: 0.507543, mean_q: 3.919323, mean_eps: 0.957250\n",
            "   506/100000: episode: 27, duration: 0.285s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.176199, mae: 2.148455, accuracy: 0.501953, mean_q: 4.126536, mean_eps: 0.955225\n",
            "   521/100000: episode: 28, duration: 0.267s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.190976, mae: 2.213559, accuracy: 0.520833, mean_q: 4.270103, mean_eps: 0.953830\n",
            "   532/100000: episode: 29, duration: 0.155s, episode steps:  11, steps per second:  71, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.321822, mae: 2.294027, accuracy: 0.517045, mean_q: 4.342032, mean_eps: 0.952660\n",
            "   561/100000: episode: 30, duration: 0.410s, episode steps:  29, steps per second:  71, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.293803, mae: 2.372598, accuracy: 0.511853, mean_q: 4.448183, mean_eps: 0.950860\n",
            "   578/100000: episode: 31, duration: 0.460s, episode steps:  17, steps per second:  37, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.296506, mae: 2.470714, accuracy: 0.516544, mean_q: 4.682299, mean_eps: 0.948790\n",
            "   595/100000: episode: 32, duration: 0.288s, episode steps:  17, steps per second:  59, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.261724, mae: 2.519770, accuracy: 0.474265, mean_q: 4.754909, mean_eps: 0.947260\n",
            "   616/100000: episode: 33, duration: 0.341s, episode steps:  21, steps per second:  62, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.296283, mae: 2.616754, accuracy: 0.470238, mean_q: 4.997855, mean_eps: 0.945550\n",
            "   630/100000: episode: 34, duration: 0.235s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.307637, mae: 2.661195, accuracy: 0.488839, mean_q: 5.011138, mean_eps: 0.943975\n",
            "   638/100000: episode: 35, duration: 0.237s, episode steps:   8, steps per second:  34, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.388135, mae: 2.731921, accuracy: 0.527344, mean_q: 5.180902, mean_eps: 0.942985\n",
            "   660/100000: episode: 36, duration: 0.885s, episode steps:  22, steps per second:  25, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.365084, mae: 2.771361, accuracy: 0.500000, mean_q: 5.247562, mean_eps: 0.941635\n",
            "   677/100000: episode: 37, duration: 0.671s, episode steps:  17, steps per second:  25, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.382576, mae: 2.830496, accuracy: 0.540441, mean_q: 5.377298, mean_eps: 0.939880\n",
            "   688/100000: episode: 38, duration: 0.554s, episode steps:  11, steps per second:  20, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.402985, mae: 2.905864, accuracy: 0.463068, mean_q: 5.470461, mean_eps: 0.938620\n",
            "   699/100000: episode: 39, duration: 0.237s, episode steps:  11, steps per second:  46, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.459352, mae: 2.933329, accuracy: 0.480114, mean_q: 5.408773, mean_eps: 0.937630\n",
            "   749/100000: episode: 40, duration: 1.431s, episode steps:  50, steps per second:  35, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.359030, mae: 3.060204, accuracy: 0.491250, mean_q: 5.818853, mean_eps: 0.934885\n",
            "   762/100000: episode: 41, duration: 0.657s, episode steps:  13, steps per second:  20, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.433031, mae: 3.157365, accuracy: 0.490385, mean_q: 5.969531, mean_eps: 0.932050\n",
            "   790/100000: episode: 42, duration: 0.961s, episode steps:  28, steps per second:  29, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.361746, mae: 3.265678, accuracy: 0.479911, mean_q: 6.293597, mean_eps: 0.930205\n",
            "   827/100000: episode: 43, duration: 1.213s, episode steps:  37, steps per second:  30, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.453672, mae: 3.378582, accuracy: 0.492399, mean_q: 6.434636, mean_eps: 0.927280\n",
            "   843/100000: episode: 44, duration: 0.575s, episode steps:  16, steps per second:  28, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.469581, mae: 3.510532, accuracy: 0.488281, mean_q: 6.746552, mean_eps: 0.924895\n",
            "   866/100000: episode: 45, duration: 0.835s, episode steps:  23, steps per second:  28, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.607993, mae: 3.565403, accuracy: 0.489130, mean_q: 6.754396, mean_eps: 0.923140\n",
            "   883/100000: episode: 46, duration: 0.701s, episode steps:  17, steps per second:  24, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.407832, mae: 3.656837, accuracy: 0.461397, mean_q: 7.035956, mean_eps: 0.921340\n",
            "   904/100000: episode: 47, duration: 0.620s, episode steps:  21, steps per second:  34, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.562245, mae: 3.715733, accuracy: 0.519345, mean_q: 7.098552, mean_eps: 0.919630\n",
            "   914/100000: episode: 48, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.560185, mae: 3.783015, accuracy: 0.506250, mean_q: 7.262115, mean_eps: 0.918235\n",
            "   931/100000: episode: 49, duration: 0.292s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.539393, mae: 3.831488, accuracy: 0.525735, mean_q: 7.305296, mean_eps: 0.917020\n",
            "   955/100000: episode: 50, duration: 0.488s, episode steps:  24, steps per second:  49, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.599121, mae: 3.928352, accuracy: 0.490885, mean_q: 7.476584, mean_eps: 0.915175\n",
            "  1003/100000: episode: 51, duration: 0.893s, episode steps:  48, steps per second:  54, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.630703, mae: 4.044752, accuracy: 0.475911, mean_q: 7.786994, mean_eps: 0.911935\n",
            "  1028/100000: episode: 52, duration: 0.414s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.574744, mae: 4.158582, accuracy: 0.498750, mean_q: 8.088826, mean_eps: 0.908650\n",
            "  1060/100000: episode: 53, duration: 0.702s, episode steps:  32, steps per second:  46, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.623991, mae: 4.275217, accuracy: 0.500977, mean_q: 8.320134, mean_eps: 0.906085\n",
            "  1075/100000: episode: 54, duration: 0.446s, episode steps:  15, steps per second:  34, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.759703, mae: 4.394727, accuracy: 0.450000, mean_q: 8.497680, mean_eps: 0.903970\n",
            "  1101/100000: episode: 55, duration: 0.725s, episode steps:  26, steps per second:  36, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.734199, mae: 4.430100, accuracy: 0.467548, mean_q: 8.557883, mean_eps: 0.902125\n",
            "  1115/100000: episode: 56, duration: 0.202s, episode steps:  14, steps per second:  69, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.657968, mae: 4.551660, accuracy: 0.500000, mean_q: 8.919146, mean_eps: 0.900325\n",
            "  1138/100000: episode: 57, duration: 0.375s, episode steps:  23, steps per second:  61, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.612721, mae: 4.640700, accuracy: 0.470109, mean_q: 9.033399, mean_eps: 0.898660\n",
            "  1197/100000: episode: 58, duration: 0.912s, episode steps:  59, steps per second:  65, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.677926, mae: 4.800782, accuracy: 0.477225, mean_q: 9.388555, mean_eps: 0.894970\n",
            "  1222/100000: episode: 59, duration: 0.417s, episode steps:  25, steps per second:  60, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.771703, mae: 4.963353, accuracy: 0.473750, mean_q: 9.649935, mean_eps: 0.891190\n",
            "  1243/100000: episode: 60, duration: 0.688s, episode steps:  21, steps per second:  31, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.998097, mae: 5.030386, accuracy: 0.467262, mean_q: 9.705533, mean_eps: 0.889120\n",
            "  1260/100000: episode: 61, duration: 0.296s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.747821, mae: 5.072683, accuracy: 0.465074, mean_q: 9.832651, mean_eps: 0.887410\n",
            "  1338/100000: episode: 62, duration: 0.701s, episode steps:  78, steps per second: 111, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.814260, mae: 5.249320, accuracy: 0.482372, mean_q: 10.208045, mean_eps: 0.883135\n",
            "  1347/100000: episode: 63, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.909156, mae: 5.397468, accuracy: 0.503472, mean_q: 10.558210, mean_eps: 0.879220\n",
            "  1392/100000: episode: 64, duration: 0.399s, episode steps:  45, steps per second: 113, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.619951, mae: 5.516845, accuracy: 0.507639, mean_q: 10.879574, mean_eps: 0.876790\n",
            "  1450/100000: episode: 65, duration: 0.529s, episode steps:  58, steps per second: 110, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 0.701641, mae: 5.741610, accuracy: 0.500000, mean_q: 11.393275, mean_eps: 0.872155\n",
            "  1473/100000: episode: 66, duration: 0.205s, episode steps:  23, steps per second: 112, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.690411, mae: 5.900563, accuracy: 0.485054, mean_q: 11.773311, mean_eps: 0.868510\n",
            "  1485/100000: episode: 67, duration: 0.107s, episode steps:  12, steps per second: 112, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.967267, mae: 6.037366, accuracy: 0.453125, mean_q: 11.943738, mean_eps: 0.866935\n",
            "  1508/100000: episode: 68, duration: 0.227s, episode steps:  23, steps per second: 102, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.658087, mae: 6.122341, accuracy: 0.471467, mean_q: 12.135863, mean_eps: 0.865360\n",
            "  1589/100000: episode: 69, duration: 0.814s, episode steps:  81, steps per second:  99, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1.003325, mae: 6.299971, accuracy: 0.495756, mean_q: 12.497392, mean_eps: 0.860680\n",
            "  1700/100000: episode: 70, duration: 1.371s, episode steps: 111, steps per second:  81, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.828538, mae: 6.718522, accuracy: 0.498029, mean_q: 13.378279, mean_eps: 0.852040\n",
            "  1727/100000: episode: 71, duration: 0.352s, episode steps:  27, steps per second:  77, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.802300, mae: 7.045254, accuracy: 0.517361, mean_q: 14.075542, mean_eps: 0.845830\n",
            "  1772/100000: episode: 72, duration: 0.580s, episode steps:  45, steps per second:  78, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.315950, mae: 7.154758, accuracy: 0.523611, mean_q: 14.193324, mean_eps: 0.842590\n",
            "  1795/100000: episode: 73, duration: 0.306s, episode steps:  23, steps per second:  75, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.517135, mae: 7.387719, accuracy: 0.505435, mean_q: 14.647678, mean_eps: 0.839530\n",
            "  1892/100000: episode: 74, duration: 1.306s, episode steps:  97, steps per second:  74, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1.002551, mae: 7.592382, accuracy: 0.532216, mean_q: 15.184000, mean_eps: 0.834130\n",
            "  1931/100000: episode: 75, duration: 0.506s, episode steps:  39, steps per second:  77, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.391691, mae: 7.835065, accuracy: 0.552885, mean_q: 15.609000, mean_eps: 0.828010\n",
            "  1959/100000: episode: 76, duration: 0.385s, episode steps:  28, steps per second:  73, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 1.315921, mae: 7.963571, accuracy: 0.545759, mean_q: 15.838910, mean_eps: 0.824995\n",
            "  1969/100000: episode: 77, duration: 0.139s, episode steps:  10, steps per second:  72, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.528239, mae: 8.154960, accuracy: 0.515625, mean_q: 16.366685, mean_eps: 0.823285\n",
            "  1989/100000: episode: 78, duration: 0.279s, episode steps:  20, steps per second:  72, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.800872, mae: 8.003814, accuracy: 0.539062, mean_q: 15.906587, mean_eps: 0.821935\n",
            "  2004/100000: episode: 79, duration: 0.224s, episode steps:  15, steps per second:  67, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.499036, mae: 8.077905, accuracy: 0.527083, mean_q: 16.067661, mean_eps: 0.820360\n",
            "  2018/100000: episode: 80, duration: 0.205s, episode steps:  14, steps per second:  68, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.908866, mae: 8.046826, accuracy: 0.555804, mean_q: 15.835138, mean_eps: 0.819055\n",
            "  2052/100000: episode: 81, duration: 0.472s, episode steps:  34, steps per second:  72, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.177334, mae: 8.248631, accuracy: 0.520221, mean_q: 16.496347, mean_eps: 0.816895\n",
            "  2091/100000: episode: 82, duration: 0.544s, episode steps:  39, steps per second:  72, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 1.396965, mae: 8.289464, accuracy: 0.541667, mean_q: 16.589576, mean_eps: 0.813610\n",
            "  2114/100000: episode: 83, duration: 0.329s, episode steps:  23, steps per second:  70, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 1.329353, mae: 8.481922, accuracy: 0.539402, mean_q: 17.004966, mean_eps: 0.810820\n",
            "  2142/100000: episode: 84, duration: 0.420s, episode steps:  28, steps per second:  67, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 1.539806, mae: 8.604895, accuracy: 0.552455, mean_q: 17.273126, mean_eps: 0.808525\n",
            "  2160/100000: episode: 85, duration: 0.268s, episode steps:  18, steps per second:  67, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.425117, mae: 8.675052, accuracy: 0.498264, mean_q: 17.283344, mean_eps: 0.806455\n",
            "  2222/100000: episode: 86, duration: 0.561s, episode steps:  62, steps per second: 111, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.169258, mae: 8.867047, accuracy: 0.532258, mean_q: 17.767678, mean_eps: 0.802855\n",
            "  2265/100000: episode: 87, duration: 0.404s, episode steps:  43, steps per second: 106, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.271835, mae: 9.104873, accuracy: 0.545058, mean_q: 18.267054, mean_eps: 0.798130\n",
            "  2280/100000: episode: 88, duration: 0.138s, episode steps:  15, steps per second: 109, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.545682, mae: 9.098547, accuracy: 0.510417, mean_q: 18.199356, mean_eps: 0.795520\n",
            "  2300/100000: episode: 89, duration: 0.198s, episode steps:  20, steps per second: 101, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1.277602, mae: 9.007786, accuracy: 0.543750, mean_q: 18.204755, mean_eps: 0.793945\n",
            "  2364/100000: episode: 90, duration: 0.604s, episode steps:  64, steps per second: 106, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 1.199718, mae: 9.296820, accuracy: 0.535156, mean_q: 18.703934, mean_eps: 0.790165\n",
            "  2401/100000: episode: 91, duration: 0.320s, episode steps:  37, steps per second: 116, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 1.273326, mae: 9.435326, accuracy: 0.534628, mean_q: 19.013564, mean_eps: 0.785620\n",
            "  2446/100000: episode: 92, duration: 0.413s, episode steps:  45, steps per second: 109, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.578 [0.000, 1.000],  loss: 1.241925, mae: 9.732142, accuracy: 0.556944, mean_q: 19.598107, mean_eps: 0.781930\n",
            "  2484/100000: episode: 93, duration: 0.333s, episode steps:  38, steps per second: 114, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.315835, mae: 9.775494, accuracy: 0.560033, mean_q: 19.667324, mean_eps: 0.778195\n",
            "  2500/100000: episode: 94, duration: 0.140s, episode steps:  16, steps per second: 114, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 1.189174, mae: 9.911489, accuracy: 0.562500, mean_q: 19.951647, mean_eps: 0.775765\n",
            "  2513/100000: episode: 95, duration: 0.130s, episode steps:  13, steps per second: 100, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.717664, mae: 9.732049, accuracy: 0.545673, mean_q: 19.529582, mean_eps: 0.774460\n",
            "  2578/100000: episode: 96, duration: 0.576s, episode steps:  65, steps per second: 113, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.197806, mae: 10.019291, accuracy: 0.522115, mean_q: 20.253300, mean_eps: 0.770950\n",
            "  2676/100000: episode: 97, duration: 0.878s, episode steps:  98, steps per second: 112, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.002687, mae: 10.357165, accuracy: 0.550064, mean_q: 21.083556, mean_eps: 0.763615\n",
            "  2768/100000: episode: 98, duration: 0.863s, episode steps:  92, steps per second: 107, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.405560, mae: 10.807242, accuracy: 0.553329, mean_q: 21.958082, mean_eps: 0.755065\n",
            "  2795/100000: episode: 99, duration: 0.242s, episode steps:  27, steps per second: 112, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.975082, mae: 11.149778, accuracy: 0.540509, mean_q: 22.753235, mean_eps: 0.749710\n",
            "  2814/100000: episode: 100, duration: 0.175s, episode steps:  19, steps per second: 108, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.704872, mae: 11.136496, accuracy: 0.534539, mean_q: 22.733634, mean_eps: 0.747640\n",
            "  2907/100000: episode: 101, duration: 0.809s, episode steps:  93, steps per second: 115, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.022605, mae: 11.241819, accuracy: 0.549059, mean_q: 23.039715, mean_eps: 0.742600\n",
            "  2924/100000: episode: 102, duration: 0.159s, episode steps:  17, steps per second: 107, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.081753, mae: 11.722359, accuracy: 0.509191, mean_q: 23.953828, mean_eps: 0.737650\n",
            "  2972/100000: episode: 103, duration: 0.409s, episode steps:  48, steps per second: 117, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.325300, mae: 11.779459, accuracy: 0.542969, mean_q: 24.073783, mean_eps: 0.734725\n",
            "  2996/100000: episode: 104, duration: 0.243s, episode steps:  24, steps per second:  99, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.203610, mae: 11.959018, accuracy: 0.539062, mean_q: 24.490378, mean_eps: 0.731485\n",
            "  3072/100000: episode: 105, duration: 0.651s, episode steps:  76, steps per second: 117, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.447725, mae: 12.160539, accuracy: 0.546875, mean_q: 24.892734, mean_eps: 0.726985\n",
            "  3112/100000: episode: 106, duration: 0.344s, episode steps:  40, steps per second: 116, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.518485, mae: 12.514038, accuracy: 0.543750, mean_q: 25.646229, mean_eps: 0.721765\n",
            "  3198/100000: episode: 107, duration: 0.719s, episode steps:  86, steps per second: 120, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 1.371845, mae: 12.769899, accuracy: 0.554869, mean_q: 26.235471, mean_eps: 0.716095\n",
            "  3215/100000: episode: 108, duration: 0.150s, episode steps:  17, steps per second: 113, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.811452, mae: 13.043957, accuracy: 0.556985, mean_q: 26.779786, mean_eps: 0.711460\n",
            "  3225/100000: episode: 109, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.363398, mae: 12.903441, accuracy: 0.531250, mean_q: 26.447471, mean_eps: 0.710245\n",
            "  3248/100000: episode: 110, duration: 0.200s, episode steps:  23, steps per second: 115, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.459277, mae: 13.165756, accuracy: 0.542120, mean_q: 27.093140, mean_eps: 0.708760\n",
            "  3259/100000: episode: 111, duration: 0.107s, episode steps:  11, steps per second: 103, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 1.515391, mae: 13.220216, accuracy: 0.519886, mean_q: 27.102986, mean_eps: 0.707230\n",
            "  3275/100000: episode: 112, duration: 0.160s, episode steps:  16, steps per second: 100, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.154756, mae: 13.566004, accuracy: 0.511719, mean_q: 27.771607, mean_eps: 0.706015\n",
            "  3324/100000: episode: 113, duration: 0.618s, episode steps:  49, steps per second:  79, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.936917, mae: 13.515559, accuracy: 0.567602, mean_q: 27.747649, mean_eps: 0.703090\n",
            "  3430/100000: episode: 114, duration: 1.315s, episode steps: 106, steps per second:  81, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.622426, mae: 13.918738, accuracy: 0.547759, mean_q: 28.601474, mean_eps: 0.696115\n",
            "  3495/100000: episode: 115, duration: 0.879s, episode steps:  65, steps per second:  74, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.091382, mae: 14.353957, accuracy: 0.552885, mean_q: 29.407236, mean_eps: 0.688420\n",
            "  3601/100000: episode: 116, duration: 1.423s, episode steps: 106, steps per second:  74, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.756280, mae: 14.767688, accuracy: 0.575472, mean_q: 30.420791, mean_eps: 0.680725\n",
            "  3816/100000: episode: 117, duration: 2.983s, episode steps: 215, steps per second:  72, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.225394, mae: 15.545121, accuracy: 0.566279, mean_q: 31.889919, mean_eps: 0.666280\n",
            "  3851/100000: episode: 118, duration: 0.483s, episode steps:  35, steps per second:  72, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.831789, mae: 16.276087, accuracy: 0.554464, mean_q: 33.377372, mean_eps: 0.655030\n",
            "  3965/100000: episode: 119, duration: 1.115s, episode steps: 114, steps per second: 102, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.245210, mae: 16.572321, accuracy: 0.567434, mean_q: 34.057228, mean_eps: 0.648325\n",
            "  4049/100000: episode: 120, duration: 0.740s, episode steps:  84, steps per second: 114, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.328608, mae: 17.174001, accuracy: 0.565476, mean_q: 35.250484, mean_eps: 0.639415\n",
            "  4082/100000: episode: 121, duration: 0.301s, episode steps:  33, steps per second: 110, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 2.771673, mae: 17.494064, accuracy: 0.556818, mean_q: 35.852174, mean_eps: 0.634150\n",
            "  4130/100000: episode: 122, duration: 0.424s, episode steps:  48, steps per second: 113, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.811712, mae: 17.513851, accuracy: 0.569661, mean_q: 36.014352, mean_eps: 0.630505\n",
            "  4149/100000: episode: 123, duration: 0.165s, episode steps:  19, steps per second: 115, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 2.271541, mae: 17.807562, accuracy: 0.577303, mean_q: 36.710268, mean_eps: 0.627490\n",
            "  4233/100000: episode: 124, duration: 0.742s, episode steps:  84, steps per second: 113, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.926665, mae: 17.996801, accuracy: 0.566592, mean_q: 37.093972, mean_eps: 0.622855\n",
            "  4347/100000: episode: 125, duration: 0.976s, episode steps: 114, steps per second: 117, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.796333, mae: 18.633433, accuracy: 0.574013, mean_q: 38.285288, mean_eps: 0.613945\n",
            "  4434/100000: episode: 126, duration: 0.721s, episode steps:  87, steps per second: 121, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 2.574985, mae: 19.044855, accuracy: 0.563218, mean_q: 39.314975, mean_eps: 0.604900\n",
            "  4495/100000: episode: 127, duration: 0.537s, episode steps:  61, steps per second: 114, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.404344, mae: 19.557410, accuracy: 0.573258, mean_q: 40.316964, mean_eps: 0.598240\n",
            "  4567/100000: episode: 128, duration: 0.628s, episode steps:  72, steps per second: 115, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.622307, mae: 19.616850, accuracy: 0.584635, mean_q: 40.510634, mean_eps: 0.592255\n",
            "  4614/100000: episode: 129, duration: 0.421s, episode steps:  47, steps per second: 112, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 3.992550, mae: 20.149560, accuracy: 0.578457, mean_q: 41.393349, mean_eps: 0.586900\n",
            "  4825/100000: episode: 130, duration: 1.849s, episode steps: 211, steps per second: 114, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.551949, mae: 20.787365, accuracy: 0.579236, mean_q: 42.712489, mean_eps: 0.575290\n",
            "  4929/100000: episode: 131, duration: 0.924s, episode steps: 104, steps per second: 113, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.145148, mae: 21.629942, accuracy: 0.590445, mean_q: 44.521933, mean_eps: 0.561115\n",
            "  5001/100000: episode: 132, duration: 0.650s, episode steps:  72, steps per second: 111, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.839890, mae: 22.416643, accuracy: 0.593316, mean_q: 46.061305, mean_eps: 0.553195\n",
            "  5072/100000: episode: 133, duration: 0.917s, episode steps:  71, steps per second:  77, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.656148, mae: 22.619136, accuracy: 0.605634, mean_q: 46.826649, mean_eps: 0.546760\n",
            "  5096/100000: episode: 134, duration: 0.308s, episode steps:  24, steps per second:  78, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.074768, mae: 22.972746, accuracy: 0.601562, mean_q: 47.348670, mean_eps: 0.542485\n",
            "  5167/100000: episode: 135, duration: 0.909s, episode steps:  71, steps per second:  78, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.185937, mae: 23.094816, accuracy: 0.570423, mean_q: 47.477198, mean_eps: 0.538210\n",
            "  5315/100000: episode: 136, duration: 2.061s, episode steps: 148, steps per second:  72, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.452691, mae: 23.616784, accuracy: 0.607264, mean_q: 48.581929, mean_eps: 0.528355\n",
            "  5398/100000: episode: 137, duration: 1.137s, episode steps:  83, steps per second:  73, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 3.107076, mae: 24.373023, accuracy: 0.598268, mean_q: 50.185255, mean_eps: 0.517960\n",
            "  5424/100000: episode: 138, duration: 0.368s, episode steps:  26, steps per second:  71, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 7.125276, mae: 24.698234, accuracy: 0.612981, mean_q: 50.757562, mean_eps: 0.513055\n",
            "  5521/100000: episode: 139, duration: 1.290s, episode steps:  97, steps per second:  75, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 4.678508, mae: 24.916834, accuracy: 0.591495, mean_q: 51.116481, mean_eps: 0.507520\n",
            "  5545/100000: episode: 140, duration: 0.319s, episode steps:  24, steps per second:  75, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 4.812526, mae: 25.638137, accuracy: 0.582031, mean_q: 52.599344, mean_eps: 0.502075\n",
            "  5631/100000: episode: 141, duration: 1.049s, episode steps:  86, steps per second:  82, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.638290, mae: 25.663606, accuracy: 0.589026, mean_q: 52.638242, mean_eps: 0.497125\n",
            "  5803/100000: episode: 142, duration: 1.458s, episode steps: 172, steps per second: 118, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 5.210145, mae: 26.124413, accuracy: 0.590480, mean_q: 53.624919, mean_eps: 0.485515\n",
            "  5856/100000: episode: 143, duration: 0.470s, episode steps:  53, steps per second: 113, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.446595, mae: 26.896266, accuracy: 0.577241, mean_q: 55.061187, mean_eps: 0.475390\n",
            "  6008/100000: episode: 144, duration: 1.334s, episode steps: 152, steps per second: 114, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5.175423, mae: 27.445642, accuracy: 0.592928, mean_q: 56.210983, mean_eps: 0.466165\n",
            "  6188/100000: episode: 145, duration: 1.585s, episode steps: 180, steps per second: 114, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4.242395, mae: 28.010489, accuracy: 0.611458, mean_q: 57.614278, mean_eps: 0.451225\n",
            "  6224/100000: episode: 146, duration: 0.332s, episode steps:  36, steps per second: 109, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 5.707563, mae: 28.367967, accuracy: 0.608507, mean_q: 58.411409, mean_eps: 0.441505\n",
            "  6380/100000: episode: 147, duration: 1.396s, episode steps: 156, steps per second: 112, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.885223, mae: 28.961238, accuracy: 0.616386, mean_q: 59.497902, mean_eps: 0.432865\n",
            "  6536/100000: episode: 148, duration: 1.402s, episode steps: 156, steps per second: 111, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.784197, mae: 29.691893, accuracy: 0.613181, mean_q: 60.907374, mean_eps: 0.418825\n",
            "  6675/100000: episode: 149, duration: 1.193s, episode steps: 139, steps per second: 117, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 4.976605, mae: 30.322987, accuracy: 0.620279, mean_q: 62.152225, mean_eps: 0.405550\n",
            "  6721/100000: episode: 150, duration: 0.418s, episode steps:  46, steps per second: 110, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.637580, mae: 30.665209, accuracy: 0.622962, mean_q: 62.946857, mean_eps: 0.397225\n",
            "  6955/100000: episode: 151, duration: 2.958s, episode steps: 234, steps per second:  79, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 5.813820, mae: 31.229368, accuracy: 0.627537, mean_q: 64.168576, mean_eps: 0.384625\n",
            "  7161/100000: episode: 152, duration: 2.738s, episode steps: 206, steps per second:  75, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5.347836, mae: 32.333613, accuracy: 0.630309, mean_q: 66.269202, mean_eps: 0.364825\n",
            "  7385/100000: episode: 153, duration: 2.684s, episode steps: 224, steps per second:  83, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 6.220241, mae: 32.928343, accuracy: 0.629185, mean_q: 67.570571, mean_eps: 0.345475\n",
            "  7555/100000: episode: 154, duration: 1.439s, episode steps: 170, steps per second: 118, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.149540, mae: 33.807954, accuracy: 0.632904, mean_q: 69.158787, mean_eps: 0.327745\n",
            "  7757/100000: episode: 155, duration: 1.750s, episode steps: 202, steps per second: 115, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.194378, mae: 34.103506, accuracy: 0.632735, mean_q: 69.806628, mean_eps: 0.311005\n",
            "  7954/100000: episode: 156, duration: 1.713s, episode steps: 197, steps per second: 115, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.963837, mae: 34.770067, accuracy: 0.643718, mean_q: 71.284690, mean_eps: 0.293050\n",
            "  8089/100000: episode: 157, duration: 1.197s, episode steps: 135, steps per second: 113, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 6.551276, mae: 35.317861, accuracy: 0.652083, mean_q: 72.394737, mean_eps: 0.278110\n",
            "  8268/100000: episode: 158, duration: 1.546s, episode steps: 179, steps per second: 116, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 5.843631, mae: 36.262372, accuracy: 0.644902, mean_q: 74.254042, mean_eps: 0.263980\n",
            "  8476/100000: episode: 159, duration: 1.744s, episode steps: 208, steps per second: 119, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.916693, mae: 36.840501, accuracy: 0.640625, mean_q: 75.415880, mean_eps: 0.246565\n",
            "  8716/100000: episode: 160, duration: 2.965s, episode steps: 240, steps per second:  81, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 7.405158, mae: 37.412180, accuracy: 0.642839, mean_q: 76.402559, mean_eps: 0.226405\n",
            "  8879/100000: episode: 161, duration: 2.165s, episode steps: 163, steps per second:  75, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 6.217593, mae: 38.167494, accuracy: 0.645706, mean_q: 77.948159, mean_eps: 0.208270\n",
            "  9064/100000: episode: 162, duration: 2.495s, episode steps: 185, steps per second:  74, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 6.255572, mae: 38.300397, accuracy: 0.655405, mean_q: 78.240408, mean_eps: 0.192610\n",
            "  9290/100000: episode: 163, duration: 2.084s, episode steps: 226, steps per second: 108, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8.320553, mae: 38.739972, accuracy: 0.647954, mean_q: 78.997747, mean_eps: 0.174115\n",
            "  9454/100000: episode: 164, duration: 1.369s, episode steps: 164, steps per second: 120, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.368279, mae: 39.482038, accuracy: 0.653011, mean_q: 80.719892, mean_eps: 0.156565\n",
            "  9678/100000: episode: 165, duration: 1.891s, episode steps: 224, steps per second: 118, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 7.488911, mae: 39.443270, accuracy: 0.654297, mean_q: 80.398243, mean_eps: 0.139105\n",
            " 10178/100000: episode: 166, duration: 4.266s, episode steps: 500, steps per second: 117, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 6.174223, mae: 40.723162, accuracy: 0.654438, mean_q: 83.064168, mean_eps: 0.109361\n",
            " 10409/100000: episode: 167, duration: 2.485s, episode steps: 231, steps per second:  93, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.899298, mae: 42.086396, accuracy: 0.648945, mean_q: 85.766722, mean_eps: 0.100000\n",
            " 10632/100000: episode: 168, duration: 3.318s, episode steps: 223, steps per second:  67, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.354286, mae: 42.647094, accuracy: 0.638873, mean_q: 86.757059, mean_eps: 0.100000\n",
            " 10855/100000: episode: 169, duration: 3.890s, episode steps: 223, steps per second:  57, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 9.293496, mae: 42.893656, accuracy: 0.643638, mean_q: 87.295688, mean_eps: 0.100000\n",
            " 11094/100000: episode: 170, duration: 2.093s, episode steps: 239, steps per second: 114, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.533623, mae: 43.819875, accuracy: 0.654812, mean_q: 89.365336, mean_eps: 0.100000\n",
            " 11276/100000: episode: 171, duration: 1.537s, episode steps: 182, steps per second: 118, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 10.045257, mae: 44.462557, accuracy: 0.625859, mean_q: 90.241152, mean_eps: 0.100000\n",
            " 11577/100000: episode: 172, duration: 2.570s, episode steps: 301, steps per second: 117, episode reward: 301.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 7.259146, mae: 44.522516, accuracy: 0.629672, mean_q: 90.443145, mean_eps: 0.100000\n",
            " 11809/100000: episode: 173, duration: 2.025s, episode steps: 232, steps per second: 115, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 7.254434, mae: 45.053159, accuracy: 0.643992, mean_q: 91.684377, mean_eps: 0.100000\n",
            " 12110/100000: episode: 174, duration: 3.349s, episode steps: 301, steps per second:  90, episode reward: 301.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 8.173702, mae: 45.494969, accuracy: 0.646387, mean_q: 92.390846, mean_eps: 0.100000\n",
            " 12451/100000: episode: 175, duration: 4.425s, episode steps: 341, steps per second:  77, episode reward: 341.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 9.569846, mae: 46.210104, accuracy: 0.646994, mean_q: 93.755000, mean_eps: 0.100000\n",
            " 12624/100000: episode: 176, duration: 1.977s, episode steps: 173, steps per second:  88, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 12.793046, mae: 46.529451, accuracy: 0.638728, mean_q: 94.317409, mean_eps: 0.100000\n",
            " 12813/100000: episode: 177, duration: 1.645s, episode steps: 189, steps per second: 115, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 7.584329, mae: 46.618543, accuracy: 0.643519, mean_q: 94.930706, mean_eps: 0.100000\n",
            " 13057/100000: episode: 178, duration: 2.107s, episode steps: 244, steps per second: 116, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.241902, mae: 47.211876, accuracy: 0.638832, mean_q: 95.859603, mean_eps: 0.100000\n",
            " 13365/100000: episode: 179, duration: 2.676s, episode steps: 308, steps per second: 115, episode reward: 308.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.621639, mae: 47.129894, accuracy: 0.645495, mean_q: 95.581940, mean_eps: 0.100000\n",
            " 13549/100000: episode: 180, duration: 1.575s, episode steps: 184, steps per second: 117, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.468328, mae: 47.081195, accuracy: 0.640115, mean_q: 95.548384, mean_eps: 0.100000\n",
            " 13722/100000: episode: 181, duration: 1.506s, episode steps: 173, steps per second: 115, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8.164719, mae: 47.609717, accuracy: 0.624639, mean_q: 96.563189, mean_eps: 0.100000\n",
            " 13925/100000: episode: 182, duration: 2.556s, episode steps: 203, steps per second:  79, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 8.402398, mae: 47.354076, accuracy: 0.639932, mean_q: 96.218605, mean_eps: 0.100000\n",
            " 14102/100000: episode: 183, duration: 2.443s, episode steps: 177, steps per second:  72, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 8.387099, mae: 47.649323, accuracy: 0.625177, mean_q: 96.658978, mean_eps: 0.100000\n",
            " 14434/100000: episode: 184, duration: 3.908s, episode steps: 332, steps per second:  85, episode reward: 332.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 6.156304, mae: 47.679319, accuracy: 0.644108, mean_q: 96.838976, mean_eps: 0.100000\n",
            " 14635/100000: episode: 185, duration: 1.713s, episode steps: 201, steps per second: 117, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 9.129793, mae: 48.124295, accuracy: 0.654073, mean_q: 97.833100, mean_eps: 0.100000\n",
            " 14819/100000: episode: 186, duration: 1.554s, episode steps: 184, steps per second: 118, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.697890, mae: 48.436377, accuracy: 0.639266, mean_q: 98.266804, mean_eps: 0.100000\n",
            " 15077/100000: episode: 187, duration: 2.189s, episode steps: 258, steps per second: 118, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6.780455, mae: 48.567074, accuracy: 0.658551, mean_q: 98.655686, mean_eps: 0.100000\n",
            " 15297/100000: episode: 188, duration: 1.925s, episode steps: 220, steps per second: 114, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 7.351108, mae: 49.140958, accuracy: 0.643608, mean_q: 99.759251, mean_eps: 0.100000\n",
            " 15479/100000: episode: 189, duration: 2.427s, episode steps: 182, steps per second:  75, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 11.163052, mae: 48.751408, accuracy: 0.657795, mean_q: 98.968246, mean_eps: 0.100000\n",
            " 15694/100000: episode: 190, duration: 8.536s, episode steps: 215, steps per second:  25, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 7.616727, mae: 49.089036, accuracy: 0.646948, mean_q: 99.864908, mean_eps: 0.100000\n",
            " 15881/100000: episode: 191, duration: 2.573s, episode steps: 187, steps per second:  73, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.394107, mae: 49.362480, accuracy: 0.622159, mean_q: 100.323861, mean_eps: 0.100000\n",
            " 16201/100000: episode: 192, duration: 3.322s, episode steps: 320, steps per second:  96, episode reward: 320.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.230330, mae: 49.440318, accuracy: 0.629102, mean_q: 100.551680, mean_eps: 0.100000\n",
            " 16462/100000: episode: 193, duration: 2.266s, episode steps: 261, steps per second: 115, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 6.797209, mae: 50.048310, accuracy: 0.648228, mean_q: 101.638791, mean_eps: 0.100000\n",
            " 16672/100000: episode: 194, duration: 1.885s, episode steps: 210, steps per second: 111, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.363645, mae: 50.517860, accuracy: 0.642411, mean_q: 102.462472, mean_eps: 0.100000\n",
            " 16851/100000: episode: 195, duration: 1.580s, episode steps: 179, steps per second: 113, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 7.709276, mae: 50.483184, accuracy: 0.655203, mean_q: 102.515692, mean_eps: 0.100000\n",
            " 17043/100000: episode: 196, duration: 1.628s, episode steps: 192, steps per second: 118, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 5.322502, mae: 50.606130, accuracy: 0.661784, mean_q: 102.897665, mean_eps: 0.100000\n",
            " 17298/100000: episode: 197, duration: 2.823s, episode steps: 255, steps per second:  90, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.698921, mae: 50.889747, accuracy: 0.657843, mean_q: 103.496622, mean_eps: 0.100000\n",
            " 17525/100000: episode: 198, duration: 3.073s, episode steps: 227, steps per second:  74, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.337592, mae: 50.956779, accuracy: 0.647852, mean_q: 103.250150, mean_eps: 0.100000\n",
            " 17725/100000: episode: 199, duration: 2.758s, episode steps: 200, steps per second:  73, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.333354, mae: 51.094257, accuracy: 0.653438, mean_q: 103.418580, mean_eps: 0.100000\n",
            " 17895/100000: episode: 200, duration: 1.499s, episode steps: 170, steps per second: 113, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 5.917873, mae: 51.071697, accuracy: 0.658088, mean_q: 103.546843, mean_eps: 0.100000\n",
            " 18088/100000: episode: 201, duration: 1.695s, episode steps: 193, steps per second: 114, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 7.329526, mae: 50.853675, accuracy: 0.651554, mean_q: 103.023504, mean_eps: 0.100000\n",
            " 18258/100000: episode: 202, duration: 1.466s, episode steps: 170, steps per second: 116, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 4.865921, mae: 50.730928, accuracy: 0.648897, mean_q: 102.830078, mean_eps: 0.100000\n",
            " 18548/100000: episode: 203, duration: 2.494s, episode steps: 290, steps per second: 116, episode reward: 290.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.977843, mae: 51.229348, accuracy: 0.641164, mean_q: 103.961422, mean_eps: 0.100000\n",
            " 18747/100000: episode: 204, duration: 1.772s, episode steps: 199, steps per second: 112, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.807032, mae: 52.125810, accuracy: 0.655779, mean_q: 105.696629, mean_eps: 0.100000\n",
            " 19070/100000: episode: 205, duration: 4.698s, episode steps: 323, steps per second:  69, episode reward: 323.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.299711, mae: 51.914653, accuracy: 0.649865, mean_q: 105.306149, mean_eps: 0.100000\n",
            " 19404/100000: episode: 206, duration: 4.511s, episode steps: 334, steps per second:  74, episode reward: 334.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.448351, mae: 52.445904, accuracy: 0.654753, mean_q: 106.275659, mean_eps: 0.100000\n",
            " 19718/100000: episode: 207, duration: 4.216s, episode steps: 314, steps per second:  74, episode reward: 314.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.212281, mae: 52.797604, accuracy: 0.645701, mean_q: 107.090813, mean_eps: 0.100000\n",
            " 20026/100000: episode: 208, duration: 3.888s, episode steps: 308, steps per second:  79, episode reward: 308.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.451666, mae: 53.235883, accuracy: 0.640016, mean_q: 107.891938, mean_eps: 0.100000\n",
            " 20258/100000: episode: 209, duration: 2.037s, episode steps: 232, steps per second: 114, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.412549, mae: 53.667162, accuracy: 0.660830, mean_q: 108.712656, mean_eps: 0.100000\n",
            " 20443/100000: episode: 210, duration: 2.330s, episode steps: 185, steps per second:  79, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 8.428937, mae: 54.077709, accuracy: 0.648142, mean_q: 109.303841, mean_eps: 0.100000\n",
            " 20636/100000: episode: 211, duration: 2.689s, episode steps: 193, steps per second:  72, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 5.251966, mae: 53.825127, accuracy: 0.664670, mean_q: 109.142053, mean_eps: 0.100000\n",
            " 20823/100000: episode: 212, duration: 2.518s, episode steps: 187, steps per second:  74, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.198956, mae: 54.445797, accuracy: 0.655247, mean_q: 110.352605, mean_eps: 0.100000\n",
            " 21098/100000: episode: 213, duration: 2.539s, episode steps: 275, steps per second: 108, episode reward: 275.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 8.802965, mae: 54.356525, accuracy: 0.658068, mean_q: 110.123085, mean_eps: 0.100000\n",
            " 21309/100000: episode: 214, duration: 1.854s, episode steps: 211, steps per second: 114, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 6.191865, mae: 54.050880, accuracy: 0.650622, mean_q: 109.424745, mean_eps: 0.100000\n",
            " 21602/100000: episode: 215, duration: 2.538s, episode steps: 293, steps per second: 115, episode reward: 293.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.335093, mae: 54.225865, accuracy: 0.660303, mean_q: 109.696378, mean_eps: 0.100000\n",
            " 21885/100000: episode: 216, duration: 2.450s, episode steps: 283, steps per second: 116, episode reward: 283.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.694345, mae: 54.372522, accuracy: 0.653489, mean_q: 109.949267, mean_eps: 0.100000\n",
            " 22145/100000: episode: 217, duration: 2.916s, episode steps: 260, steps per second:  89, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 7.046694, mae: 54.191531, accuracy: 0.667188, mean_q: 109.724654, mean_eps: 0.100000\n",
            " 22499/100000: episode: 218, duration: 4.784s, episode steps: 354, steps per second:  74, episode reward: 354.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.420049, mae: 54.591523, accuracy: 0.660929, mean_q: 110.504208, mean_eps: 0.100000\n",
            " 22739/100000: episode: 219, duration: 2.585s, episode steps: 240, steps per second:  93, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.388094, mae: 54.143190, accuracy: 0.666927, mean_q: 109.628463, mean_eps: 0.100000\n",
            " 22949/100000: episode: 220, duration: 1.862s, episode steps: 210, steps per second: 113, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 10.511805, mae: 53.504457, accuracy: 0.673661, mean_q: 108.231553, mean_eps: 0.100000\n",
            " 23288/100000: episode: 221, duration: 3.075s, episode steps: 339, steps per second: 110, episode reward: 339.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.153054, mae: 54.429451, accuracy: 0.660767, mean_q: 110.148767, mean_eps: 0.100000\n",
            " 23583/100000: episode: 222, duration: 2.631s, episode steps: 295, steps per second: 112, episode reward: 295.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.077593, mae: 54.278778, accuracy: 0.658369, mean_q: 109.711115, mean_eps: 0.100000\n",
            " 23882/100000: episode: 223, duration: 3.262s, episode steps: 299, steps per second:  92, episode reward: 299.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.738034, mae: 54.278256, accuracy: 0.661476, mean_q: 109.787375, mean_eps: 0.100000\n",
            " 24137/100000: episode: 224, duration: 3.412s, episode steps: 255, steps per second:  75, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.524364, mae: 54.179958, accuracy: 0.664461, mean_q: 109.582004, mean_eps: 0.100000\n",
            " 24397/100000: episode: 225, duration: 3.044s, episode steps: 260, steps per second:  85, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 5.040592, mae: 54.180936, accuracy: 0.675601, mean_q: 109.669226, mean_eps: 0.100000\n",
            " 24630/100000: episode: 226, duration: 2.389s, episode steps: 233, steps per second:  98, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.479036, mae: 53.270068, accuracy: 0.682940, mean_q: 107.897168, mean_eps: 0.100000\n",
            " 24827/100000: episode: 227, duration: 1.743s, episode steps: 197, steps per second: 113, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5.456666, mae: 53.322139, accuracy: 0.648319, mean_q: 107.899111, mean_eps: 0.100000\n",
            " 25053/100000: episode: 228, duration: 2.031s, episode steps: 226, steps per second: 111, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 6.003322, mae: 53.873555, accuracy: 0.656112, mean_q: 109.121374, mean_eps: 0.100000\n",
            " 25255/100000: episode: 229, duration: 1.860s, episode steps: 202, steps per second: 109, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.591359, mae: 53.485969, accuracy: 0.658880, mean_q: 108.224283, mean_eps: 0.100000\n",
            " 25657/100000: episode: 230, duration: 4.568s, episode steps: 402, steps per second:  88, episode reward: 402.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.369651, mae: 53.729773, accuracy: 0.650420, mean_q: 108.704153, mean_eps: 0.100000\n",
            " 25927/100000: episode: 231, duration: 3.683s, episode steps: 270, steps per second:  73, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 6.114437, mae: 53.870982, accuracy: 0.657176, mean_q: 109.081748, mean_eps: 0.100000\n",
            " 26167/100000: episode: 232, duration: 2.410s, episode steps: 240, steps per second: 100, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 5.956572, mae: 53.770537, accuracy: 0.655990, mean_q: 108.869920, mean_eps: 0.100000\n",
            " 26372/100000: episode: 233, duration: 1.854s, episode steps: 205, steps per second: 111, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.619141, mae: 53.461319, accuracy: 0.656098, mean_q: 107.993753, mean_eps: 0.100000\n",
            " 26629/100000: episode: 234, duration: 2.318s, episode steps: 257, steps per second: 111, episode reward: 257.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 5.081113, mae: 53.935424, accuracy: 0.645428, mean_q: 109.305521, mean_eps: 0.100000\n",
            " 27061/100000: episode: 235, duration: 3.814s, episode steps: 432, steps per second: 113, episode reward: 432.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.242594, mae: 53.681985, accuracy: 0.667969, mean_q: 108.612772, mean_eps: 0.100000\n",
            " 27332/100000: episode: 236, duration: 3.440s, episode steps: 271, steps per second:  79, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.633807, mae: 54.083021, accuracy: 0.653598, mean_q: 109.297325, mean_eps: 0.100000\n",
            " 27629/100000: episode: 237, duration: 4.172s, episode steps: 297, steps per second:  71, episode reward: 297.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.352030, mae: 53.729606, accuracy: 0.650042, mean_q: 108.653337, mean_eps: 0.100000\n",
            " 27899/100000: episode: 238, duration: 2.624s, episode steps: 270, steps per second: 103, episode reward: 270.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.568311, mae: 54.055896, accuracy: 0.665741, mean_q: 109.361044, mean_eps: 0.100000\n",
            " 28222/100000: episode: 239, duration: 2.831s, episode steps: 323, steps per second: 114, episode reward: 323.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 4.326319, mae: 53.946092, accuracy: 0.664183, mean_q: 109.119890, mean_eps: 0.100000\n",
            " 28472/100000: episode: 240, duration: 2.236s, episode steps: 250, steps per second: 112, episode reward: 250.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.298661, mae: 53.513601, accuracy: 0.669125, mean_q: 108.192274, mean_eps: 0.100000\n",
            " 28661/100000: episode: 241, duration: 1.677s, episode steps: 189, steps per second: 113, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.137704, mae: 54.089516, accuracy: 0.658234, mean_q: 109.532257, mean_eps: 0.100000\n",
            " 28900/100000: episode: 242, duration: 2.613s, episode steps: 239, steps per second:  91, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.339585, mae: 53.362485, accuracy: 0.672725, mean_q: 107.883285, mean_eps: 0.100000\n",
            " 29183/100000: episode: 243, duration: 3.845s, episode steps: 283, steps per second:  74, episode reward: 283.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4.143572, mae: 53.625570, accuracy: 0.674801, mean_q: 108.465926, mean_eps: 0.100000\n",
            " 29486/100000: episode: 244, duration: 3.631s, episode steps: 303, steps per second:  83, episode reward: 303.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.229271, mae: 53.893284, accuracy: 0.671411, mean_q: 109.167641, mean_eps: 0.100000\n",
            " 29808/100000: episode: 245, duration: 2.910s, episode steps: 322, steps per second: 111, episode reward: 322.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.114840, mae: 53.600251, accuracy: 0.666828, mean_q: 108.425465, mean_eps: 0.100000\n",
            " 30216/100000: episode: 246, duration: 3.631s, episode steps: 408, steps per second: 112, episode reward: 408.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.614540, mae: 53.556200, accuracy: 0.671492, mean_q: 108.260990, mean_eps: 0.100000\n",
            " 30423/100000: episode: 247, duration: 1.833s, episode steps: 207, steps per second: 113, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.412176, mae: 53.006500, accuracy: 0.683726, mean_q: 107.223708, mean_eps: 0.100000\n",
            " 30720/100000: episode: 248, duration: 3.480s, episode steps: 297, steps per second:  85, episode reward: 297.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.346876, mae: 53.065907, accuracy: 0.676768, mean_q: 107.186030, mean_eps: 0.100000\n",
            " 30932/100000: episode: 249, duration: 2.932s, episode steps: 212, steps per second:  72, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 5.475842, mae: 53.285516, accuracy: 0.671875, mean_q: 107.577903, mean_eps: 0.100000\n",
            " 31166/100000: episode: 250, duration: 2.799s, episode steps: 234, steps per second:  84, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.001312, mae: 53.274394, accuracy: 0.679354, mean_q: 107.640808, mean_eps: 0.100000\n",
            " 31400/100000: episode: 251, duration: 2.149s, episode steps: 234, steps per second: 109, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.621864, mae: 52.736772, accuracy: 0.672943, mean_q: 106.512736, mean_eps: 0.100000\n",
            " 31613/100000: episode: 252, duration: 1.937s, episode steps: 213, steps per second: 110, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.461365, mae: 52.767349, accuracy: 0.680604, mean_q: 106.550263, mean_eps: 0.100000\n",
            " 31853/100000: episode: 253, duration: 2.190s, episode steps: 240, steps per second: 110, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.928678, mae: 52.783265, accuracy: 0.668750, mean_q: 106.684137, mean_eps: 0.100000\n",
            " 32101/100000: episode: 254, duration: 2.182s, episode steps: 248, steps per second: 114, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.845252, mae: 52.390266, accuracy: 0.675655, mean_q: 105.796566, mean_eps: 0.100000\n",
            " 32350/100000: episode: 255, duration: 2.877s, episode steps: 249, steps per second:  87, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.341814, mae: 52.341307, accuracy: 0.675828, mean_q: 105.729912, mean_eps: 0.100000\n",
            " 32615/100000: episode: 256, duration: 3.576s, episode steps: 265, steps per second:  74, episode reward: 265.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.679841, mae: 52.499970, accuracy: 0.676651, mean_q: 105.785506, mean_eps: 0.100000\n",
            " 32825/100000: episode: 257, duration: 2.674s, episode steps: 210, steps per second:  79, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.159480, mae: 51.874683, accuracy: 0.646726, mean_q: 104.643341, mean_eps: 0.100000\n",
            " 33051/100000: episode: 258, duration: 2.039s, episode steps: 226, steps per second: 111, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.268923, mae: 52.681258, accuracy: 0.667035, mean_q: 106.254987, mean_eps: 0.100000\n",
            " 33299/100000: episode: 259, duration: 2.195s, episode steps: 248, steps per second: 113, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.591444, mae: 52.030867, accuracy: 0.673387, mean_q: 104.993679, mean_eps: 0.100000\n",
            " 33565/100000: episode: 260, duration: 2.355s, episode steps: 266, steps per second: 113, episode reward: 266.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3.428551, mae: 51.665277, accuracy: 0.687265, mean_q: 104.440981, mean_eps: 0.100000\n",
            " 33795/100000: episode: 261, duration: 2.073s, episode steps: 230, steps per second: 111, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 4.093852, mae: 51.994934, accuracy: 0.676766, mean_q: 104.956779, mean_eps: 0.100000\n",
            " 33985/100000: episode: 262, duration: 2.043s, episode steps: 190, steps per second:  93, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.774210, mae: 51.896897, accuracy: 0.675493, mean_q: 104.785573, mean_eps: 0.100000\n",
            " 34228/100000: episode: 263, duration: 3.281s, episode steps: 243, steps per second:  74, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.683309, mae: 51.686112, accuracy: 0.671168, mean_q: 104.369880, mean_eps: 0.100000\n",
            " 34564/100000: episode: 264, duration: 4.134s, episode steps: 336, steps per second:  81, episode reward: 336.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.423037, mae: 51.450954, accuracy: 0.663225, mean_q: 103.930391, mean_eps: 0.100000\n",
            " 35044/100000: episode: 265, duration: 4.287s, episode steps: 480, steps per second: 112, episode reward: 480.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.955933, mae: 51.633873, accuracy: 0.679883, mean_q: 104.271905, mean_eps: 0.100000\n",
            " 35295/100000: episode: 266, duration: 2.302s, episode steps: 251, steps per second: 109, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.352539, mae: 51.603438, accuracy: 0.670817, mean_q: 104.210958, mean_eps: 0.100000\n",
            " 35467/100000: episode: 267, duration: 1.637s, episode steps: 172, steps per second: 105, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.899393, mae: 51.697588, accuracy: 0.682958, mean_q: 104.215259, mean_eps: 0.100000\n",
            " 35684/100000: episode: 268, duration: 2.480s, episode steps: 217, steps per second:  88, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.501536, mae: 51.043230, accuracy: 0.681020, mean_q: 102.977379, mean_eps: 0.100000\n",
            " 35925/100000: episode: 269, duration: 3.247s, episode steps: 241, steps per second:  74, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 4.181723, mae: 50.967870, accuracy: 0.664678, mean_q: 102.778347, mean_eps: 0.100000\n",
            " 36210/100000: episode: 270, duration: 3.640s, episode steps: 285, steps per second:  78, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.872099, mae: 50.909384, accuracy: 0.642325, mean_q: 102.680916, mean_eps: 0.100000\n",
            " 36382/100000: episode: 271, duration: 1.542s, episode steps: 172, steps per second: 112, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.847846, mae: 51.093497, accuracy: 0.656977, mean_q: 103.151806, mean_eps: 0.100000\n",
            " 36578/100000: episode: 272, duration: 1.828s, episode steps: 196, steps per second: 107, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 5.164624, mae: 50.581442, accuracy: 0.651626, mean_q: 102.078865, mean_eps: 0.100000\n",
            " 36745/100000: episode: 273, duration: 1.547s, episode steps: 167, steps per second: 108, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.897332, mae: 50.622311, accuracy: 0.641467, mean_q: 102.047512, mean_eps: 0.100000\n",
            " 36910/100000: episode: 274, duration: 1.488s, episode steps: 165, steps per second: 111, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 3.462686, mae: 50.183292, accuracy: 0.645455, mean_q: 101.280542, mean_eps: 0.100000\n",
            " 37101/100000: episode: 275, duration: 1.712s, episode steps: 191, steps per second: 112, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.679922, mae: 50.102014, accuracy: 0.650687, mean_q: 101.091673, mean_eps: 0.100000\n",
            " 37465/100000: episode: 276, duration: 4.268s, episode steps: 364, steps per second:  85, episode reward: 364.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.313181, mae: 50.230115, accuracy: 0.671703, mean_q: 101.334040, mean_eps: 0.100000\n",
            " 37711/100000: episode: 277, duration: 3.441s, episode steps: 246, steps per second:  71, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.077245, mae: 50.362312, accuracy: 0.663618, mean_q: 101.596158, mean_eps: 0.100000\n",
            " 37975/100000: episode: 278, duration: 2.828s, episode steps: 264, steps per second:  93, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.673297, mae: 50.140683, accuracy: 0.679332, mean_q: 101.086097, mean_eps: 0.100000\n",
            " 38184/100000: episode: 279, duration: 1.882s, episode steps: 209, steps per second: 111, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.816674, mae: 49.606800, accuracy: 0.683762, mean_q: 100.070186, mean_eps: 0.100000\n",
            " 38412/100000: episode: 280, duration: 2.031s, episode steps: 228, steps per second: 112, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.003806, mae: 49.954318, accuracy: 0.668860, mean_q: 100.685114, mean_eps: 0.100000\n",
            " 38574/100000: episode: 281, duration: 1.454s, episode steps: 162, steps per second: 111, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 4.316159, mae: 49.672367, accuracy: 0.650077, mean_q: 99.814783, mean_eps: 0.100000\n",
            " 38803/100000: episode: 282, duration: 2.081s, episode steps: 229, steps per second: 110, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4.796103, mae: 48.961170, accuracy: 0.696507, mean_q: 98.640391, mean_eps: 0.100000\n",
            " 38999/100000: episode: 283, duration: 2.172s, episode steps: 196, steps per second:  90, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4.931192, mae: 48.869912, accuracy: 0.699139, mean_q: 98.516647, mean_eps: 0.100000\n",
            " 39254/100000: episode: 284, duration: 3.551s, episode steps: 255, steps per second:  72, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.841771, mae: 48.871470, accuracy: 0.688603, mean_q: 98.573358, mean_eps: 0.100000\n",
            " 39491/100000: episode: 285, duration: 3.288s, episode steps: 237, steps per second:  72, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.796649, mae: 48.945437, accuracy: 0.671677, mean_q: 98.550082, mean_eps: 0.100000\n",
            " 39692/100000: episode: 286, duration: 1.810s, episode steps: 201, steps per second: 111, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.147162, mae: 48.500511, accuracy: 0.661692, mean_q: 97.816730, mean_eps: 0.100000\n",
            " 39858/100000: episode: 287, duration: 1.521s, episode steps: 166, steps per second: 109, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 3.740796, mae: 48.299062, accuracy: 0.652485, mean_q: 97.160049, mean_eps: 0.100000\n",
            " 40092/100000: episode: 288, duration: 2.114s, episode steps: 234, steps per second: 111, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.975997, mae: 48.271245, accuracy: 0.677484, mean_q: 97.236014, mean_eps: 0.100000\n",
            " 40359/100000: episode: 289, duration: 2.454s, episode steps: 267, steps per second: 109, episode reward: 267.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2.453458, mae: 47.715992, accuracy: 0.693820, mean_q: 96.238255, mean_eps: 0.100000\n",
            " 40620/100000: episode: 290, duration: 2.655s, episode steps: 261, steps per second:  98, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.117420, mae: 47.750291, accuracy: 0.676964, mean_q: 96.238865, mean_eps: 0.100000\n",
            " 40812/100000: episode: 291, duration: 2.545s, episode steps: 192, steps per second:  75, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.675672, mae: 48.008658, accuracy: 0.679362, mean_q: 96.816357, mean_eps: 0.100000\n",
            " 40993/100000: episode: 292, duration: 2.582s, episode steps: 181, steps per second:  70, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 1.849423, mae: 47.357975, accuracy: 0.676968, mean_q: 95.559664, mean_eps: 0.100000\n",
            " 41233/100000: episode: 293, duration: 2.861s, episode steps: 240, steps per second:  84, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.121062, mae: 47.994541, accuracy: 0.689583, mean_q: 96.729804, mean_eps: 0.100000\n",
            " 41520/100000: episode: 294, duration: 2.653s, episode steps: 287, steps per second: 108, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.916698, mae: 47.497625, accuracy: 0.678789, mean_q: 95.731023, mean_eps: 0.100000\n",
            " 41716/100000: episode: 295, duration: 1.837s, episode steps: 196, steps per second: 107, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.438050, mae: 47.303984, accuracy: 0.662946, mean_q: 95.327700, mean_eps: 0.100000\n",
            " 41937/100000: episode: 296, duration: 2.080s, episode steps: 221, steps per second: 106, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.216063, mae: 47.333513, accuracy: 0.673501, mean_q: 95.348240, mean_eps: 0.100000\n",
            " 42190/100000: episode: 297, duration: 2.405s, episode steps: 253, steps per second: 105, episode reward: 253.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.522208, mae: 47.039055, accuracy: 0.667737, mean_q: 94.756823, mean_eps: 0.100000\n",
            " 42399/100000: episode: 298, duration: 2.820s, episode steps: 209, steps per second:  74, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.096075, mae: 46.680489, accuracy: 0.679575, mean_q: 94.081544, mean_eps: 0.100000\n",
            " 42650/100000: episode: 299, duration: 3.665s, episode steps: 251, steps per second:  68, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.239995, mae: 46.681694, accuracy: 0.682893, mean_q: 94.134791, mean_eps: 0.100000\n",
            " 42857/100000: episode: 300, duration: 2.335s, episode steps: 207, steps per second:  89, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.060792, mae: 46.171914, accuracy: 0.641757, mean_q: 93.136813, mean_eps: 0.100000\n",
            " 43068/100000: episode: 301, duration: 1.932s, episode steps: 211, steps per second: 109, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.212702, mae: 46.359302, accuracy: 0.645142, mean_q: 93.383594, mean_eps: 0.100000\n",
            " 43347/100000: episode: 302, duration: 2.539s, episode steps: 279, steps per second: 110, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.992052, mae: 46.548022, accuracy: 0.657370, mean_q: 93.778345, mean_eps: 0.100000\n",
            " 43571/100000: episode: 303, duration: 2.052s, episode steps: 224, steps per second: 109, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.970746, mae: 46.320901, accuracy: 0.641183, mean_q: 93.315279, mean_eps: 0.100000\n",
            " 43769/100000: episode: 304, duration: 1.843s, episode steps: 198, steps per second: 107, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.450182, mae: 46.347831, accuracy: 0.642203, mean_q: 93.261651, mean_eps: 0.100000\n",
            " 43987/100000: episode: 305, duration: 2.654s, episode steps: 218, steps per second:  82, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.219503, mae: 45.829485, accuracy: 0.657827, mean_q: 92.323340, mean_eps: 0.100000\n",
            " 44172/100000: episode: 306, duration: 2.635s, episode steps: 185, steps per second:  70, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 3.014340, mae: 45.853397, accuracy: 0.649155, mean_q: 92.296848, mean_eps: 0.100000\n",
            " 44401/100000: episode: 307, duration: 3.127s, episode steps: 229, steps per second:  73, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.493936, mae: 45.668976, accuracy: 0.660617, mean_q: 91.997723, mean_eps: 0.100000\n",
            " 44615/100000: episode: 308, duration: 1.983s, episode steps: 214, steps per second: 108, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.725446, mae: 45.703620, accuracy: 0.669246, mean_q: 92.221684, mean_eps: 0.100000\n",
            " 44825/100000: episode: 309, duration: 1.975s, episode steps: 210, steps per second: 106, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.309096, mae: 46.082062, accuracy: 0.662798, mean_q: 92.894154, mean_eps: 0.100000\n",
            " 45002/100000: episode: 310, duration: 1.664s, episode steps: 177, steps per second: 106, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.753237, mae: 45.754763, accuracy: 0.645657, mean_q: 92.159478, mean_eps: 0.100000\n",
            " 45263/100000: episode: 311, duration: 2.468s, episode steps: 261, steps per second: 106, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.802240, mae: 45.490277, accuracy: 0.650383, mean_q: 91.471345, mean_eps: 0.100000\n",
            " 45518/100000: episode: 312, duration: 2.662s, episode steps: 255, steps per second:  96, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.959347, mae: 45.463847, accuracy: 0.632353, mean_q: 91.554363, mean_eps: 0.100000\n",
            " 45725/100000: episode: 313, duration: 2.797s, episode steps: 207, steps per second:  74, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.641848, mae: 45.625204, accuracy: 0.644173, mean_q: 91.812329, mean_eps: 0.100000\n",
            " 45897/100000: episode: 314, duration: 2.502s, episode steps: 172, steps per second:  69, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.116626, mae: 45.224957, accuracy: 0.654797, mean_q: 91.104211, mean_eps: 0.100000\n",
            " 46085/100000: episode: 315, duration: 2.316s, episode steps: 188, steps per second:  81, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 2.000862, mae: 45.143414, accuracy: 0.649102, mean_q: 90.833277, mean_eps: 0.100000\n",
            " 46300/100000: episode: 316, duration: 2.020s, episode steps: 215, steps per second: 106, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.503744, mae: 45.116146, accuracy: 0.646512, mean_q: 90.945912, mean_eps: 0.100000\n",
            " 46498/100000: episode: 317, duration: 1.869s, episode steps: 198, steps per second: 106, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.468163, mae: 44.938036, accuracy: 0.648990, mean_q: 90.554639, mean_eps: 0.100000\n",
            " 46692/100000: episode: 318, duration: 1.908s, episode steps: 194, steps per second: 102, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.496197, mae: 44.758726, accuracy: 0.652062, mean_q: 90.175367, mean_eps: 0.100000\n",
            " 46984/100000: episode: 319, duration: 2.766s, episode steps: 292, steps per second: 106, episode reward: 292.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.742106, mae: 44.943120, accuracy: 0.636344, mean_q: 90.505443, mean_eps: 0.100000\n",
            " 47170/100000: episode: 320, duration: 2.325s, episode steps: 186, steps per second:  80, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1.626438, mae: 45.085292, accuracy: 0.615927, mean_q: 90.740504, mean_eps: 0.100000\n",
            " 47369/100000: episode: 321, duration: 2.970s, episode steps: 199, steps per second:  67, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.005886, mae: 44.783634, accuracy: 0.619190, mean_q: 90.136737, mean_eps: 0.100000\n",
            " 47586/100000: episode: 322, duration: 3.121s, episode steps: 217, steps per second:  70, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.115450, mae: 44.363841, accuracy: 0.614631, mean_q: 89.181503, mean_eps: 0.100000\n",
            " 47774/100000: episode: 323, duration: 1.798s, episode steps: 188, steps per second: 105, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.215553, mae: 44.544465, accuracy: 0.621011, mean_q: 89.645387, mean_eps: 0.100000\n",
            " 47968/100000: episode: 324, duration: 1.900s, episode steps: 194, steps per second: 102, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 2.365484, mae: 45.230878, accuracy: 0.628222, mean_q: 91.019898, mean_eps: 0.100000\n",
            " 48201/100000: episode: 325, duration: 2.169s, episode steps: 233, steps per second: 107, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.819182, mae: 44.448522, accuracy: 0.649410, mean_q: 89.502142, mean_eps: 0.100000\n",
            " 48451/100000: episode: 326, duration: 2.429s, episode steps: 250, steps per second: 103, episode reward: 250.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.223499, mae: 44.236311, accuracy: 0.639875, mean_q: 89.170524, mean_eps: 0.100000\n",
            " 48645/100000: episode: 327, duration: 2.080s, episode steps: 194, steps per second:  93, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.513156, mae: 44.212426, accuracy: 0.636437, mean_q: 89.078840, mean_eps: 0.100000\n",
            " 48859/100000: episode: 328, duration: 3.040s, episode steps: 214, steps per second:  70, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.648708, mae: 44.353107, accuracy: 0.609667, mean_q: 89.192830, mean_eps: 0.100000\n",
            " 49055/100000: episode: 329, duration: 2.943s, episode steps: 196, steps per second:  67, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.958641, mae: 44.395462, accuracy: 0.620217, mean_q: 89.348719, mean_eps: 0.100000\n",
            " 49262/100000: episode: 330, duration: 2.404s, episode steps: 207, steps per second:  86, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.534629, mae: 44.408246, accuracy: 0.617905, mean_q: 89.337769, mean_eps: 0.100000\n",
            " 49501/100000: episode: 331, duration: 2.240s, episode steps: 239, steps per second: 107, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.502750, mae: 44.531130, accuracy: 0.624085, mean_q: 89.748022, mean_eps: 0.100000\n",
            " 49729/100000: episode: 332, duration: 2.226s, episode steps: 228, steps per second: 102, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.779490, mae: 44.434535, accuracy: 0.628427, mean_q: 89.423142, mean_eps: 0.100000\n",
            " 49942/100000: episode: 333, duration: 2.074s, episode steps: 213, steps per second: 103, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.869952, mae: 44.602756, accuracy: 0.619425, mean_q: 89.841603, mean_eps: 0.100000\n",
            " 50183/100000: episode: 334, duration: 2.308s, episode steps: 241, steps per second: 104, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.325690, mae: 44.587330, accuracy: 0.630187, mean_q: 89.861997, mean_eps: 0.100000\n",
            " 50431/100000: episode: 335, duration: 3.468s, episode steps: 248, steps per second:  72, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.606123, mae: 44.630796, accuracy: 0.635963, mean_q: 90.099919, mean_eps: 0.100000\n",
            " 50659/100000: episode: 336, duration: 3.337s, episode steps: 228, steps per second:  68, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.863703, mae: 44.425422, accuracy: 0.628015, mean_q: 89.478463, mean_eps: 0.100000\n",
            " 50920/100000: episode: 337, duration: 2.842s, episode steps: 261, steps per second:  92, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.800119, mae: 44.942000, accuracy: 0.628831, mean_q: 90.587329, mean_eps: 0.100000\n",
            " 51276/100000: episode: 338, duration: 3.425s, episode steps: 356, steps per second: 104, episode reward: 356.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.943263, mae: 44.818546, accuracy: 0.637026, mean_q: 90.429175, mean_eps: 0.100000\n",
            " 51581/100000: episode: 339, duration: 2.848s, episode steps: 305, steps per second: 107, episode reward: 305.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.426384, mae: 45.369248, accuracy: 0.636373, mean_q: 91.527090, mean_eps: 0.100000\n",
            " 51827/100000: episode: 340, duration: 2.424s, episode steps: 246, steps per second: 102, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.643366, mae: 45.172424, accuracy: 0.641641, mean_q: 91.124464, mean_eps: 0.100000\n",
            " 52088/100000: episode: 341, duration: 3.596s, episode steps: 261, steps per second:  73, episode reward: 261.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.096433, mae: 45.574017, accuracy: 0.635297, mean_q: 91.816274, mean_eps: 0.100000\n",
            " 52588/100000: episode: 342, duration: 6.103s, episode steps: 500, steps per second:  82, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.418037, mae: 45.739483, accuracy: 0.645500, mean_q: 92.321974, mean_eps: 0.100000\n",
            " 52868/100000: episode: 343, duration: 2.693s, episode steps: 280, steps per second: 104, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.437264, mae: 45.851039, accuracy: 0.646540, mean_q: 92.447557, mean_eps: 0.100000\n",
            " 53123/100000: episode: 344, duration: 2.426s, episode steps: 255, steps per second: 105, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.035886, mae: 46.381088, accuracy: 0.644608, mean_q: 93.617851, mean_eps: 0.100000\n",
            " 53623/100000: episode: 345, duration: 5.924s, episode steps: 500, steps per second:  84, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.101570, mae: 46.217747, accuracy: 0.632687, mean_q: 93.108805, mean_eps: 0.100000\n",
            " 54123/100000: episode: 346, duration: 6.359s, episode steps: 500, steps per second:  79, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.797208, mae: 46.356359, accuracy: 0.641437, mean_q: 93.435789, mean_eps: 0.100000\n",
            " 54623/100000: episode: 347, duration: 4.829s, episode steps: 500, steps per second: 104, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 2.228108, mae: 46.678732, accuracy: 0.646250, mean_q: 94.110061, mean_eps: 0.100000\n",
            " 55123/100000: episode: 348, duration: 5.626s, episode steps: 500, steps per second:  89, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.509923, mae: 47.060953, accuracy: 0.624125, mean_q: 94.858101, mean_eps: 0.100000\n",
            " 55623/100000: episode: 349, duration: 6.501s, episode steps: 500, steps per second:  77, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.349195, mae: 47.205225, accuracy: 0.615250, mean_q: 95.085493, mean_eps: 0.100000\n",
            " 56123/100000: episode: 350, duration: 4.918s, episode steps: 500, steps per second: 102, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 1.813038, mae: 47.232976, accuracy: 0.608125, mean_q: 95.098042, mean_eps: 0.100000\n",
            " 56623/100000: episode: 351, duration: 5.188s, episode steps: 500, steps per second:  96, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 4.546539, mae: 47.832440, accuracy: 0.595063, mean_q: 96.243747, mean_eps: 0.100000\n",
            " 57123/100000: episode: 352, duration: 6.859s, episode steps: 500, steps per second:  73, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.161877, mae: 48.177019, accuracy: 0.592375, mean_q: 96.981863, mean_eps: 0.100000\n",
            " 57623/100000: episode: 353, duration: 4.939s, episode steps: 500, steps per second: 101, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.682313, mae: 48.049202, accuracy: 0.600375, mean_q: 96.781766, mean_eps: 0.100000\n",
            " 58123/100000: episode: 354, duration: 5.104s, episode steps: 500, steps per second:  98, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.260362, mae: 48.550067, accuracy: 0.589938, mean_q: 97.676455, mean_eps: 0.100000\n",
            " 58623/100000: episode: 355, duration: 7.217s, episode steps: 500, steps per second:  69, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.630155, mae: 48.702313, accuracy: 0.591875, mean_q: 98.026891, mean_eps: 0.100000\n",
            " 59123/100000: episode: 356, duration: 4.727s, episode steps: 500, steps per second: 106, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.006043, mae: 48.779984, accuracy: 0.566375, mean_q: 98.225620, mean_eps: 0.100000\n",
            " 59623/100000: episode: 357, duration: 4.868s, episode steps: 500, steps per second: 103, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.753311, mae: 48.694543, accuracy: 0.574812, mean_q: 98.059083, mean_eps: 0.100000\n",
            " 60123/100000: episode: 358, duration: 7.049s, episode steps: 500, steps per second:  71, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.329987, mae: 49.327847, accuracy: 0.568000, mean_q: 99.356117, mean_eps: 0.100000\n",
            " 60623/100000: episode: 359, duration: 5.179s, episode steps: 500, steps per second:  97, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.575747, mae: 49.718893, accuracy: 0.552687, mean_q: 100.232058, mean_eps: 0.100000\n",
            " 61123/100000: episode: 360, duration: 4.862s, episode steps: 500, steps per second: 103, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.169577, mae: 50.186254, accuracy: 0.554250, mean_q: 101.101601, mean_eps: 0.100000\n",
            " 61623/100000: episode: 361, duration: 6.718s, episode steps: 500, steps per second:  74, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.743934, mae: 51.043625, accuracy: 0.560625, mean_q: 102.965923, mean_eps: 0.100000\n",
            " 62123/100000: episode: 362, duration: 5.392s, episode steps: 500, steps per second:  93, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.722621, mae: 51.336783, accuracy: 0.567562, mean_q: 103.360861, mean_eps: 0.100000\n",
            " 62623/100000: episode: 363, duration: 4.701s, episode steps: 500, steps per second: 106, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.017792, mae: 51.440996, accuracy: 0.579000, mean_q: 103.793597, mean_eps: 0.100000\n",
            " 63123/100000: episode: 364, duration: 6.381s, episode steps: 500, steps per second:  78, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.088144, mae: 51.975291, accuracy: 0.561813, mean_q: 104.890580, mean_eps: 0.100000\n",
            " 63623/100000: episode: 365, duration: 5.822s, episode steps: 500, steps per second:  86, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.460624, mae: 52.275456, accuracy: 0.569937, mean_q: 105.524807, mean_eps: 0.100000\n",
            " 64123/100000: episode: 366, duration: 4.720s, episode steps: 500, steps per second: 106, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.922871, mae: 52.769328, accuracy: 0.560063, mean_q: 106.465074, mean_eps: 0.100000\n",
            " 64465/100000: episode: 367, duration: 3.672s, episode steps: 342, steps per second:  93, episode reward: 342.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.614969, mae: 53.198977, accuracy: 0.551352, mean_q: 107.462241, mean_eps: 0.100000\n",
            " 64881/100000: episode: 368, duration: 6.270s, episode steps: 416, steps per second:  66, episode reward: 416.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 6.337328, mae: 53.634159, accuracy: 0.555288, mean_q: 108.151730, mean_eps: 0.100000\n",
            " 65381/100000: episode: 369, duration: 4.804s, episode steps: 500, steps per second: 104, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.889523, mae: 54.269291, accuracy: 0.549875, mean_q: 109.498926, mean_eps: 0.100000\n",
            " 65667/100000: episode: 370, duration: 2.820s, episode steps: 286, steps per second: 101, episode reward: 286.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 7.867455, mae: 54.004353, accuracy: 0.572552, mean_q: 108.900440, mean_eps: 0.100000\n",
            " 66003/100000: episode: 371, duration: 3.711s, episode steps: 336, steps per second:  91, episode reward: 336.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.133189, mae: 54.271510, accuracy: 0.563988, mean_q: 109.613043, mean_eps: 0.100000\n",
            " 66503/100000: episode: 372, duration: 7.037s, episode steps: 500, steps per second:  71, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4.285024, mae: 54.417212, accuracy: 0.569438, mean_q: 109.991031, mean_eps: 0.100000\n",
            " 66762/100000: episode: 373, duration: 2.539s, episode steps: 259, steps per second: 102, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 7.519064, mae: 55.071327, accuracy: 0.567447, mean_q: 111.107010, mean_eps: 0.100000\n",
            " 67121/100000: episode: 374, duration: 3.542s, episode steps: 359, steps per second: 101, episode reward: 359.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.220815, mae: 55.076208, accuracy: 0.567114, mean_q: 111.105804, mean_eps: 0.100000\n",
            " 67379/100000: episode: 375, duration: 2.553s, episode steps: 258, steps per second: 101, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.639465, mae: 55.044346, accuracy: 0.559109, mean_q: 111.306159, mean_eps: 0.100000\n",
            " 67593/100000: episode: 376, duration: 2.744s, episode steps: 214, steps per second:  78, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.206596, mae: 54.974887, accuracy: 0.561624, mean_q: 111.099003, mean_eps: 0.100000\n",
            " 67830/100000: episode: 377, duration: 3.607s, episode steps: 237, steps per second:  66, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.708973, mae: 55.449385, accuracy: 0.569884, mean_q: 112.096145, mean_eps: 0.100000\n",
            " 68074/100000: episode: 378, duration: 3.126s, episode steps: 244, steps per second:  78, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4.031886, mae: 55.633257, accuracy: 0.561860, mean_q: 112.437245, mean_eps: 0.100000\n",
            " 68301/100000: episode: 379, duration: 2.222s, episode steps: 227, steps per second: 102, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 7.331906, mae: 55.761511, accuracy: 0.569108, mean_q: 112.645564, mean_eps: 0.100000\n",
            " 68534/100000: episode: 380, duration: 2.294s, episode steps: 233, steps per second: 102, episode reward: 233.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 10.229204, mae: 55.720164, accuracy: 0.590263, mean_q: 112.401604, mean_eps: 0.100000\n",
            " 68797/100000: episode: 381, duration: 2.635s, episode steps: 263, steps per second: 100, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.189944, mae: 55.850574, accuracy: 0.587571, mean_q: 112.844167, mean_eps: 0.100000\n",
            " 69051/100000: episode: 382, duration: 2.827s, episode steps: 254, steps per second:  90, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.624550, mae: 56.002994, accuracy: 0.572589, mean_q: 113.075353, mean_eps: 0.100000\n",
            " 69281/100000: episode: 383, duration: 3.418s, episode steps: 230, steps per second:  67, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 5.446772, mae: 55.722236, accuracy: 0.593342, mean_q: 112.625717, mean_eps: 0.100000\n",
            " 69486/100000: episode: 384, duration: 3.105s, episode steps: 205, steps per second:  66, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.985362, mae: 56.265903, accuracy: 0.576982, mean_q: 113.433767, mean_eps: 0.100000\n",
            " 69712/100000: episode: 385, duration: 2.287s, episode steps: 226, steps per second:  99, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2.911653, mae: 56.484145, accuracy: 0.571488, mean_q: 114.263673, mean_eps: 0.100000\n",
            " 69952/100000: episode: 386, duration: 2.460s, episode steps: 240, steps per second:  98, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.360000, mae: 56.510752, accuracy: 0.568099, mean_q: 114.255200, mean_eps: 0.100000\n",
            " 70160/100000: episode: 387, duration: 2.093s, episode steps: 208, steps per second:  99, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.076153, mae: 56.233878, accuracy: 0.558444, mean_q: 113.800792, mean_eps: 0.100000\n",
            " 70381/100000: episode: 388, duration: 2.205s, episode steps: 221, steps per second: 100, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.237014, mae: 56.921948, accuracy: 0.564621, mean_q: 115.157779, mean_eps: 0.100000\n",
            " 70578/100000: episode: 389, duration: 3.442s, episode steps: 197, steps per second:  57, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.763741, mae: 57.336289, accuracy: 0.568687, mean_q: 115.876701, mean_eps: 0.100000\n",
            " 70804/100000: episode: 390, duration: 7.320s, episode steps: 226, steps per second:  31, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 9.735089, mae: 57.121224, accuracy: 0.575636, mean_q: 115.279024, mean_eps: 0.100000\n",
            " 71016/100000: episode: 391, duration: 2.979s, episode steps: 212, steps per second:  71, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2.013407, mae: 57.480705, accuracy: 0.561616, mean_q: 116.446762, mean_eps: 0.100000\n",
            " 71229/100000: episode: 392, duration: 2.145s, episode steps: 213, steps per second:  99, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 1.927295, mae: 58.024304, accuracy: 0.563380, mean_q: 117.407157, mean_eps: 0.100000\n",
            " 71421/100000: episode: 393, duration: 1.860s, episode steps: 192, steps per second: 103, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.982151, mae: 58.952007, accuracy: 0.572591, mean_q: 119.367274, mean_eps: 0.100000\n",
            " 71626/100000: episode: 394, duration: 2.018s, episode steps: 205, steps per second: 102, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.616817, mae: 58.502430, accuracy: 0.580030, mean_q: 118.396966, mean_eps: 0.100000\n",
            " 71820/100000: episode: 395, duration: 1.954s, episode steps: 194, steps per second:  99, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2.126773, mae: 59.053722, accuracy: 0.567655, mean_q: 119.679807, mean_eps: 0.100000\n",
            " 72030/100000: episode: 396, duration: 2.305s, episode steps: 210, steps per second:  91, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.449277, mae: 59.667814, accuracy: 0.577381, mean_q: 120.875690, mean_eps: 0.100000\n",
            " 72217/100000: episode: 397, duration: 2.711s, episode steps: 187, steps per second:  69, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 5.297917, mae: 59.702298, accuracy: 0.594418, mean_q: 120.959761, mean_eps: 0.100000\n",
            " 72459/100000: episode: 398, duration: 3.626s, episode steps: 242, steps per second:  67, episode reward: 242.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6.351531, mae: 59.913909, accuracy: 0.584323, mean_q: 121.115098, mean_eps: 0.100000\n",
            " 72669/100000: episode: 399, duration: 2.340s, episode steps: 210, steps per second:  90, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.271213, mae: 59.977905, accuracy: 0.584226, mean_q: 121.245277, mean_eps: 0.100000\n",
            " 72883/100000: episode: 400, duration: 2.191s, episode steps: 214, steps per second:  98, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 11.613942, mae: 59.978295, accuracy: 0.569071, mean_q: 120.936439, mean_eps: 0.100000\n",
            " 73080/100000: episode: 401, duration: 2.017s, episode steps: 197, steps per second:  98, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 9.565673, mae: 60.272944, accuracy: 0.578680, mean_q: 121.736171, mean_eps: 0.100000\n",
            " 73336/100000: episode: 402, duration: 2.587s, episode steps: 256, steps per second:  99, episode reward: 256.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.486115, mae: 60.581558, accuracy: 0.566284, mean_q: 122.301346, mean_eps: 0.100000\n",
            " 73550/100000: episode: 403, duration: 2.414s, episode steps: 214, steps per second:  89, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 2.130651, mae: 60.582134, accuracy: 0.582506, mean_q: 122.542259, mean_eps: 0.100000\n",
            " 73749/100000: episode: 404, duration: 2.881s, episode steps: 199, steps per second:  69, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.798357, mae: 60.989581, accuracy: 0.579303, mean_q: 123.234197, mean_eps: 0.100000\n",
            " 73959/100000: episode: 405, duration: 3.170s, episode steps: 210, steps per second:  66, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.458886, mae: 61.073865, accuracy: 0.592262, mean_q: 123.658609, mean_eps: 0.100000\n",
            " 74173/100000: episode: 406, duration: 2.379s, episode steps: 214, steps per second:  90, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.110178, mae: 61.261237, accuracy: 0.590245, mean_q: 123.660494, mean_eps: 0.100000\n",
            " 74360/100000: episode: 407, duration: 1.869s, episode steps: 187, steps per second: 100, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.871127, mae: 61.058962, accuracy: 0.581718, mean_q: 123.178702, mean_eps: 0.100000\n",
            " 74560/100000: episode: 408, duration: 1.999s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 5.615143, mae: 61.211550, accuracy: 0.581562, mean_q: 123.458111, mean_eps: 0.100000\n",
            " 74786/100000: episode: 409, duration: 2.252s, episode steps: 226, steps per second: 100, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 10.853693, mae: 60.945911, accuracy: 0.592506, mean_q: 122.678671, mean_eps: 0.100000\n",
            " 74976/100000: episode: 410, duration: 1.903s, episode steps: 190, steps per second: 100, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 5.304238, mae: 60.788869, accuracy: 0.598191, mean_q: 122.848526, mean_eps: 0.100000\n",
            " 75159/100000: episode: 411, duration: 2.421s, episode steps: 183, steps per second:  76, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.454 [0.000, 1.000],  loss: 4.960409, mae: 61.120778, accuracy: 0.596141, mean_q: 123.578157, mean_eps: 0.100000\n",
            " 75353/100000: episode: 412, duration: 2.990s, episode steps: 194, steps per second:  65, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.723806, mae: 61.070228, accuracy: 0.580863, mean_q: 123.343318, mean_eps: 0.100000\n",
            " 75560/100000: episode: 413, duration: 3.016s, episode steps: 207, steps per second:  69, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 4.067195, mae: 61.262352, accuracy: 0.579257, mean_q: 123.944894, mean_eps: 0.100000\n",
            " 75754/100000: episode: 414, duration: 2.012s, episode steps: 194, steps per second:  96, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 11.951561, mae: 61.075127, accuracy: 0.592622, mean_q: 123.416250, mean_eps: 0.100000\n",
            " 75916/100000: episode: 415, duration: 2.577s, episode steps: 162, steps per second:  63, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 2.562448, mae: 60.985138, accuracy: 0.578897, mean_q: 123.547464, mean_eps: 0.100000\n",
            " 76102/100000: episode: 416, duration: 3.807s, episode steps: 186, steps per second:  49, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.720638, mae: 61.361215, accuracy: 0.570733, mean_q: 124.191784, mean_eps: 0.100000\n",
            " 76293/100000: episode: 417, duration: 2.381s, episode steps: 191, steps per second:  80, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 5.923664, mae: 62.069058, accuracy: 0.580170, mean_q: 125.579983, mean_eps: 0.100000\n",
            " 76481/100000: episode: 418, duration: 2.898s, episode steps: 188, steps per second:  65, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 9.490112, mae: 62.091674, accuracy: 0.581782, mean_q: 125.320794, mean_eps: 0.100000\n",
            " 76645/100000: episode: 419, duration: 2.583s, episode steps: 164, steps per second:  64, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2.341854, mae: 62.361571, accuracy: 0.582317, mean_q: 126.202913, mean_eps: 0.100000\n",
            " 76811/100000: episode: 420, duration: 2.070s, episode steps: 166, steps per second:  80, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 3.966506, mae: 62.085900, accuracy: 0.568336, mean_q: 125.738072, mean_eps: 0.100000\n",
            " 76980/100000: episode: 421, duration: 1.677s, episode steps: 169, steps per second: 101, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7.300504, mae: 62.258815, accuracy: 0.575629, mean_q: 125.812306, mean_eps: 0.100000\n",
            " 77157/100000: episode: 422, duration: 1.826s, episode steps: 177, steps per second:  97, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 4.593189, mae: 62.439960, accuracy: 0.589513, mean_q: 126.257722, mean_eps: 0.100000\n",
            " 77316/100000: episode: 423, duration: 1.583s, episode steps: 159, steps per second: 100, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 4.193601, mae: 63.195678, accuracy: 0.584906, mean_q: 127.911347, mean_eps: 0.100000\n",
            " 77497/100000: episode: 424, duration: 1.852s, episode steps: 181, steps per second:  98, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.145135, mae: 62.696502, accuracy: 0.597548, mean_q: 126.598897, mean_eps: 0.100000\n",
            " 77673/100000: episode: 425, duration: 1.789s, episode steps: 176, steps per second:  98, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 5.307948, mae: 63.220849, accuracy: 0.590554, mean_q: 127.737551, mean_eps: 0.100000\n",
            " 77887/100000: episode: 426, duration: 2.991s, episode steps: 214, steps per second:  72, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.521009, mae: 62.923096, accuracy: 0.598131, mean_q: 127.145068, mean_eps: 0.100000\n",
            " 78115/100000: episode: 427, duration: 3.554s, episode steps: 228, steps per second:  64, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.573662, mae: 62.581781, accuracy: 0.621985, mean_q: 126.362722, mean_eps: 0.100000\n",
            " 78355/100000: episode: 428, duration: 2.891s, episode steps: 240, steps per second:  83, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 6.420129, mae: 63.052999, accuracy: 0.608464, mean_q: 127.222163, mean_eps: 0.100000\n",
            " 78689/100000: episode: 429, duration: 3.333s, episode steps: 334, steps per second: 100, episode reward: 334.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.626908, mae: 62.227788, accuracy: 0.610217, mean_q: 125.752700, mean_eps: 0.100000\n",
            " 79023/100000: episode: 430, duration: 3.355s, episode steps: 334, steps per second: 100, episode reward: 334.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 7.694216, mae: 62.992456, accuracy: 0.613492, mean_q: 127.360145, mean_eps: 0.100000\n",
            " 79248/100000: episode: 431, duration: 2.468s, episode steps: 225, steps per second:  91, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 8.060654, mae: 62.724012, accuracy: 0.593611, mean_q: 126.667011, mean_eps: 0.100000\n",
            " 79424/100000: episode: 432, duration: 2.675s, episode steps: 176, steps per second:  66, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.506780, mae: 62.856895, accuracy: 0.605646, mean_q: 127.316261, mean_eps: 0.100000\n",
            " 79637/100000: episode: 433, duration: 3.396s, episode steps: 213, steps per second:  63, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.743449, mae: 62.999630, accuracy: 0.603873, mean_q: 127.481517, mean_eps: 0.100000\n",
            " 79821/100000: episode: 434, duration: 2.258s, episode steps: 184, steps per second:  81, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 6.485402, mae: 63.661472, accuracy: 0.589164, mean_q: 128.647040, mean_eps: 0.100000\n",
            " 80050/100000: episode: 435, duration: 2.345s, episode steps: 229, steps per second:  98, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.259913, mae: 63.331072, accuracy: 0.600027, mean_q: 127.963338, mean_eps: 0.100000\n",
            " 80231/100000: episode: 436, duration: 1.882s, episode steps: 181, steps per second:  96, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 9.693727, mae: 63.733625, accuracy: 0.598239, mean_q: 128.635849, mean_eps: 0.100000\n",
            " 80518/100000: episode: 437, duration: 2.947s, episode steps: 287, steps per second:  97, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6.146950, mae: 64.054536, accuracy: 0.591572, mean_q: 129.493096, mean_eps: 0.100000\n",
            " 81018/100000: episode: 438, duration: 6.945s, episode steps: 500, steps per second:  72, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.212070, mae: 64.965487, accuracy: 0.595625, mean_q: 131.225909, mean_eps: 0.100000\n",
            " 81308/100000: episode: 439, duration: 3.878s, episode steps: 290, steps per second:  75, episode reward: 290.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 5.760181, mae: 64.486949, accuracy: 0.601185, mean_q: 130.321056, mean_eps: 0.100000\n",
            " 81808/100000: episode: 440, duration: 5.262s, episode steps: 500, steps per second:  95, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.475273, mae: 64.879764, accuracy: 0.609625, mean_q: 130.936257, mean_eps: 0.100000\n",
            " 82057/100000: episode: 441, duration: 2.577s, episode steps: 249, steps per second:  97, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.348646, mae: 65.136878, accuracy: 0.597264, mean_q: 131.428568, mean_eps: 0.100000\n",
            " 82267/100000: episode: 442, duration: 2.850s, episode steps: 210, steps per second:  74, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.376719, mae: 64.534423, accuracy: 0.612946, mean_q: 130.088518, mean_eps: 0.100000\n",
            " 82486/100000: episode: 443, duration: 3.474s, episode steps: 219, steps per second:  63, episode reward: 219.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 9.973093, mae: 64.371611, accuracy: 0.594892, mean_q: 129.451969, mean_eps: 0.100000\n",
            " 82986/100000: episode: 444, duration: 5.876s, episode steps: 500, steps per second:  85, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.675558, mae: 64.037719, accuracy: 0.603750, mean_q: 129.005747, mean_eps: 0.100000\n",
            " 83147/100000: episode: 445, duration: 1.734s, episode steps: 161, steps per second:  93, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 12.643253, mae: 64.193886, accuracy: 0.595691, mean_q: 129.123013, mean_eps: 0.100000\n",
            " 83304/100000: episode: 446, duration: 1.720s, episode steps: 157, steps per second:  91, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 16.635748, mae: 63.842036, accuracy: 0.591959, mean_q: 128.057193, mean_eps: 0.100000\n",
            " 83474/100000: episode: 447, duration: 1.955s, episode steps: 170, steps per second:  87, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 4.312360, mae: 63.875807, accuracy: 0.581434, mean_q: 128.539770, mean_eps: 0.100000\n",
            " 83645/100000: episode: 448, duration: 3.003s, episode steps: 171, steps per second:  57, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 5.710414, mae: 63.512393, accuracy: 0.594846, mean_q: 127.984843, mean_eps: 0.100000\n",
            " 83825/100000: episode: 449, duration: 2.891s, episode steps: 180, steps per second:  62, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 11.153745, mae: 63.759410, accuracy: 0.599653, mean_q: 128.032421, mean_eps: 0.100000\n",
            " 84037/100000: episode: 450, duration: 3.121s, episode steps: 212, steps per second:  68, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 5.580473, mae: 62.619319, accuracy: 0.608638, mean_q: 125.765920, mean_eps: 0.100000\n",
            " 84200/100000: episode: 451, duration: 1.737s, episode steps: 163, steps per second:  94, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 9.375130, mae: 62.846776, accuracy: 0.607170, mean_q: 126.264873, mean_eps: 0.100000\n",
            " 84614/100000: episode: 452, duration: 4.347s, episode steps: 414, steps per second:  95, episode reward: 414.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7.656651, mae: 62.365020, accuracy: 0.612923, mean_q: 125.249197, mean_eps: 0.100000\n",
            " 84798/100000: episode: 453, duration: 1.917s, episode steps: 184, steps per second:  96, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 4.874609, mae: 62.227198, accuracy: 0.610564, mean_q: 125.117802, mean_eps: 0.100000\n",
            " 84983/100000: episode: 454, duration: 2.298s, episode steps: 185, steps per second:  81, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 10.706449, mae: 61.473751, accuracy: 0.610473, mean_q: 123.471750, mean_eps: 0.100000\n",
            " 85331/100000: episode: 455, duration: 5.661s, episode steps: 348, steps per second:  61, episode reward: 348.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 6.990614, mae: 61.388048, accuracy: 0.620690, mean_q: 123.427310, mean_eps: 0.100000\n",
            " 85500/100000: episode: 456, duration: 2.172s, episode steps: 169, steps per second:  78, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 7.329491, mae: 61.121566, accuracy: 0.595599, mean_q: 123.032219, mean_eps: 0.100000\n",
            " 85649/100000: episode: 457, duration: 1.568s, episode steps: 149, steps per second:  95, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.987318, mae: 60.988397, accuracy: 0.594169, mean_q: 122.758503, mean_eps: 0.100000\n",
            " 85816/100000: episode: 458, duration: 1.813s, episode steps: 167, steps per second:  92, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7.318000, mae: 60.884947, accuracy: 0.604603, mean_q: 122.246424, mean_eps: 0.100000\n",
            " 86074/100000: episode: 459, duration: 2.808s, episode steps: 258, steps per second:  92, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.656478, mae: 60.320822, accuracy: 0.624516, mean_q: 121.310950, mean_eps: 0.100000\n",
            " 86334/100000: episode: 460, duration: 2.834s, episode steps: 260, steps per second:  92, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 6.110672, mae: 60.355369, accuracy: 0.611538, mean_q: 121.351810, mean_eps: 0.100000\n",
            " 86496/100000: episode: 461, duration: 2.418s, episode steps: 162, steps per second:  67, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 7.612122, mae: 59.481961, accuracy: 0.616319, mean_q: 119.532891, mean_eps: 0.100000\n",
            " 86660/100000: episode: 462, duration: 2.628s, episode steps: 164, steps per second:  62, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 7.706892, mae: 59.638552, accuracy: 0.609375, mean_q: 119.644189, mean_eps: 0.100000\n",
            " 86790/100000: episode: 463, duration: 2.019s, episode steps: 130, steps per second:  64, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.416256, mae: 58.892299, accuracy: 0.603846, mean_q: 118.322307, mean_eps: 0.100000\n",
            " 86945/100000: episode: 464, duration: 1.721s, episode steps: 155, steps per second:  90, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 7.260338, mae: 59.066417, accuracy: 0.607661, mean_q: 118.523687, mean_eps: 0.100000\n",
            " 87445/100000: episode: 465, duration: 5.072s, episode steps: 500, steps per second:  99, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.875385, mae: 58.195096, accuracy: 0.597562, mean_q: 116.897410, mean_eps: 0.100000\n",
            " 87945/100000: episode: 466, duration: 5.788s, episode steps: 500, steps per second:  86, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.836179, mae: 57.849593, accuracy: 0.607875, mean_q: 116.228170, mean_eps: 0.100000\n",
            " 88112/100000: episode: 467, duration: 2.711s, episode steps: 167, steps per second:  62, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 8.564128, mae: 57.220565, accuracy: 0.597867, mean_q: 114.884453, mean_eps: 0.100000\n",
            " 88612/100000: episode: 468, duration: 6.143s, episode steps: 500, steps per second:  81, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.100552, mae: 56.898195, accuracy: 0.615187, mean_q: 114.389960, mean_eps: 0.100000\n",
            " 89112/100000: episode: 469, duration: 5.154s, episode steps: 500, steps per second:  97, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.045960, mae: 56.536711, accuracy: 0.626313, mean_q: 113.516852, mean_eps: 0.100000\n",
            " 89612/100000: episode: 470, duration: 7.105s, episode steps: 500, steps per second:  70, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.098564, mae: 55.340895, accuracy: 0.619125, mean_q: 111.224148, mean_eps: 0.100000\n",
            " 90112/100000: episode: 471, duration: 5.934s, episode steps: 500, steps per second:  84, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 5.842891, mae: 54.741381, accuracy: 0.607625, mean_q: 109.945115, mean_eps: 0.100000\n",
            " 90462/100000: episode: 472, duration: 3.696s, episode steps: 350, steps per second:  95, episode reward: 350.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 4.095878, mae: 54.611105, accuracy: 0.603304, mean_q: 109.756674, mean_eps: 0.100000\n",
            " 90962/100000: episode: 473, duration: 6.682s, episode steps: 500, steps per second:  75, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.492080, mae: 54.593838, accuracy: 0.604062, mean_q: 109.764733, mean_eps: 0.100000\n",
            " 91462/100000: episode: 474, duration: 6.348s, episode steps: 500, steps per second:  79, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.040749, mae: 54.276108, accuracy: 0.605812, mean_q: 109.105183, mean_eps: 0.100000\n",
            " 91962/100000: episode: 475, duration: 5.157s, episode steps: 500, steps per second:  97, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.833110, mae: 53.671454, accuracy: 0.601437, mean_q: 107.798469, mean_eps: 0.100000\n",
            " 92462/100000: episode: 476, duration: 6.851s, episode steps: 500, steps per second:  73, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.426596, mae: 53.416173, accuracy: 0.600562, mean_q: 107.301309, mean_eps: 0.100000\n",
            " 92962/100000: episode: 477, duration: 6.124s, episode steps: 500, steps per second:  82, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.751947, mae: 53.310355, accuracy: 0.598375, mean_q: 107.246086, mean_eps: 0.100000\n",
            " 93462/100000: episode: 478, duration: 5.259s, episode steps: 500, steps per second:  95, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.693362, mae: 53.520633, accuracy: 0.596437, mean_q: 107.715122, mean_eps: 0.100000\n",
            " 93962/100000: episode: 479, duration: 7.578s, episode steps: 500, steps per second:  66, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.827619, mae: 53.483636, accuracy: 0.598125, mean_q: 107.646142, mean_eps: 0.100000\n",
            " 94462/100000: episode: 480, duration: 5.641s, episode steps: 500, steps per second:  89, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.312278, mae: 53.768935, accuracy: 0.603125, mean_q: 108.268288, mean_eps: 0.100000\n",
            " 94962/100000: episode: 481, duration: 5.370s, episode steps: 500, steps per second:  93, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.378292, mae: 54.223417, accuracy: 0.602812, mean_q: 109.369016, mean_eps: 0.100000\n",
            " 95462/100000: episode: 482, duration: 7.800s, episode steps: 500, steps per second:  64, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.759448, mae: 55.155327, accuracy: 0.601938, mean_q: 111.007423, mean_eps: 0.100000\n",
            " 95962/100000: episode: 483, duration: 5.389s, episode steps: 500, steps per second:  93, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.592100, mae: 55.684521, accuracy: 0.605875, mean_q: 112.275491, mean_eps: 0.100000\n",
            " 96462/100000: episode: 484, duration: 5.829s, episode steps: 500, steps per second:  86, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.125260, mae: 56.442281, accuracy: 0.612125, mean_q: 113.759908, mean_eps: 0.100000\n",
            " 96962/100000: episode: 485, duration: 7.329s, episode steps: 500, steps per second:  68, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.604650, mae: 57.417340, accuracy: 0.602688, mean_q: 115.797307, mean_eps: 0.100000\n",
            " 97462/100000: episode: 486, duration: 5.386s, episode steps: 500, steps per second:  93, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.753837, mae: 58.149524, accuracy: 0.583375, mean_q: 117.333151, mean_eps: 0.100000\n",
            " 97962/100000: episode: 487, duration: 6.171s, episode steps: 500, steps per second:  81, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.849270, mae: 59.229288, accuracy: 0.577000, mean_q: 119.497394, mean_eps: 0.100000\n",
            " 98462/100000: episode: 488, duration: 7.148s, episode steps: 500, steps per second:  70, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.751739, mae: 60.155727, accuracy: 0.574688, mean_q: 121.681413, mean_eps: 0.100000\n",
            " 98474/100000: episode: 489, duration: 0.151s, episode steps:  12, steps per second:  79, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.756460, mae: 61.034416, accuracy: 0.565104, mean_q: 123.777640, mean_eps: 0.100000\n",
            " 98974/100000: episode: 490, duration: 5.353s, episode steps: 500, steps per second:  93, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.467719, mae: 61.315099, accuracy: 0.581625, mean_q: 124.039420, mean_eps: 0.100000\n",
            " 99474/100000: episode: 491, duration: 6.952s, episode steps: 500, steps per second:  72, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.495196, mae: 62.699283, accuracy: 0.587313, mean_q: 126.811836, mean_eps: 0.100000\n",
            " 99974/100000: episode: 492, duration: 6.325s, episode steps: 500, steps per second:  79, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.087932, mae: 63.942138, accuracy: 0.583438, mean_q: 129.240519, mean_eps: 0.100000\n",
            "done, took 1153.147 seconds\n"
          ]
        }
      ],
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "\n",
        "# Define the maximum capacity of the experience replay buffer to be 100,000 transitions\n",
        "memory = SequentialMemory(limit=100000, window_length=1)\n",
        "\n",
        "# Setup the policy to use for action selection\n",
        "inner_policy = EpsGreedyQPolicy() # policy used \n",
        "policy = LinearAnnealedPolicy(inner_policy=inner_policy, \n",
        "                              attr='eps',       # epsilon in the inner policy to change  \n",
        "                              value_max=1.0,    # maximum value of attribute that is varying\n",
        "                              value_min=0.1,    # minimum value of attribute that is varying\n",
        "                              value_test=0.01,  # test if the value selected is < 0.01\n",
        "                              nb_steps=10000)   # 10000 steps between value_max and value_min\n",
        "\n",
        "# Define the neural network model for Deep Q Learning (DQN)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(1, env.observation_space.shape[0])),\n",
        "    Flatten(),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(env.action_space.n, activation='linear')\n",
        "])\n",
        "\n",
        "# Print the neural network architecture summary\n",
        "print(model.summary())\n",
        "\n",
        "# Define the DQN agent\n",
        "agent = DQNAgent(model=model,\n",
        "                 nb_actions=env.action_space.n,\n",
        "                 memory=memory,\n",
        "                 nb_steps_warmup=30,\n",
        "                 target_model_update=1e-2,\n",
        "                 policy=policy)\n",
        "\n",
        "# Compile the agent with the legacy Adam optimizer and metrics to track\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
        "metrics = ['mae', 'accuracy']\n",
        "agent.compile(optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "# Train the agent with 100,000 training steps\n",
        "history = agent.fit(env, nb_steps=100000, visualize=False, verbose=2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PWoGbP-E64Au"
      },
      "source": [
        "The code below shows the agent's decision for a given observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pu00m3IzQ_aN",
        "outputId": "5682bf55-4c7d-4b9d-cb7a-831c245cea3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation: [-0.04304064 -0.00265329 -0.00938949 -0.0178334 ]\n",
            "Chosen action: 1\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "action = agent.forward(observation)\n",
        "print(\"Observation:\", observation)\n",
        "print(\"Chosen action:\", action)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6OfNxPyt6971"
      },
      "source": [
        "And a full episode, the accumulated reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X8ojyZcT318",
        "outputId": "89938752-b9ec-4940-c7b7-1974dab6728e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative reward for this round: 500.0\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    observation, reward, done, info = env.step(agent.forward(observation))\n",
        "    cumulative_reward += reward\n",
        "print(\"Cumulative reward for this round:\", cumulative_reward)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtIQ0Rti1gm"
      },
      "source": [
        "## Task 2: Demonstrate the effectiveness of the RL agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OAwlfF3LBV0D"
      },
      "source": [
        "Here we run the agent again for 100 times, and append the reward for each round to a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRGBPweHXc7h",
        "outputId": "92d28009-0254-4559-cdf8-e9d2bdf0335f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative reward for round 1: 500.0\n",
            "Cumulative reward for round 2: 500.0\n",
            "Cumulative reward for round 3: 500.0\n",
            "Cumulative reward for round 4: 500.0\n",
            "Cumulative reward for round 5: 500.0\n",
            "Cumulative reward for round 6: 500.0\n",
            "Cumulative reward for round 7: 500.0\n",
            "Cumulative reward for round 8: 500.0\n",
            "Cumulative reward for round 9: 500.0\n",
            "Cumulative reward for round 10: 500.0\n",
            "Cumulative reward for round 11: 500.0\n",
            "Cumulative reward for round 12: 500.0\n",
            "Cumulative reward for round 13: 500.0\n",
            "Cumulative reward for round 14: 500.0\n",
            "Cumulative reward for round 15: 500.0\n",
            "Cumulative reward for round 16: 500.0\n",
            "Cumulative reward for round 17: 500.0\n",
            "Cumulative reward for round 18: 500.0\n",
            "Cumulative reward for round 19: 500.0\n",
            "Cumulative reward for round 20: 500.0\n",
            "Cumulative reward for round 21: 500.0\n",
            "Cumulative reward for round 22: 500.0\n",
            "Cumulative reward for round 23: 500.0\n",
            "Cumulative reward for round 24: 500.0\n",
            "Cumulative reward for round 25: 500.0\n",
            "Cumulative reward for round 26: 500.0\n",
            "Cumulative reward for round 27: 500.0\n",
            "Cumulative reward for round 28: 500.0\n",
            "Cumulative reward for round 29: 500.0\n",
            "Cumulative reward for round 30: 500.0\n",
            "Cumulative reward for round 31: 500.0\n",
            "Cumulative reward for round 32: 500.0\n",
            "Cumulative reward for round 33: 500.0\n",
            "Cumulative reward for round 34: 500.0\n",
            "Cumulative reward for round 35: 500.0\n",
            "Cumulative reward for round 36: 500.0\n",
            "Cumulative reward for round 37: 500.0\n",
            "Cumulative reward for round 38: 500.0\n",
            "Cumulative reward for round 39: 500.0\n",
            "Cumulative reward for round 40: 500.0\n",
            "Cumulative reward for round 41: 500.0\n",
            "Cumulative reward for round 42: 500.0\n",
            "Cumulative reward for round 43: 500.0\n",
            "Cumulative reward for round 44: 500.0\n",
            "Cumulative reward for round 45: 500.0\n",
            "Cumulative reward for round 46: 500.0\n",
            "Cumulative reward for round 47: 500.0\n",
            "Cumulative reward for round 48: 500.0\n",
            "Cumulative reward for round 49: 500.0\n",
            "Cumulative reward for round 50: 500.0\n",
            "Cumulative reward for round 51: 500.0\n",
            "Cumulative reward for round 52: 500.0\n",
            "Cumulative reward for round 53: 500.0\n",
            "Cumulative reward for round 54: 500.0\n",
            "Cumulative reward for round 55: 500.0\n",
            "Cumulative reward for round 56: 500.0\n",
            "Cumulative reward for round 57: 500.0\n",
            "Cumulative reward for round 58: 500.0\n",
            "Cumulative reward for round 59: 500.0\n",
            "Cumulative reward for round 60: 500.0\n",
            "Cumulative reward for round 61: 500.0\n",
            "Cumulative reward for round 62: 500.0\n",
            "Cumulative reward for round 63: 500.0\n",
            "Cumulative reward for round 64: 500.0\n",
            "Cumulative reward for round 65: 500.0\n",
            "Cumulative reward for round 66: 500.0\n",
            "Cumulative reward for round 67: 500.0\n",
            "Cumulative reward for round 68: 500.0\n",
            "Cumulative reward for round 69: 500.0\n",
            "Cumulative reward for round 70: 500.0\n",
            "Cumulative reward for round 71: 500.0\n",
            "Cumulative reward for round 72: 500.0\n",
            "Cumulative reward for round 73: 500.0\n",
            "Cumulative reward for round 74: 500.0\n",
            "Cumulative reward for round 75: 500.0\n",
            "Cumulative reward for round 76: 500.0\n",
            "Cumulative reward for round 77: 500.0\n",
            "Cumulative reward for round 78: 500.0\n",
            "Cumulative reward for round 79: 500.0\n",
            "Cumulative reward for round 80: 500.0\n",
            "Cumulative reward for round 81: 500.0\n",
            "Cumulative reward for round 82: 500.0\n",
            "Cumulative reward for round 83: 500.0\n",
            "Cumulative reward for round 84: 500.0\n",
            "Cumulative reward for round 85: 500.0\n",
            "Cumulative reward for round 86: 500.0\n",
            "Cumulative reward for round 87: 500.0\n",
            "Cumulative reward for round 88: 500.0\n",
            "Cumulative reward for round 89: 500.0\n",
            "Cumulative reward for round 90: 500.0\n",
            "Cumulative reward for round 91: 500.0\n",
            "Cumulative reward for round 92: 500.0\n",
            "Cumulative reward for round 93: 500.0\n",
            "Cumulative reward for round 94: 500.0\n",
            "Cumulative reward for round 95: 500.0\n",
            "Cumulative reward for round 96: 500.0\n",
            "Cumulative reward for round 97: 500.0\n",
            "Cumulative reward for round 98: 500.0\n",
            "Cumulative reward for round 99: 500.0\n",
            "Cumulative reward for round 100: 500.0\n"
          ]
        }
      ],
      "source": [
        "raw_episode_results = []\n",
        "no_episodes = 1\n",
        "while no_episodes <=100:\n",
        "  observation = env.reset()\n",
        "  cumulative_reward = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "    observation, reward, done, info = env.step(agent.forward(observation))\n",
        "    cumulative_reward += reward\n",
        "\n",
        "  raw_episode_results.append(cumulative_reward)\n",
        "  print(f\"Cumulative reward for round {no_episodes}:\", cumulative_reward)\n",
        "  no_episodes+=1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Tg3PZlhJBV0I",
        "outputId": "30ff9ca3-0c7a-4be8-d8a3-4d0de7bf337c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "episode_results = np.array(raw_episode_results)\n",
        "print(raw_episode_results)\n",
        "plt.plot(episode_results)\n",
        "plt.title('Cumulative reward for each episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.xlabel('episode')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t70aYnpwBV0I"
      },
      "source": [
        "The average reward for the DQN agent is >195."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poomlq8LBV0I",
        "outputId": "f04798ff-3557-4228-f7f5-0d82efe081b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average cumulative reward: 500.0\n",
            "Is my agent good enough? True\n"
          ]
        }
      ],
      "source": [
        "print(\"Average cumulative reward:\", episode_results.mean())\n",
        "print(\"Is my agent good enough?\", episode_results.mean() > 195)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0DCT38lFA6"
      },
      "source": [
        "## Task 3: Render one episode played by the agent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1awMr9lc_w"
      },
      "source": [
        "Plug your agent to the code below to obtain rendered result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "k2S2xrvwBytO",
        "outputId": "56808d6b-d373-492a-a97f-3f8bd7df9472"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAaF9tZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1NSByMjkxNyAwYTg0ZDk4IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAACBGWIhAAz//727L4FNf2f0JcRLMXaSnA+KqSAgHc0wAAAAwAAAwAAFgn0I7DkqgN3QAAAHGAFBCwCPCVC2EhH2Olc/N/weIGAIdf99FbgePh1xZB5Ybe+I4lr5oAaqfFEOHrEFJLvZSS29+oHp6P9Ms5gWLv7ILXiE8aFlbCn/zJVMnZ5PvfSlTOYzmUdkpqIgNpvsuOiRP8aQ0woJAifXM/KubM1sXrBAXIjyrx9qwrzoDoD/C0Qe+oJiazfhDdFauGTlImget2fXOaJUQCLGexfvoNFIMpUi20LYWU4CD8ODEhgoDg+b9SJH6VmUU5d3kcEkE00vXgwe6coiTfdqfbJdmoGpqmbOtXF94DmX5SPWJZgE90AYwVt5laRtyMcJNWuhWRLjmvU4SidcPVNNvk+QT5WX0N/vWmwoq2mZ8WuclOsMfzlBujFRqvFx1HebI3IAB/PPfYvyG+ZW0pRdS92ZBNrpA9QTGYDR6I4ZN4XOuuEiAy2NsxrYdnR2SyvDz1Vl3Rlo6EFZFALiB+Wx6Oo1ro2IotrqcdqLdf9PHsIKQw/PMMEhMavR0jYuP2XeoMRgOjh2ltg6X+fWAUTYwSEftLF88pj1isAAvXAPkAcOt/Isk+dHCod5oIFa/zkdG9eEaiLh3bKVXnNhzplgcAETAB6ngAAAwAAAwAAAwAAAwAERQAAAJRBmiRsQv/+jLAAABqdq00jIwNCfhd1wCsDCz+JpHedezR7EhOtSAVCGpNaRprCS++3217klH4Uho60q9WIWsEWsBX7G50LQG92JvhQHNUSOmPib4ua/6Ob/qsgNSkxubYraG7Ht9UsHwL7JZPWAS38jtNVonLpfyB+20NSx/cjAZJxZxjuuwS3UiM5MGDpw+8STHBgAAAAIEGeQniEfwAACGv65xvNL1UaMRLAOawS5AyCg1o2Qp8dAAAAJAGeYXRH/wAADYXdL3bOLRDRrV/Y2ar4sr8I4TQUERi89KtmzAAAABIBnmNqR/8AAAMAteah/08ljekAAADhQZpoSahBaJlMCF///oywAABGAgAEYAWH0Txl6qvVnqbhwwtgqjzMu8hDU0Cs9J3sHsQG+HUYKuH0knY2doej2BuvkxjZqW1QbN0h7Ke4tWmBHDPxgnXI7Gtm/2K1B6hffnljhOBdCQYZsZ+6sNgMEwxlmc+Z/A3yqe633qqEw3we0LwSS53761IudZNntwtVfcATyY7MO9phcmYUe4gqNpOyndMPO1R1rKP+BBJSB1vAPTmg3wxP5F1Q8uZ22ED6v4vqkvHdfmH35wK4JTEBb4V1wZjkzPGXugzDuYBU//6pAAAAUkGehkURLCP/AAAWs0fOrcS1JFAi5miITpXNR5n/krXZMcrlF47K9c6IfblT1M7qWSBIIvK+NbQz3GCP9cXAAuKzL/GALHOoceuAicxjUmfAelUAAAAyAZ6ldEf/AAAjwxdp8P5nkR7Jz689ePT0j0WpGTkjNqPhYwkS/oHrXH6tQhtIRPnybMEAAAA4AZ6nakf/AAAjsix5WTOjJJ55OODDWus7g2OhWNrRUKFHimiHLbsrw1bYQAbrdg9TVjdddpUtbbgAAADLQZqqSahBbJlMFEwz//6eEAAARURnwmEuxwKmHwA3U8moUiMemIlMNL3mUCUQy+Cm6zTQkATP/awXVrAgYA92w+UVfiriqalKm31egDUeObKiQGp6zxO3Tghue4pV/kdwkbI//+Mk3zK/2OS1VvXlr6RQET+jetbsE+qc38/t7gxsQqQnwMmfqWepy6ZfYE4yQsG+bR6JSSdfHfAU1LaZTQpfwF4AiwuZjHmzcZE1wXxX67rWUv/Qp/kPfH10tWOmXBvK14qpO2ja58AAAABEAZ7Jakf/AAAjO35orHyKBdJe9oNsrY8zprO0dCAP3jCA1kuXz/ihCrfUJ38zNGetarj2nvgU/m6RwKrHwjUj2wXnZi0AAACnQZrOSeEKUmUwIX/+jLAAAEZAe1Sd17sAcbtvChSwiZUC3SLXHVimzHWu1YxgJRjIYd/3pzU8ZN6Vo3bYI+fuLPUeBLoTZW98GAqg+DpRjj4vyNoO6OT12Loe4uyvscFjMEzUmLediq0sKjSgbm5YmuKx0YiZvrnbocTwAQuncjo3mWj4qD/RAVmy8W0SVqGXozhqY3tPElv2YsFrFwJmaI9UR36+beAAAAA4QZ7sRTRMI/8AABazSIbkl5flxH5+X7N1KM7jVl3CA33yyDDytl1Fp3D8HEDapNbtd0hvQmwYT4AAAAA6AZ8LdEf/AAAkwxHkZWr/LeIT84A6zv3VTxEjn1XYWeJQWuHojoZxnLbtoRRHzMpOOIATGAnvqTkybQAAABsBnw1qR/8AACTHt3fCZLa2tc71p1M2ie4NwfMAAABkQZsRSahBaJlMCGf//p4QAABHRSDazAS2ecodBTFQIKgHFU93vhpVABuiX6HnsFAd3sqzLrBzXPl8BLeHtKNOBcgrVdGTd+yKuT5iWut3nWAGNLVQXylOwqsDMq8gO7l5RLPl4QAAAC5Bny9FESwj/wAAF0uKlGTj8II4n7wZy0SDpWeCsWSXepUbiREXVwcZDxwX4PKAAAAAOgGfUGpH/wAAJLHMJJB643oaeMyY/+HD0GF3aCCV3je9L/75optW1jO0f/5iSsRIrpO9cv7wCk4AXFgAAACfQZtVSahBbJlMCGf//p4QAABHRGetkw97MMLSZbacrwGtlW9uxolRbfIRNMO91h/dzVy88srQOWPsFoE3QgNE9JuKQF2m8Q7iACWrGQHqWnVgz2b0yFcBm7bBfQxbVWjkLmgujiaCchitoDAVQ+IT0FhOvO5WjEVq7bOSgwkX2yoU42BSm1yyWHzDy0DJf5j7oZt5gB2otl0mvsjft4GBAAAATEGfc0UVLCP/AAAXQ0e3DPPhYjPzwMpN403KEcSglk9lR5h7lSje/+zIMAA3TBfw3oJbM9ReXXySFFaWoKyzsOlmc1tzadEXWCn62YAAAAA6AZ+SdEf/AAAkwl/9i84+AKjrPsMBe0oqpFW6ZZsrbpfs46246mBpNcZfMnhjaZxwLxbNM4ar2sTrZgAAADIBn5RqR/8AACS/BCdt4L9aVAlHVqo3Y4I7wftAfDKdadQQxlF3g/nsKkhfd74qrhNswQAAAKtBm5lJqEFsmUwIZ//+nhAAAEdDjzBis/WN/7cNrRgd7ZBbmUjlncxXKUuooDABzgwYlfQvz1fuFHBPOdoJOvqD/dnot0HU3GhlWxXUPvUupCqmrdrh2ahIhvxuIPc4auIM/jfz1g92HkWfhlwSf8lPL6xN1jrEOdM5lZVCdf53f8wZVyb4mvm+VNa5Sdsgbr1hXmwUhpl7WB1IuVIQT+qSPZgLnE0WfzS/Z+AAAABQQZ+3RRUsI/8AABdCq+i0Q8xwvqax5+VuXU4e6njTOV/gJK7yOUxmk+IA2QJT4ew7kDj6deVBSUwpmO2G7y8PA99Ejb5/mF+SFh9UkXrudmEAAAA9AZ/WdEf/AAAkq/he1pCBrzifdgvx27XAdRhKSpaBBUuVSnin5lujnivbG+AuYnYMA1xu5kpvcDZJbQskwQAAADIBn9hqR/8AACSuXjOB2kRaKXgsx8J6rK5d0og+4eUZtYVqLgtA/0kHZ0JPCMSuGcTNmAAAAJtBm91JqEFsmUwIX//+jLAAAEgke5ekVDwAu/h9yvUi0pdkV/4uA37Nkk2CE+vr6de1jImga0/nmCFhuU6MXdRVibW5znS9PNt6S6PNo517+F/YhM/WXkjcvi6wTBwedD8/bm5agzzmTRiCLqyDq9q1h1tyZVF+Sj5h7TIJDNsMcej9/jnpH+r9iW4b1QSy3GZJjWzEYv7+CxI24QAAAE9Bn/tFFSwj/wAAFzT61RCtaQhdSdM95xXcP/0AADdhcRf7148v7cdAmpRx2mhvBYm0N5LFOz+nvzc9MYo92RPxvizRaADECbcnitu4kNmAAAAARwGeGnRH/wAAJKnpxxOY2SDuW7nrRJxWFCMoP7Xnho6G0FwHbzzaHzVG3y2qHXa0JzcwpRCyFcUhhbwAmkx7qJ+fVEKXqhJhAAAAOAGeHGpH/wAAJL2DLs7lkvtuJCdbaAEscFvSKmF9czT4N3pT0W4Wn3IQyJBwzXtMYfHSHVg02pXvAAAAeEGaAEmoQWyZTAhn//6eEAAAR1DmKtyt5aeDJ0jjuAQEuNp+eZiUeoRQIA0ylJUdYlby8RdCB/qZvwEw8jMMO7M6uBcbznVYNvJsLLdeJ6VPl17AcjdI44AqTcUzGdV0CpOqZYxp0b9pOVHzntNA3i6nxfLD134vwAAAADpBnj5FFSwj/wAAF0KsL1f/bMBnVrfmRwgAi+55ZkmlM+s9RPbFbbG8kSjlnnRXGb7yld/3vm4GtfFgAAAALQGeX2pH/wAAJL2EA/v/Ge5HGn1owYCd8imu0EyTDWEMsPAonEqwW7dE0NY68QAAAMRBmkRJqEFsmUwIZ//+nhAAAElDzYhD/jWTZVsFABuyBQwuoa4UDwLgD/+TtTO0itdrtVqqLZrQEjxN1PA6x0wLGjW4MH3/cQZ4lrMTp6FI/EfVSoLaBzsStZbmHvLtJ8aEbv/wwFd50xuGyszHC+j2xJn0DZNpZ/QFrK5334BbeVkLUhscukBRqaIkcbTw3Y3KPLt4b/9rE7PswKHgSmMvLxHmRYQ5ZGs1zOVq866LZvQt4VfjiEZse8Swn0t7L4iI6M1uAAAAQUGeYkUVLCP/AAAX6XxO9hAw/Wsz9WSggZ/cW8t3t0jn65kSlMpVGIahwhR4bZWY/usANPJ9KNkrSzzYQbY21N9xAAAALgGegXRH/wAAJKnq+NXDPAswPGdB/alLDgI10yKsFMSXaI/CsR3I/JlPEHg7FeAAAAArAZ6Dakf/AAAlvvRxDTIfbcpMp1FA7Sec98QlVFiELSwcS946WOuAnGB2gQAAAI1BmohJqEFsmUwIX//+jLAAAEpWEs5V2MD0zeKnUG/1N4JJpEuPmosu0OgCM2stM8PA7b+Z/j2J9phe1cUzWGKhBOYDsllfyxNaP0kRt29VHbl0nLT1G3Dbh0qmYPFj1WeV52XhesRJ+42KmyyYXYzxgp2x8A0IbPYn9h6Haid1YBc8K2ThYiD2HpfEqDMAAABDQZ6mRRUsI/8AABff61zcwSeC8I7+UspO1EPj+gvOI7Xy7pxbOY0QhQAtpLkznzKLUwNenomHcldTS/pHVbF0LGLPgQAAACcBnsV0R/8AACXAmR/OzDDrevW+Ios4QJE7b7S5qQ9xNJ34PKehacEAAAAtAZ7Hakf/AAAlvYMuzuRKWRNUt0PuCFOUXeNN9XhCV7e/ABCCL8G3LB6gUpBgAAAApkGazEmoQWyZTAhf//6MsAAASiSFD78ywCa7kBK0rYic2wPy0wogaPHEFMIkjjEFNck4fzhQxvYgs8RkIw/p6HMJVOSm+qB8cnjhA9AWtk95+u2OhtbT/uSFKXdG50zvAfow3TPDI8fIbYiKr4Pzu2wovx0et5e+D3Gdcm/9rC/cyrXISZigWS1UBtr4jCOnVyND9hvANl3afDbdPc+s+7UE22cYf6QAAABJQZ7qRRUsI/8AABff6+i0C2f+nlsvEj+riZD8TakDa5qYyIww4gcZeJbEtWElP88XbXLuFgiIBoDRZo5auRADZNiS4SLgV60zVwAAACIBnwl0R/8AACWp6ccThXQ25K1/pIhJFpOZMP/Ks0G9RPyQAAAAJgGfC2pH/wAAJa5eM4IvtBfHP1s49dqMntwAalwdBgRcRFAb/7Z8AAAAfUGbEEmoQWyZTAhf//6MsAAASgDtz4Ayyu2Z0yrEVCADg8Wa4YafZLy0uzoqrYxl6FkWGM13i5b9Gy9uY/A9tAVl2+SVg0wzyGVfoHuSvxpF8KB5adAXDnUGKkmuffc35jWyi0A+0JJiHjXh5Nw5Or+8TETjj/8zrjoJq2KBAAAAKkGfLkUVLCP/AAAX4IfOr8PLwMOh9KFsV5RwPwIw/LR4TAi0VpegnjvuEQAAADoBn010R/8AACWp6lO9RHoQAIPzvGSs+uwBHzvMEfgzATMmT5pNmiS035p0ymp67edtbqV68rp9WzuFAAAANAGfT2pH/wAAJbI8yQ36w3DpGWGd1rjX9RcJz1f4jdKqAnZryd/gPZmYcL2EH0HsRJpxTZUAAABeQZtUSahBbJlMCFf//jhAAAEm4l4EVvdTgs0s0MfcGPItIyKyd9DmTzPZm1siRNRt7MJ6v5arpK2OHV7CAzd0h59Zv61/rxusGTVS3k/aTg4PKoAtgRGYB4NAtzLcIAAAACxBn3JFFSwj/wAAGIl8w2Cu7fL90sC6739C8sh7d9HS01rKjmZ9hLjgcZ3VWQAAACMBn5F0R/8AACWp6xwpup+H15dsac9t89S0RLBNVgf3swf1lQAAACIBn5NqR/8AACa+9T4YxcSIbCFQDYaDxOG/dlAJ5/3/v9cIAAAAPkGblUmoQWyZTAhf//6MsAAATBB7LEGf9/SwoSopFyIZXwTwDEANjZ/KQZKAdL6UNUkH3jzqf8/fxM2/f6xBAAAAckGbuUnhClJlMCFf/jhAAAElN0aNgMbGkZbghdVUza//MGEND40OqZGDHIEhEthxr17Koinnhm/p7bosvG5DVtmizHPX6TejT2T6d7rue6wPQKqor+cvS99CYfF/uTVhswChJdqbVxCM7kZtcRFz/eDuIAAAADdBn9dFNEwj/wAAGICHK24YQPmAnyOs9uDO6yRCGLZYnkDl9OsjIu3bjjgrDyYsem0difdCLNsrAAAAMwGf9nRH/wAAJqnqU7rYAOTDjFAPSxoTUlaWtSv+TEjYbSl75WkSuLFQ7BTc9BJrMtITnwAAABsBn/hqR/8AACa/BCcSyWkhGe98cPlSK4cOf8AAAABrQZv6SahBaJlMCF///oywAABMKZPy2MAUECV/h2cdhw+QqXna3Yxzii9n9yeiAlh8K6GyzIaW5Yy7/L3yVdqWeYQP7lDIyT3BYQ/atGcRfMUk7jmTtpJeT50fPjX7h1op/RXb5GEU5PjxN8EAAAB3QZocSeEKUmUwURLC//6MsAAATDIGwGAOLGrBWtV4IEkxwXk0r2FHMeThKxMFvjN8SjkucryZAHAyGci2Lw/s5SILz4SpTNVTHCG3tAmRmM4LsUYS62BnE9Xev0EFfxmXGKJrWvKKucVRQ85JBwtcaTGAZCgeykwAAAAiAZ47akf/AAAmscwJt8NsPH9++Go0hqAO57psxPRyQXhygQAAAF9Bmj1J4Q6JlMCF//6MsAAATALPieF8YR5t1PW5+ZlMJlBXfrbPTmQnBYqAGqlt1cebPNycWNjlv8oiSmVi0/K6SPx30/6eNYoYJfxmP0TaXrccq9kutnKOQW/4DRvUrQAAAGdBml9J4Q8mUwUVPDP//p4QAABLesdjKiBgN6ZL5ahrVAKyVFPpw91IxHrIy7WzfijRcwZ4mHEXxbzqMrvDwiqXKOFoHCqbTORLukgehZiM8ixw/0N1loORoIcibfOpeLWHoH7gHWVUAAAAGQGefmpH/wAAJrHMCW+pb7IVSPtum+OYyoAAAABTQZpjSeEPJlMCGf/+nhAAAEtkj9FRFf3ejlkBM6MaGERpqNr48gfBHozIAYmINuMl45KydyELCoLTOzNaMpj+t7oDt4XofjdwQzT3PawdpEiGl2UAAABGQZ6BRRE8I/8AABhz6rN3S+U7DwWtsidron7OlUWBt/HBARt25RDn1NYzfkn+EmfY1ZclyQtAAnAkJ3G8LRyRavatz9W17gAAACkBnqB0R/8AACar+FhoflNhEDR6TBKYm8VhZxmDhyW5QHQVxZlHvL5P3QAAAB4BnqJqR/8AACa/BCcSyWmauF2zKjBMW/krNVImcLAAAABjQZqnSahBaJlMCGf//p4QAABLRFvQQCU/xM8idIrbUH78h8ivAubahr1WXAkVMlkAZmcvjtBHTXcbbA/7L6aWiJ7Y4/7tu6Dbg+erSDpSGXD9ODmoB8+2rXINvEwABpFNwp1BAAAALkGexUURLCP/AAAYjYTc5NlffUJMihEZBRnqZ+oklUV+NKaCiqcKanFFnLWOP3EAAAAXAZ7kdEf/AAAmwxdeuZ10P2NAos3MSikAAAAhAZ7makf/AAAmvxr6Zo7++bxVsseAbggn8gDt9rG58S35AAAAY0Ga60moQWyZTAhn//6eEAAAS1EJ/qALlUo8y57MpT7iA4BquAfWL7Xz3WR5a11Y89PYuHx8jYtoyjd78bUOIio+T+YDrJSFwdrsxEjeCKKu0g2CmI3+/1DgpejSd1193a0gAwAAACRBnwlFFSwj/wAAGImOAuQrMt3/GIihnFGzRL5NkdWYZa7FufAAAAAUAZ8odEf/AAAFZFxPe4g2078DB80AAAAtAZ8qakf/AAAmsisGIKQoZ9zdUFOxyFp8opiPjpXxlncNHq2sAH0KT/RdOy3cAAAAS0GbL0moQWyZTAhn//6eEAAAS1EJekMMYAVzAApi2hOjv/Jf9nEPDizPtNh7Nnm78iu5SZHAJegk6Z87oVupYVcN7cz7GXE9sRf7QAAAACZBn01FFSwj/wAAGIKYVYl1z/49t55pC0ejSW82hSShqUB0/iBtgQAAAC8Bn2x0R/8AACbDPhCfAP4R8Qc7c4HAAuegQdoMqX715Ixl+mAWebW4Sihy1JgLpwAAABQBn25qR/8AAAVnNNXhXBjmZmCpgQAAADdBm3NJqEFsmUwIZ//+nhAAAEtUqU51hJ8KZPs+tG8Xktv9rv5alI+Hk0eSfNROQpD3dmDDuthwAAAAIEGfkUUVLCP/AAAYgphViXXP/Nz3O8/z1lj1vRUyUuy4AAAAPAGfsHRH/wAAJsM+C8pne6vqZQAtgIf5xfNUOg1pAvhPLyl2FqXbQPC1pq3+K0c2lQZ3cIjzD9Vi2ibbFwAAABIBn7JqR/8AAAVnNNW/epuQbMAAAABsQZu3SahBbJlMCGf//p4QAABNQUaAEzCrYg7JU0WZ0xNk5OLCWsTJnhRaVPJZm2TKVMmOhSoQzsfk1OJmx0ijMx1lkZN85rmOzX5+YCDwSknrI9J5NdFwbxIgbOgBNcNqmcArwou93bueeP7aAAAANUGf1UUVLCP/AAAYgp9MUdi6n7kn2ock2k8f6gO5oRnVwN73kZNSBOxYA72W3SgGrthMgqjBAAAAMwGf9HRH/wAAJ9GV65RlFng10/OFxZJ1NQqIpviCibyWWIDvgAd8lEhWfJ1z/pKpDmY/FgAAABoBn/ZqR/8AACfJ8wcTOOGbzBweLNSBNAI2wQAAAEdBm/tJqEFsmUwIZ//+nhAAAE1FBR8aTzEX8Fq/ZLXz8ppWJ6Kl0VgBBYTsGEnMsycZN2mTyHuo8qxPTj6MDv1uBmkHaGa5dQAAACJBnhlFFSwj/wAAGSmDzbcVzKyZSqVbEETkqP+CJB33mOacAAAADgGeOHRH/wAAAwAAAwGpAAAAHwGeOmpH/wAAJ8mAcAK8x9MT9eJd7uqJeLGSXAVPmnAAAABeQZo/SahBbJlMCGf//p4QAABNUQlPBVaoi/meWut7Pt0c/GSHbyACIN+dObimID0Ntj7UhF/1g57O2TKwqKF/ha8V7+jZZ59+3r5DE5O/QvwdBW14NhhMwC+zDvp7gQAAAChBnl1FFSwj/wAAGSmDzR4yDsBN7tXa620HcoazrS7WDFYSJLnDLkG3AAAAHgGefHRH/wAABaxbeZHi3mPMjrPl4hY2iq/HshdiwAAAABcBnn5qR/8AACfJgHDcoNcXYC/uLVRvwAAAAFxBmmNJqEFsmUwIZ//+nhAAAAsMw1yAM6hS5gho1M9SIMF57YMvgKlKwS9DzQLm0GJzqPPKb0GB5h0iwuZSG+E5XieKbcMGU6htedJBdyn+6G5eNQXWZQLnZ7a8cQAAAChBnoFFFSwj/wAAAwOLDpOZUU3gwTZBPXjuk65JulbYtPF/2L68OdZcAAAAHgGeoHRH/wAAAwILcQ8g0e6SwsVlVNP0/NzBPZ4I6wAAABsBnqJqR/8AAAWvNNWxkwvP68AVDV/NkEWw5cAAAABoQZqnSahBbJlMCGf//p4QAAAc7VlwAROOSh5dM1nLvgSiuYzP30X1CEBcl//NnZfk5dN/9MXe633kZSxFc+GR0xM7F0yP4Zhgyh5rgGQF0TiFTueSCNgYB0ljkC6kmMfOrHVdiOhFB8kAAAAiQZ7FRRUsI/8AAAlr+ufGoEV2yOKZwOd27U7d3gz75dNrLwAAACwBnuR0R/8AAAWs98RMSRbvQXanHCi7+FmtqoHkmKdCbLHKc/dC/ZdVJBI5cQAAABABnuZqR/8AAAWvt0600AKLAAAAjkGa60moQWyZTAhn//6eEAAAHPKY/bLM5+lZLFJACueJi/f+BEfAF/9a4h8cdmLEZPTL+5eyjnO8bpYn9T3qAEba/jXsGQ+s04x8Up/ZRDuQuB7KvlEiEDUZpL+DYenb9EKS/PZ0wgudZOjDbMttmjwBKM4Dps5x9GwW1PRBMkDBgwAzzF3OHgJIo5nZlkQAAAA7QZ8JRRUsI/8AAAlodUFfoioR9VElHNPn4hcLFQY0wb+ELDfaB1YP17qKo/aOwH/jkdDYbiDoGtiYY6wAAAAcAZ8odEf/AAAO1D7XNuCw5wxSJ3AewjIXQHIacQAAACEBnypqR/8AAA7b9LiCdB/I/DljleX0g08JlVpFQq9ZOxYAAAAXQZsvSahBbJlMCGf//p4QAAADAAADAz4AAABDQZ9NRRUsI/8AAAltjcCa8sVq1cIHxe4BoYC5Zrsar+hbAI5mBkmFazCa4iKgBNK+ZTr9BW2IcpeKoo5SIjMz9oivzQAAABgBn2x0R/8AAA7UPtNhCnLyKQIEZ6T9sR8AAAAXAZ9uakf/AAAO1mouqm4KsSwTNRLqSvkAAABgQZtzSahBbJlMCGf//p4QAABNRQU9d4zALmrK6zIdkG9pdeFV7LtwbnYEZUtPffuY6xEetAyKWtgGRyxdihykVTWmglNkew8f+GOEfMwrzvYMrySk55nwtoIbEoJQl9DAAAAANkGfkUUVLCP/AAAZKX6zfBBRruaHYguKBFRNJMpMTZWBwUlpfMO4Z7eoAS+kx+S50rDexEPA2wAAACcBn7B0R/8AACfRle0E7A2pBS+9PtJwnUnb6rIINlIUgUJe18dvOusAAAAZAZ+yakf/AAAnyfLuqaZOO9bu5TBkcntOzAAAAF1Bm7dJqEFsmUwIZ//+nhAAAB0fUVghA1uveYrlZoetQIha1YjZhc/2ACOfIa8YHa3gV1yZjcs1TlS10PKl288iSjBjglSmzW95DLuOPNEsjA+HV95QZZCjBxAknz4AAAAgQZ/VRRUsI/8AAAMDif0m7WtL1UaMRjTSoLsW8VzZbtUAAAAzAZ/0dEf/AAAFiJUCAC6gE0YNxboouNpwKQjIskngMvyeNVFl+Q+k8Hw/m0+eEmRCMvaoAAAAEAGf9mpH/wAAAwBLfjewaMEAAABHQZv7SahBbJlMCGf//p4QAABNRSi0ArXK7rYxKUx+kPXODY7/se8fEQ0t9Pd6OFH0mVu8OArO61zyWRGfPUXy0s20KdThIpMAAAAwQZ4ZRRUsI/8AABkpg823FXRDk6QfiXuoJ7EHzGRwHSRK6YSAdbAdptZzLc13EsWAAAAAGgGeOHRH/wAAAwIaw4/vSROYZm3YmfUW2c2LAAAAIgGeOmpH/wAAJ9mEs9xKl8Yjv1V30jEDdJVDuIQAKv+ix1gAAABNQZo/SahBbJlMCGf//p4QAABNUfX1cL3pe2/YbJrKJoHVNABO2/OzsGUTisgvlMB5EFicF769dkkp8EGWDC7ndZPA2REUSuJtRsQcMkEAAAAiQZ5dRRUsI/8AABkiazdrWl+vXnaKwJPYL2/UONXtzKuWXQAAAC0Bnnx0R/8AACfeX1KEUGBOCrVg40uWqvxuOTi4f5BVl0YnhHc4Ljvh6bYnaoAAAAARAZ5+akf/AAADAhvxsAk1ApIAAABFQZpjSahBbJlMCGf//p4QAABNVQ1mqtURtzTuvIzOcduUw9jp+pkcDGYwEAlOyf0sKXjKzHtWW2mB/WyE1C9+UweUneuFAAAAS0GegUUVLCP/AAAZKYPNtVJWSBwALDrGvAjU+rFV9a2isTtUoqnhW491N6v4TD1f/0uK0BobgFwkoxqUDYFz2fUHNh7R1fi1qYQLFgAAABwBnqB0R/8AAA58U8J0BIxJQoo/K6wchtfXBFlxAAAAGAGeompH/wAAJ9mEs9xJaZsn+f5AjeCNSAAAAKRBmqdJqEFsmUwIX//+jLAAAE4RrBDlBtgBut8B5U2BsK35emuHEWwGj/XgBjrGmY3LVtJZf0ez8MiimCLB/cJ27fVpOZDjVwJh6494WlZbpGUo0S3t4XVGv2NOOo9fWCenTkjSAEQE/C5vYWxIfLYD/o1ccrJq4QQsi5JUhuqnplxWd3Z+TQ1OrBnN4yj5KmAbGg3T8buulF9DTbwul4A4GsQqcwAAADBBnsVFFSwj/wAAGSmDzbVSVq5FIQyBPeE0yRexAFEJwC0CXGhQNtjXHfT6AHSWPPMAAAAtAZ7kdEf/AAAn0ZFntHrQ/wc5xnRLaSXebTmYMLGt8GdFmbvxd6fgxyaBls05AAAAJgGe5mpH/wAAJ9lzJ8fstYLlLlGLrjcwj6erewsM4foMSTw+S2dPAAAAH0Ga6kmoQWyZTAhn//6eEAAACo+798D1eByjFXw0uWUAAABPQZ8IRRUsI/8AAAlte6QCd7oKxsK5lEzgh+IiOMlTro7wKg5oCNSKrm88aQea100DyXKds9MnreW1jWQMgwzKOT+jd/TgqgIMtoz4O3s17gAAACEBnylqR/8AAAMCG/BCb6zOWPq0in66nknpiq6WBZ/zsqEAAACAQZsuSahBbJlMCGf//p4QAABNRSi0ArUU/IjYwESRrivLKKiDJmNz2BoOQr+l40xqsucB2V4yUeazd7PJgi1WDx01tRjYP3ZCqc2y7QjXtKFur3XkzsOqmntM9BUPJ9myi6cq6ROqZwD1Yb2ddvoL2kDH74U/cA+BHO4WDLfHciAAAABHQZ9MRRUsI/8AABkiazdjclg7ABHtY1FesbYsNumTxIrSgi2EapmEXpG9ZouN6waCwJAy9GHCxWXU7z4maZSwTo6U1nf+y4AAAAAcAZ9rdEf/AAAnwpNNhCnLyKRG6BSXjwQ4EksLLwAAABsBn21qR/8AAAWvMJZ7iS0zZP9IlrBkOLkeNi0AAAAXQZtySahBbJlMCGf//p4QAAADAAADAz8AAAASQZ+QRRUsI/8AAAMAEOKfkB0wAAAAOwGfr3RH/wAABbJjgAWzS60YdmOobCdt1xRpwxl3+32KMNyS+PPQMivXCjdCyDZehK53ixMt0gLmE/nwAAAADgGfsWpH/wAAAwAAAwGpAAAASUGbtkmoQWyZTAhn//6eEAAAAwP76isdXKWuSjTnnJW0JnP02cK/4AIw3pRqr33/TJbG9B5R0stb/Qr+P2lsYnsCnuImAwbIwa0AAAAfQZ/URRUsI/8AAAMBUFX0cf/wng4WM0SwUzeWbh+1lwAAABsBn/N0R/8AAAMCDDRxr+1CBrpen693y0ZEsdcAAAAOAZ/1akf/AAADAAADAakAAABdQZv6SahBbJlMCGf//p4QAAALDKoBFkoAj25P1KFzC6AdBBc4EXIddW+ig1eqmeRAIIz68vlw03HkVJy+jDVWtTTNmqoJCg0hhf2iaFmfyglGBnSq+AT7Ug9HFzXJAAAAHkGeGEUVLCP/AAADAVlVhPmZfMJEaxct0brVObiy4QAAABoBnjd0R/8AAAWv0MHOto3DV9gRWgicZldiwAAAAA4BnjlqR/8AAAMAAAMBqQAAAB9Bmj5JqEFsmUwIZ//+nhAAAAMBkXXoACMhgmFFABdwAAAAFkGeXEUVLCP/AAADA4u4+KFuYnVbD0kAAAAQAZ57dEf/AAADAM3hPsCZgQAAAA4Bnn1qR/8AAAMAAAMBqQAAACBBmmJJqEFsmUwIZ//+nhAAAAMAB8AyuARp3dkpXuszFgAAABRBnoBFFSwj/wAAAwOLuO9nEnYEnQAAAA4Bnr90R/8AAAMAAAMBqQAAAA4BnqFqR/8AAAMAAAMBqQAAABdBmqZJqEFsmUwIZ//+nhAAAAMAAAMDPgAAABRBnsRFFSwj/wAAAwOLuO9nEnYEnQAAAA4BnuN0R/8AAAMAAAMBqQAAAA4BnuVqR/8AAAMAAAMBqQAAADhBmupJqEFsmUwIZ//+nhAAAE1FBM4c0QCtUVdXjABoyNIsObF5rSmV7XwSrpTy6xL6bzbgf8xhnwAAADhBnwhFFSwj/wAAGSJrKeJGEZc8V6uoASMHcoAiWUMyf49IACJRv1G3gScXgaNJU6NYj+vCaIyI6wAAAB4Bnyd0R/8AACfeX2inQBFUKqPhwjOPSgxAfSAENOAAAAAcAZ8pakf/AAAO2/6qS/ibjqYcEJZqSKcCsvIbYQAAAF5Bmy5JqEFsmUwIZ//+nhAAAE1VEc5uZrNPXir1Wg6ACIN+RMCvD3rPLo0vnO8e9u5xcDIxFYxD00wK7WZoIdQtaqN9+iiq0LO8/7XnDyJRcnRd350anJLc2ChFMcwzAAAAJkGfTEUVLCP/AAAZIms6TFYAwKMDiNfC7G6+q3f0w5DV/qMlFY04AAAAHQGfa3RH/wAAJ95fYq9doxIiCxKqzHHe2Qw2UGVBAAAAGwGfbWpH/wAABa8wlnuJLTNXCs96Gh8AgpUx1wAAADtBm3JJqEFsmUwIZ//+nhAAAE1FBSDt15PwWEM95p/efLDZhdFvcALM3Whm2MuCS4trPbggGVhsdHwqjQAAACNBn5BFFSwj/wAAGSmDzf3Z/kTTOCBZ/blmPNkBD4oYlx0TTgAAABsBn690R/8AAAWL0eOAaSlublyDiS39NUun6G0AAAAaAZ+xakf/AAAn2YSz3ElpmrhWa0dQ4VWFDpkAAAAlQZu2SahBbJlMCGf//p4QAABNUfX1cL3pebHt8JfD7PFa2bBOwAAAADBBn9RFFSwj/wAAGSmDzbcVyvXHohtVHl75fWhA5pSHvdd53jg7ywL/FkCMAkL7R1gAAAAcAZ/zdEf/AAAFi9HhEeTshmeyRymPJNGZ7pdxpwAAAB0Bn/VqR/8AACfZhLPcSWmauFZTHyo7fi+tjQSOsAAAAEJBm/pJqEFsmUwIZ//+nhAAAAsfs/jwlyXpLryQkogbgcwLBOVEAT4TswI/GlQ7s4hI+Mu7X1QYvN59qROu2R5wmLEAAAAiQZ4YRRUsI/8AABkt2z9h/YZByEVo4tJbrzRI6NpeTaL2qQAAAB8Bnjd0R/8AACfeX2ipoHVH1qCDO4XQ7+0AILxO8BsWAAAAFgGeOWpH/wAAJ9mEs9xJaZq4VnQgKmEAAAAXQZo+SahBbJlMCGf//p4QAAADAAADAz4AAAAdQZ5cRRUsI/8AABkt2z9h/YZByEVo3k34gyP2o9MAAAAXAZ57dEf/AAAn3l9oqaVT1PvCdscQxYEAAAAXAZ59akf/AAAn2YSz3ElpmrhWVSAgUEAAAAAXQZpiSahBbJlMCGf//p4QAAADAAADAz4AAAAoQZ6ARRUsI/8AAAMDiRC59KiCumT0TFy23NJxv5gzXJ/SLKDSGYbLgQAAABkBnr90R/8AAAWv0MHPGrjubOISJT4sd32qAAAAGwGeoWpH/wAAAwIb8aznsqjK2ijg2WaDu/diwQAAABdBmqZJqEFsmUwIZ//+nhAAAAMAAAMDPgAAACxBnsRFFSwj/wAAGSBrtQW5QWgBX84mTtWd0YDUb9kqeVOKHkMFIKxTlOo9IQAAABcBnuN0R/8AACfeX2ippVPU+8J2xxDFgQAAABgBnuVqR/8AACfZhLPcSWmlxqUHYPEECgkAAAAXQZrqSahBbJlMCGf//p4QAAADAAADAz8AAAAoQZ8IRRUsI/8AAAMDiRC59KiCumT0TFy23NJxv5gzXJ/SLKDSGYbLgAAAABkBnyd0R/8AAAWv0MHPGrjubOISJT4sd32qAAAAGwGfKWpH/wAAAwIb8aznsqjK2ijg2WaDu/diwQAAADpBmy5JqEFsmUwIZ//+nhAAAE1FBMtM6AOlIc1U2qAC5GLuyFJLvz+TWKq0mHYBzj/UnXz1USDAd8GBAAAAIkGfTEUVLCP/AAAZImsp4jsbN0goXNI2iF24sqYnxEqEK2AAAAAXAZ9rdEf/AAAn3l9oqaVT1PvCdscQxYEAAAARAZ9takf/AAAO2/6rY12AGjEAAABcQZtySahBbJlMCGf//p4QAABNVQ1mpvfByzka8BcBcpehvhIAEQbk90s9zfAt5igjYmBgJqPJ0RUJubzxSyxHo7KSBNT53OYsR3/UvoYZ0M2Z17Folc26buTC4mEAAAAwQZ+QRRUsI/8AABkiazdrWl64o3zJSOB68cmGfMMu4ABNVMPYkQ7Xya8TuLfK3HDbAAAAIQGfr3RH/wAAJ95fYq7e0g0UENoR8lRsr4UqsQwDV17ugAAAACIBn7FqR/8AAAWeEcfmJZxG/+DtUIOA0dNxiO0B9xZ41tWXAAAAOUGbtkmoQWyZTAhn//6eEAAATVUNZqfQOMF6UaJrR71VjyVGuE1zEon80aQsE0wYG3fuZEw/J8VRgAAAAChBn9RFFSwj/wAAGSmDzbcVdEOTpB+Je6gnsQ6wJTQ/EfFh05uXEDTgAAAAHAGf83RH/wAAAwIcM+CYk5DP1QzWQb6E9PZGxYEAAAAWAZ/1akf/AAAnyYBw3KDXF2Av74wMKAAAAERBm/pJqEFsmUwIZ//+nhAAAE1R9fVwvembL9gCEwYwEALcAiUdIiiizruqDzUqPPzxkHCpPrbPmyXfoWajKNpCtDxA0wAAACJBnhhFFSwj/wAAGSmDzbcVzKyZToBhF4lpiVBIgeCgeppxAAAADgGeN3RH/wAAAwAAAwGpAAAAHQGeOWpH/wAAJ9mEs9xJaZq4VlMfKjt+L62NBI6xAAAALkGaPkmoQWyZTAhn//6eEAAATVH19XDJsgVhnqV7mKZf/jQ31zvD2XeIKuBmJmAAAAAbQZ5cRRUsI/8AABkpg823FcysmUmEUmCl+UFbAAAADgGee3RH/wAAAwAAAwGpAAAAFwGefWpH/wAAJ9mEs9xJaZq4Vms7ALiAAAAAF0GaYkmoQWyZTAhn//6eEAAAAwAAAwM+AAAAHUGegEUVLCP/AAAZLds/Yf2GQchFaOJeWRQ/oYEHAAAAFwGev3RH/wAAJ95faKmlU9T7xotlbUNmAAAAFwGeoWpH/wAAJ9mEs9xJaZq4Vms7ALiBAAAAPEGapkmoQWyZTAhn//6eEAAACssFTrYik528N9isn5GaXh8XI0bmYAaYp3JSR9g0ELO++3a2adBOcLoOkAAAACNBnsRFFSwj/wAAGS3bP2H9hkHIRWje+ECXE7BUeGfRgMWxYQAAABkBnuN0R/8AACfeX2ippVPU+8aJa9MITouBAAAAHgGe5WpH/wAAJ9mEs9xJaZq4VlTWFAJdPUatqtyR1wAAAFJBmupJqEFsmUwIZ//+nhAAABxemp8982gBCm/Ou0QnzvkHWbQoGM9ZwPhFnXo8f6S8WxxeEn7n8jheqhv9RMYM1JljbMUa1eq83mqoRtJiwhl1AAAAJkGfCEUVLCP/AAAJLAy0yhdQd+WBh3wArYramIfVm8e5Vqi0ft2XAAAALQGfJ3RH/wAADnxTwnQEjElChBs8N4mGrhtxLfABY5jBfgHfjsFh8JrP3g1lwAAAAA4BnylqR/8AAAMAAAMBqQAAAEdBmy5JqEFsmUwIZ//+nhAAAAsQO+zU3vg5Y4FzEWgAtUHQJJQC9VPuyHJ6Jm2N1ns/SA9DGGHjhF8oDGXo9eBPcO91Nwgx2AAAACBBn0xFFSwj/wAAAwOJ/Sbta0sqNxG6R21t+RFMgFcnLgAAABsBn2t0R/8AAAWvtHepLucIdhI89lK931YLZcEAAAAQAZ9takf/AAADAMRJ4UB/gQAAAEpBm3JJqEFsmUwIZ//+nhAAABxM9viQcu52z9fnKW0umvHcM9a8I19qhr7QMARohCD8vvRy+HBEhMtv5cR2WEdlV30VAG6IjvUmwQAAAB9Bn5BFFSwj/wAACSwKHkXkCYHeC7e0vG5Kp8v8/zlwAAAAGwGfr3RH/wAABYvQwhhtZJFsv/Q0WkPjDiQsuAAAAA4Bn7FqR/8AAAMAAAMBqQAAAERBm7ZJqEFsmUwIZ//+nhAAAAsMqgEJlffHZnDwsDc6J1oAUV43Fv0Uqr4NdgJFshFN52xs6wTJdT2GUnDmMCLe9DHOoAAAACBBn9RFFSwj/wAAAwFZVXDSX7vK1/BSk0Isv+CfprdqgAAAABoBn/N0R/8AAAWv0MHOto3DV9gRWgicZldiwQAAAA4Bn/VqR/8AAAMAAAMBqQAAAEFBm/lJqEFsmUwI//yEAAADAos0FwaQsAF1AH6q9nXKzQHSg098/I3GloHjpaN9bXU/KeHUH1klaYFrmRFWNk1I4QAAAB1BnhdFFSx/AAAFizXETtc3tG7iUG9cHKCv3RA3oQAAABcBnjhqR/8AAAWtMA4blBsedRL5TtSI+AAAAhZliIIAD//+92ifAptaQ3qA5JXFJdtPgf+rZ3B8j+kDAAADAAADAAAVt6RlhkJ0L/JiAAAFfACyBdhEhIBSxvCoE1cz+CxlID9u2s1fz9y0z1xmr77eTr69ZmroBa90w2cFOb8/nrUthw+AY//xEckhIlm7VU04ABp4Cj2IIIoDbfpdLW0SHrMwRqsgpOpg9xCoJuHIzuJoaKKtq+PVT49dl/55cfpfhrQnf84RURCPAE61HqCckRWG4TN5rA7KNWfgEiJO8gjQAcdhuHghzQc4EFAYIyCAFiYAepyQadakI3S5oOg6rLThWb34l/NGtSAGzVksMvOCAkye4Er6mcjC7nVuvcRRoMNgt2TJt2flm4AQQIyRVjf49+/b76mZnxcXy17BnHtNgXsfelB2XMufwRg4HS11OchKCB/5M01m8na+ow21JF7kzi6CiG6N5m9648AIPccrleklLGhfFax7HmAv5ReJE3QU0E9NJ5wB0m92ho5D7+HPeKLxTur0XmwotGQ6F7eatzDFqbg+JdHubW/POvD1HFzoh2MgtRL2SuCaRX5MrM4zH9YwsdSyWPR73WH7DozzClzkqxKOpZ669DdG9kyEBxrZ7NLruvZ7XTFgCsxAF3kx0giNEnCGOCekwuQ9PrlSbA1srz0QJfwg2ZvP+PRs2styGGIOMcxLX0VpQdCAAoWZCstrGAAAAwAAAwAAgIEAAAB5QZokbEM//p4QAABNUi5AUMYO6RmK26phDWOUhMLqjO5e1yFgX3/gpaoOb/rVbcEeAgZy1gaL2bKvZJeBs2r0mBbDGQAnVACPeoQ/BYDtaUvEi+E51lQsp872m+vOOI7QbXBuWmPHpuGNHejh6TzCfYjNm8fm75h6QAAAAC9BnkJ4hH8AABkikowjosiZ9VKAu58rnRzXE2MxEofcubJn/4tpQmmAAAgW0qgNmQAAABwBnmF0R/8AACfC4lnfVs1xRseC532knFtT/mnBAAAADgGeY2pH/wAAAwAAAwGpAAAAKkGaaEmoQWiZTAhn//6eEAAAHPQPPVCoAG8FP34qDOcIdnppVYKjBcBvQAAAACNBnoZFESwj/wAACWwKHkWY/HRxJIg/eIAAAAMAABnKplg44QAAAA4BnqV0R/8AAAMAAAMBqQAAAA4BnqdqR/8AAAMAAAMBqQAAABdBmqxJqEFsmUwIZ//+nhAAAAMAAAMDPgAAAEpBnspFFSwj/wAAGSBrtQW5Gff8IAWlbzO+DXB5iDbdxlExycqzSSklNoYrbG4UALgs7X4gWpmoWzawSFy0aCV/gAAAAwFrymWBswAAABwBnul0R/8AACfeX2ippVPU+8IJjrAYLdbrsDbBAAAAHgGe62pH/wAAJ9mEs9xJaZq4VlbkvlnCABjyKrYu1QAAACJBmvBJqEFsmUwIZ//+nhAAAAMAB3OeQgEjfKYq5N9gzFtBAAAAIUGfDkUVLCP/AAADAADdbtng5Hm+nkfMAAADACU61KoLaAAAABABny10R/8AAAMAA8xL/b8gAAAADgGfL2pH/wAAAwAAAwGpAAAAF0GbNEmoQWyZTAhn//6eEAAAAwAAAwM+AAAASkGfUkUVLCP/AAAZIGu1BbkZ9/wgBaVvM74NcHmINt3GUTHJyrNJKSU2hitsbhQAuCztfiBamahbNrBIXLRoJX+AAAADAWvKZYGzAAAAHAGfcXRH/wAAJ95faKmlU9T7wgmOsBgt1uuwNsEAAAAeAZ9zakf/AAAn2YSz3ElpmrhWVuS+WcIAGPIqti7VAAAAQEGbeEmoQWyZTAhn//6eEAAACx/yIID5Fdf/dDvEsCYPRxdku1Cldsqj8zON+MfuxySsDqay63QvePfSFOWl2TEAAAAxQZ+WRRUsI/8AABkt2z9h/YZByEVvviQJ9NC/FafyNv7SyVkAoogAAAMAAMYdMsDZgAAAABwBn7V0R/8AACfeX2ippVPbgOQ00mEAQrAwc5HWAAAAGAGft2pH/wAAJ9mEs9xJaZq4a+3H214IeQAAAGlBm7xJqEFsmUwIZ//+nhAAAE1SJcARk0XYpOLeP/IrVGYis7enzFkGjLXzNrZEUTkq0FqHUXdbLi8hVRuvfrserJiN6kKThDZYB/uOQfGE93UNPiZU6Gs/eTlNNzKZfa04+kaCTmYbi6AAAAA1QZ/aRRUsI/8AABkpg823FXRE5S4gEfLaBKJNk3ahAOaLQsN4fCO/WwPAAAADAAB9IJVAbMAAAAAUAZ/5dEf/AAAFrFt5X3qJ3P8o34EAAAAsAZ/7akf/AAAnyYBw3Lg8ECd5QmyTkGMhyefEucOjteUw47ZSH33q5fV895gAAABSQZvgSahBbJlMCGf//p4QAAALDwWmzHkbPXjVPElbPoslLQ6Ovhnyd3Oj1wARoPef6FQO9ALFKNjCFH7BUFXSvT2Sh2CxlSYr16uV/of3jlWVoQAAAC1Bnh5FFSwj/wAAAwOJ/SalcRt+CLVpXX0gfdkufZ9Zp+Xn4hXAAAA3E0ywNmEAAAAbAZ49dEf/AAAFsGrJ+ZdNTIAe33H0KdzjMN2qAAAADgGeP2pH/wAAAwAAAwGpAAAAWUGaJEmoQWyZTAhn//6eEAAAHD0WfV6iQf2I0XbDzFebFickwAsy7lzgY9DxsDpFpGu05WFAx37YvwF8RX5M9J4ne5kOjTDmjALEFS+yWJ5+JOvrScYJXcKAAAAALUGeQkUVLCP/AAADA4n9Ju1rS7mE0+HvpO5hnNb+TrYSU7XRKNkAAAgkJ4QLuQAAAB0BnmF0R/8AAAWvy+wi1lsJca2cq/1AU34pF2Q2wQAAAA4BnmNqR/8AAAMAAAMBqQAAABdBmmhJqEFsmUwIZ//+nhAAAAMAAAMDPgAAADpBnoZFFSwj/wAAAwNzcPoAyQlK+asG2tY5rDx7slL7otDDh5+7VDZ7vBZOqUJRk04AAAMAOpBKoDZhAAAAHAGepXRH/wAAAwIKbtx1DdpkfPwulYUSNmaBpy4AAAAbAZ6nakf/AAAFizTV3tuIaoJIG+jw6CRwkZNPAAAATUGarEmoQWyZTAhn//6eEAAACserIXbgosIImwcge4aqScAKhv+hUbfjqoXkKo336J3tDDKG+LfmwwBUF+wQNh5xPG/jx/2YWwEbtuaQAAAAP0GeykUVLCP/AAADAVADa3Dj++Wi1JDCDIbTP0DpYwIAJrq7YLiqC4+r+XlMTPz+T8kwAAADAAADAG7glUBswQAAABgBnul0R/8AAAMATVh8/IeVo7JLkJtT4sEAAAAfAZ7rakf/AAAFizTV3tuIaoJIHADMAuF/eDVgMPvaoQAAAD1BmvBJqEFsmUwIZ//+nhAAAAsMqgEJlfUiSAD92YSdYVhCXYS0F3KsylTxR83MZDGrmmS4fx05ba570jQNAAAAK0GfDkUVLCP/AAADA4sRj+4UjbCR3FVsu3x8nT0C5AAAAwAAAwAQaE8IF3AAAAAOAZ8tdEf/AAADAAADAakAAAAbAZ8vakf/AAAFrzTVsZMLz+vAFQ1fzZBFsOXBAAAAWkGbNEmoQWyZTAhn//6eEAAAHQ5tG6Z80ALb3lbhFoFCirphF43t/chmSiuvkYyOa2Rva1uMAdVvR+0OCbJJx42D8hYnq8Ci6Tia0o7Iqk6ZA2fUP62/bFMagAAAAC1Bn1JFFSwj/wAAAwOLEY/K3/lwRcGuYc0gSL1EFLZs5MAAAAMAAAMB3IJVAbMAAAAQAZ9xdEf/AAADAMj3A5wGrQAAAB0Bn3NqR/8AAAWeEcz93mCl9rrVaQgA7a0UKJp2LQAAAFxBm3hJqEFsmUwIZ//+nhAAABxvwigAa1eydPkC+MsQomuk5VZ52E4oMhfAY3wuMolQ2eUdHADuvzsflcarDoShrh/SiGcqsaKKXI9swdrU4NvAdafSRZVWYfdHsQAAACxBn5ZFFSwj/wAAAwOLD5HX+6bkNQLGnOk1PG3CJhdAAAADAAADACIbSqA2YAAAAA8Bn7V0R/8AAA6BL/YAQsAAAAAcAZ+3akf/AAAFrzCXS+wZLK5WDhr/wHk+iLEjrQAAAHFBm7xJqEFsmUwIZ//+nhAAAE1FBPcucAoCEgl6/vYXfYKkJRkg1kVFn2ZTHC06dEgpPdH2+6VM6QQGXwo3LnLH2MrdY9THSp/xWyV0zgo5muCrvXUH5OUyHMZo2VJLR698u8FFPnFZP0RwXgyi/YhvQAAAAC9Bn9pFFSwj/wAAGSmDzbcWF7XfBD9JEIVVZhWTHDAU1BvyjV4AAAMAAAhMJ4QLuAAAABsBn/l0R/8AAAWvy+5uXDYrTGi5NTMShK46xp0AAAAeAZ/7akf/AAAn2YSz3ElpmrhWUx8qO5cfQA11Q1iwAAAAQEGb4EmoQWyZTAhn//6eEAAAAwGRpWBdbYqOHaL8F/tACC//nGfxsPXp//SWbLB0zVU8RIuRFZNs2IWHul8yMZ8AAAApQZ4eRRUsI/8AAAMDgACDojdZvgmZ6eRdF6wlHkjRnAAAAwAAMUplgoMAAAAXAZ49dEf/AAADAhwzGmaVWIhTCMvS0nYAAAAQAZ4/akf/AAAFrzTVuoBHwQAAACtBmiRJqEFsmUwIZ//+nhAAAE1FKLQB+K1a727fh+7H5iNna+i0wdFLzj1gAAAAKkGeQkUVLCP/AAAZKYPNtxYX2Q96fAiPSovFOAAAAwAAAwAAAwEzqVQKCQAAABABnmF0R/8AAAWsW3mugHpBAAAAGAGeY2pH/wAAJ9mEs9xJaZq4Vms/lSOsCAAAAF9BmmhJqEFsmUwIZ//+nhAAAE1R9fVwvel7McsxyK7kJAAnVYcir4xZ0N+Bi71q0568KydjA5kRIfEuOdZY8AAlM8hf1Rvv0S1IqLukEAV1PuxPtNgWleMG2N3sTeX1FAAAAC1BnoZFFSwj/wAAGSJrN2tbFfp6PTC4ZkIQr/E3wZ0afcAAAAMAAAMC3kPCBd0AAAAXAZ6ldEf/AAAn3l9oqaVT1PvGhscONSAAAAAbAZ6nakf/AAAFizTV3tuIaoJIG+jw6CRwkZNPAAAAHkGarEmoQWyZTAhn//6eEAAAAwGRkwbOM4GasfANmAAAAC9BnspFFSwj/wAAGSvjnf5dSsUc6pt0xIR0h0gROhb9aWsx38AAAAMAAAWMh4QLuQAAAB0Bnul0R/8AACfeX2ippVPVBp2JMPdl/TGjs6qJpwAAAB0BnutqR/8AACfZhLPcSWmauFZTHyo7fi+tjQSOsQAAAEBBmvBJqEFsmUwIZ//+nhAAAE1R9fVwvenigaMOQCR0bKEu1fV1iVwka6RcJXW9QaNd0wjOCTVsO7Fg8uSkKeH/AAAAMEGfDkUVLCP/AAAZIms3a2EpfdjyTl9DjWV+C0cK7L2GomgAAAMAAAMAACJbSqA2YAAAAB4Bny10R/8AACfeX2ippVPU/sZw4UmMQ0ZHeLyzcacAAAAaAZ8vakf/AAADAM5JUs9xJabe6Z8ekgbwLYsAAAAcQZs0SahBbJlMCGf//p4QAAALDsCBAunCoABqQAAAAC1Bn1JFFSwj/wAAAwOJELlh6NLmRfQeANwEQZ9m3TjpfgAAAwAAAwAv00ywNmAAAAAaAZ9xdEf/AAAFr9DBzraNw1fYEVoInGZXYsEAAAAbAZ9zakf/AAADAhvxrMH/E3HV9ncYh4anJjlxAAAAXkGbeEmoQWyZTAhn//6eEAAATUUlzPMACfq3LxE26d0DjrpILrTiqTgJsOirAA4uxgRZTchY5DQragFMiYXg5RLGZM0Xqt4DekJJ8IPN9TDw2fA9jL4zlb3Cho2p1VEAAAA+QZ+WRRUsI/8AABkpg823FmCGdybk6kEPaaAAnsSeAAXC3I/Sdbqxw82/ACKvxzkVzFmAAAADAAAKRlMsDZgAAAAWAZ+1dEf/AAAFr9DBzraNw1dvPCol3QAAACoBn7dqR/8AACfZhLCNjP9ZbyGP2rOMB/BHxsFZ6HOL7b+P8oa76S+7tUEAAABFQZu8SahBbJlMCGf//p4QAAAdDRSbXD+ro8puAmkFDAL4a0U9QAJnPBqRS8CA/4G6sALT89CRev9aGYOx1hiWyI7489WAAAAALEGf2kUVLCP/AAADA4n9KBqZQVgOAN8QwwVFkP27QAPkuiJWFQAABuGDLA2YAAAAHAGf+XRH/wAABbCqKvniFjTKTA58dm8qC0cmsbcAAAAQAZ/7akf/AAADAgvxvYB1wAAAABdBm+BJqEFsmUwIZ//+nhAAAAMAAAMDPwAAAEVBnh5FFSwj/wAAAwOKjJ5387a/jzQbhQatgBx8OJkwpJQ7luPUNCWDkrnYr67pLAHCPxEn47vRPMO+vjjgAAAH3CeEC7kAAAAbAZ49dEf/AAAFr9C/wzKwPVfDKnN/V3Eg3gNsAAAAHAGeP2pH/wAABa8wlnuJLTNVSEU3imdkeePwR1kAAABSQZokSahBbJlMCGf//p4QAAALD6eqpreI09tesU8FWSyKUagAq6Ijj70UTCcmRmPTl/DPmwgg/U2qCnuCzEouF37GvdHUq085nZx0hfuQ/T0BWAAAACpBnkJFFSwj/wAAAwOJ/Sbta0v1vok24QNo7CtdE8AMMT0QAAAI/CeEC7kAAAAbAZ5hdEf/AAAFr9C/wzKwPVfDKnN/V3Eg3gNtAAAAEAGeY2pH/wAAAwBNfjewZkAAAABGQZpoSahBbJlMCGf//p4QAAAEFUws3rCLoqp263JJTnnNfIHp5C9XVj6nCiC5q7Q0AM/MTvB181F99kmg6KVG4ludEErHgAAAACpBnoZFFSwj/wAAAwOJTgQWDoYqaW8eyerM99KWV3i0fe7wAAAFvdGWBJ0AAAAbAZ6ldEf/AAADAhwz4IB8K2IYK2xoT85A7WxYAAAADwGep2pH/wAABa81CgCLgQAAAExBmqxJqEFsmUwIZ//+nhAAAE1FKLQBPhVsoHKWXIJNvLoxtpFN4HK3mKt3XaNADDfBqZZGdKlvDRs7poR3vnhKOCwQKMwk0vxMKg5kAAAALEGeykUVLCP/AAAZIms3a1peuKKDwjCwOd0y029FfazGHY7HsAAAB3eOEBsxAAAAKgGe6XRH/wAAJ8KTTYQr4VW+3UPgA42lzxe2HGwKvct7F7YA66Vc9L7FgQAAABABnutqR/8AAAMAA+L/xWVBAAAAO0Ga8EmoQWyZTAhn//6eEAAACssFTFIEJk7Rm5ud3Lyb2OATqYiJUuBhawrv8TAbl1kKYAs5/vrkIc7ZAAAALkGfDkUVLCP/AAADAVBV9HADNrKtfKAZ8t7zy0OE6qVoxB6N4mAAAAMBSevhAf4AAAAcAZ8tdEf/AAADAgw0ccA01UwCeKft/8r9ivcbYAAAABIBny9qR/8AAAMAS344C/B21sEAAAB0QZs0SahBbJlMCGf//p4QAAAcXpqfPfNoAiYYO69vMyeax6GsXJ3dapfPu8BvoJdYdZWckgDwRDs9Z22rmMTsljFHyRmrIwLFXsW3YSqO1y3xvnQxPJjui/qtYpLFbbmwq+25KUK3prN8ceXsJlHrceq/rZIAAAArQZ9SRRUsI/8AAAMDif0m7WtL1xQ2i3EU+20J1e74UtA3Su0YAAZ1UZYEnAAAAB4Bn3F0R/8AAAWvy+0VNLv7baY5jCgCmFcmvQ2hjKkAAAAOAZ9zakf/AAADAAADAakAAABfQZt4SahBbJlMCGf//p4QAABNRSV4AnAJ8L/pSdmlVAavhYnvCwMx8/ojblpQkdAwH2+3DRFuwdQgXyIKRn6flNs51yYXoIdg9dxKIxjZkesgy1VJQrysPp/GNHk21QMAAAAsQZ+WRRUsI/8AABkiazdrWl64ooPCML21QgJbJ4kzzN5b0XKnEAACx5TLA2YAAAAtAZ+1dEf/AAAnwpNNWC8cXHeFwknz9ntGX723UgAxu6aeHLesjZfZnvPkSgNoAAAAEQGft2pH/wAAAwALXmmrdUfBAAAAekGbvEmoQWyZTAhf//6MsAAAHGidRz+SG1zMAK5eIMq8UJyS8eH/s1Prm+wopFwPbHc2iOtQ1tMAjVIwJRpUHKq0EBlsaOFbNHviGgJ51KjTNdtZEJ+0cNGMAgccwpDLtB3TVCBPmRlxj4kguSr1Ii5FFASS9Umocn+AAAAAO0Gf2kUVLCP/AAADA4sPkWeeEyIyD77tbP/ep1mC/voJj4ebtii1BW9YDR3rcPAAAAMAAAMAAWFTLBQQAAAAEQGf+XRH/wAAAwALWLbzXXpBAAAAKgGf+2pH/wAABa8wlnw6JAowrh4+Z8KvNI9cNAUMK45eqeF9Z0g7ys4pOAAAADJBm+BJqEFsmUwIX//+jLAAAAsn1OFn7odREmvYVeBnABuOAF+mfdbUKZRTpo/5qyiSsQAAADpBnh5FFSwj/wAACTFQKFyJdGyzPz45drZ/71OswueaCY+Hm7YrmKFwTs4gIJWdIgAAAwAADOKZYKCBAAAAKgGePXRH/wAABa/L7Rfo91GFcPHzPhV5pHuygIbgBqZrnzP+2se2ALFJwAAAABABnj9qR/8AAAMABBfje3XBAAAATkGaIkmoQWyZTBRMM//+nhAAAE2+K3tQTkA6nWYAXQzuM0obhX+8h+nkUFozXzM7ZChR+EEwiYSKlveo3cNbA/ctWQqZkCeqJrxHCdVi/QAAACoBnkFqR/8AACfZhLPfQntjKFKcfulphGa5GTr+KAAAAwAAAwAGhYMsDZkAAABKQZpGSeEKUmUwIZ/+nhAAABzvjB+AG3v3tr89MgHutJGMUac5xmTQdZJByEQeAcQq166zlHTqBAL05A3LgsiArhqUaCnNdyYhESsAAAAsQZ5kRTRMI/8AABkimFVPCn06BeSoL9Zj/hNiDvgO6eu9MrAAAAMBpWDLA2YAAAAaAZ6DdEf/AAAO1FDzJY+PpNGcQey8/vICY6wAAAAOAZ6Fakf/AAADAAADAakAAAA2QZqKSahBaJlMCGf//p4QAAAdH4PBW5fTu4t3SIjabEvZ5t1gghQEc1gI1MAO+sslz00zQSfAAAAAKkGeqEURLCP/AAAJbAoeRWaYSWuoeH7HVLpP+kKsHCstCgAAAwF5IeEC7wAAABoBnsd0R/8AAA7djYeMbCa+MKEWk1KQaJkbYAAAAA4BnslqR/8AAAMAAAMBqQAAAHxBms5JqEFsmUwIZ//+nhAAAE1FBPcucAn6L+tKqS+sZPiaMErG20+YMjPP+6tiEHoAviPmVEvxwhLZI951SKM0TqjKZceXbKCqz/1SfJzTmKGeEztvNzhSQBVIXECya4h14/H/hXRHs7IjmIC/PD6M5ZDvDZHwf5plnwMhAAAAPUGe7EUVLCP/AAAZIms3a1/mXbUlA7KJQyKV4qCTYiiCGbdG2JNJgUq/NJVUcwV2AAADAAADAADbqjLAk4EAAAAbAZ8LdEf/AAAn3l9oqaVT3Uj54Jr9o0cBczDbAAAAGwGfDWpH/wAABa0zTJN/24BWuDMx8S0dKciDuAAAAEhBmxJJqEFsmUwIZ//+nhAAAAsPp6qptiGfZuQFKQWBVxfuSM0FezzPp4KKYjOdiw1wbj0+MBhzBFA1vYOlL4HGSqD4oS+jCfAAAAAtQZ8wRRUsI/8AAAMDi0O9OhXfDOjZ3Dz3e1dxlKd/dsGAAAADAAADAMx5wgP9AAAADwGfT3RH/wAABYkPvOAHhAAAAB0Bn1FqR/8AAAWvMTSn3Lz/8eXtz4UCuzaccvHwQAAAAB9Bm1ZJqEFsmUwIZ//+nhAAAAMAFPysuQBW2Zg1mJOBAAAAIUGfdEUVLCP/AAADAADdbtng5Hm+nkfMAAADACU61KoLaQAAADcBn5N0R/8AAAWub40P3C19dxjqht98ABLFRgrRXY2CiyjC85s5nJOTVyfwpWsVX0nUA305fg1pAAAADgGflWpH/wAAAwAAAwGpAAAAZEGbmkmoQWyZTAhn//6eEAAACx+z7WOzgOhgW4cYltwyAE0B7oZSQiHlDaLsxZuno7uvZXy0B6OHYuUMi3UzJv+zaG2yYujyFqiEV5/SaALJd33CYPHiORFOo0U9ffouwCRsUMYAAAAsQZ+4RRUsI/8AAAkplujoUH91eK68GZ2B/9K3N5+QBByo3gT9sAAGJmmWBs0AAAAaAZ/XdEf/AAAFr8vtFTSqep95KnJKG7PaKxYAAAASAZ/Zakf/AAADAEqVnH5i0A64AAAATkGb3kmoQWyZTAhn//6eEAAAHG/CKAAaMaiSgJeG9+ZTg2K1hWAf39RZd4/4vS+im2w/nLsEIyClz9NFYBV9w/Je/Hu4l2aAJX0ZE/ilsQAAAClBn/xFFSwj/wAACSwKHkXkCdM8I9kJI8ZabRTu2FOLJ5ygAAD+2lUBswAAAB4Bnht0R/8AAAMCDDRxwDTPNEG5Hf8KQCpVS+sYbYEAAAAOAZ4dakf/AAADAAADAakAAABTQZoCSahBbJlMCGf//p4QAABNRQg0ArgVbF6jX7hh+v7bv0h3XjwJUEQjSZU0sP0WQAwXwz9OJhtDonFrJhoEHl7DNM114MtvPpzq+hrmdiX5JfgAAAAvQZ4gRRUsI/8AABkpg823Fec+SOuqfX9eCvFKB60oykZvN0GgAAADAAAke0qgNmEAAAAiAZ5fdEf/AAAn0ZFmtEKxIy2kIojLSqKqPmAoORGHSTbRpwAAABYBnkFqR/8AACfJgHDcoNcXHxWlEDxhAAAATUGaRkmoQWyZTAhn//6eEAAACwyqAQmZbPi/wABcgFhIkh8VMLQyNunXhabJlzqN3HkNhVJ5brJOO/QnvbgIlv1Bm84P/vRy/Y5anm+nAAAAPUGeZEUVLCP/AAADAVlVhQ1ZymcP8zfTxqaBACY3BN22YPxJZfabQztPXtxyWtqoAAADAAADAACpZTLA2YAAAAAWAZ6DdEf/AAAFr9DBzraNw1dvPCol3QAAAB0BnoVqR/8AAAWwge8Xqh73cX6y/qAnOAO0mRhy4QAAAFtBmopJqEFsmUwIZ//+nhAAAB0ObRiSSgAWZxH+5Y0JmDnktXcwObGdswKiwC6fCZqU1QZM0lIp9RfgXnpaWMzpyMso7gPL3nEnKT7vBnIe6EvHfpR3FjjkPHuQAAAALkGeqEUVLCP/AAADA4sPkWdFc53EMCb0EIsFg6FWsnR4AAADAAADAAC85TLA2YEAAAAOAZ7HdEf/AAADAAADAakAAAAdAZ7Jakf/AAAFnhG2X0xd4JAm4gVZ0gYjKLK6OsEAAABYQZrOSahBbJlMCGf//p4QAAALDwV8VHYMPXE6+wKdY8NGAE1b83+4lF79KJq+UOnEtrhURgPqoPmP/rLRviKAudLjMEnFpuddKPhU31MPKgzhV5+KjinrYQAAAC5BnuxFFSwj/wAAAwOLD5HB9lfGV8GQun9y/89UPyJevZgAAAMAAAMAAbaaZYGzAAAADgGfC3RH/wAAAwAAAwGpAAAAHAGfDWpH/wAABa8wl0AGeJmvsgxr+gOs0TPBYsAAAAA5QZsSSahBbJlMCF///oywAABOAs8LYK9loGCZwACIIydX60T/rEqGGQxNAZ53TMh0b0BurYa4YFNAAAAAPEGfMEUVLCP/AAAZImsp4kY4OVAn2puZgZspSAFp2fRm+yoV7asOFTJ+WZQEzmaIAAADAAADAELhPCBdwQAAABwBn090R/8AACfeX2ZcalT9xqx1ha/Emv7bBHWAAAAAHgGfUWpH/wAABZ4Rtl8nf3wY1Ne5llzWvjKwb+osWAAAAEZBm1VJqEFsmUwIZ//+nhAAABz0D07z3tTLtny4T4GGABWpaSsof2eABFtiRgsA0nSiWMgc041ybfV6LA5Bwvw7AAcjOGBZAAAAKkGfc0UVLCP/AAADA4sPkWdFcysmUlF4vwhiSb5RX1AAAAMAAAV3KZYGzQAAACQBn5RqR/8AAAWJPmC8kvbgoH2TnAlk/oO2J1tqOinciw7FdqkAAABDQZuZSahBbJlMCGf//p4QAABNRQg0AcUouyM0ia/ueIAIWV4EvuXlIuKgFik7/Eoqxf7UhCSvKrbI3MN50Q6raDyggAAAAEhBn7dFFSwj/wAAGSJrN2Kf1s6gButYl9GbOVI2O3Rcnr5bhKaZf2y8xDtQvjKamHq1pgh5FJ/bugAK1aTYGHCAAAAw80ywNmAAAAAdAZ/WdEf/AAAnwpNNhCnLzAlVV92yfuuMzFTFA2kAAAAaAZ/Yakf/AAAFrTAOG5Qa7ZDyLt+akyM6rFgAAABoQZvdSahBbJlMCGf//p4QAABNVRBEsckn8VHoAPhAxRWNRQJbjHQmOy7qdmarxfvXE9eK0C9naLAzQcuUc33ApRnthlZdpKewVVhBA4tM8zM2IrOWwp9/Ml4AzhCGCSgtG671nB5812gAAAAsQZ/7RRUsI/8AABkpg823FcyyMBde88u1lRsJDCaBOZSkAAADAAAE4uVQEnEAAAAQAZ4adEf/AAADAEtYeKCpgAAAACoBnhxqR/8AACfJgHDc7RR4uZqVKYEYtpZgRnOBLdmLvl4Qjsz27JbhWy8AAAAfQZoBSahBbJlMCGf//p4QAAADAAfAfbgAEYbsxRZmLQAAACFBnj9FFSwj/wAAAwAA3W7Z4OR5vp5HzAAAAwAlOtSqC2gAAAAOAZ5edEf/AAADAAADAakAAAAOAZ5Aakf/AAADAAADAakAAAAXQZpFSahBbJlMCGf//p4QAAADAAADAz8AAAA/QZ5jRRUsI/8AABkga7UE8GlXk8cL8SfACWMOULpjHj1suoSr1ZVxSMMKQfcbRf98jN/9IhqyAAADARLaVQGzAAAAHAGegnRH/wAAJ8KTTYQpzDZhgSZWldhldAUWUGAAAAAbAZ6Eakf/AAAnyYBw3KDgRbMkZ2X+PMYAv02gAAAAF0GaiUmoQWyZTAhn//6eEAAAAwAAAwM/AAAAK0Gep0UVLCP/AAAZLds/Yf2GQemXL0Ut/Nj+DiYswPNoAAADAAAJhcqgJOAAAAAcAZ7GdEf/AAAnwpNNhCnMNmGBJlaV2GV0BRZQYQAAABsBnshqR/8AACfJgHDcoOBFsyRnZf48xgC/TaAAAAB1QZrNSahBbJlMCGf//p4QAABNUiXAEZNF2J8JzbyB2Jjt0PSEcrpxKf5VlRRCMyGIRTyV805smcaLOgRBh3O9sVcB6A+g92yzibg2Zsn/uOAFmGXd/cjgj9MoGf1IME4+qiu2F/eNOsx0hVmIQ1Uki7n0zijpAAAAQEGe60UVLCP/AAAZKYPMu0ShTJ4ZthAC2Wtok4Dd4KLnzgMiHf5XUr6hzXwYPETN4ZFoVgAAAwAAAwABJITwgXcAAAAgAZ8KdEf/AAAFrFJpdBq9FaVhmQQoiyIckzGf4bebdi0AAAAsAZ8Makf/AAAnyYBw3DHnrK5Ow5wBP+cTicIIzZhAFNeEx42/FMSUPlBoyxcAAAAXQZsRSahBbJlMCGf//p4QAAADAAADAz4AAAAhQZ8vRRUsI/8AAAMAAN1u2eDkeb6eR8wAAAMAJTrUqgtoAAAADgGfTnRH/wAAAwAAAwGpAAAADgGfUGpH/wAAAwAAAwGpAAAAV0GbVUmoQWyZTAhn//6eEAAACx+z4My4w6GBbiFsWZpqEJMaL5yv3RbimAUv1I68AK54fyv5c5LCquHlsvjBO5PfQHHJGf5J6ryXzOsmfS6erdi99Y6/wAAAACxBn3NFFSwj/wAAAwOJ/Sbta0vTuRzQ89l4p2Xyd8IFMktAqsAAAF+mmWBswQAAABsBn5J0R/8AAAWsUmmwhTl5FIE01NGWmYIuOsEAAAAQAZ+Uakf/AAADAE1+N7BmQQAAAIFBm5lJqEFsmUwIZ//+nhAAAE1FKaAAnMLw0nI39dHXppD6HbKBmzURu3zMtpx0kD2fe0DlOQQbuT5kLb88vHn5R+Y4Pz1ZduSpTKOr1XayeZ0z7qxbQdpCrJ1Lws9buIihKxS7Ut1cMq3kr45OMvhoC5DL6uyKShEb6SOGR1f/iOAAAAAvQZ+3RRUsI/8AABkiayniRiwesPYCexFYQsfBlY0MWP9rijX0zr9gAAAsJDwgXcAAAAAcAZ/WdEf/AAAnwpNNhCnLyKQIEZ65GgPdC/MrFwAAAA4Bn9hqR/8AAAMAAAMBqQAAAB9Bm91JqEFsmUwIZ//+nhAAABz0DznGpAEEiyFXAAyoAAAAI0Gf+0UVLCP/AAAJbAuKifNYjj+ITfQXOcAAAAMAAiNUywcdAAAADwGeGnRH/wAABaxb9gCvgAAAAA4BnhxqR/8AAAMAAAMBqQAAABdBmgFJqEFsmUwIZ//+nhAAAAMAAAMDPwAAACFBnj9FFSwj/wAAAwAA3W7Z4OR5vp5HzAAAAwAlOtSqC2gAAAAOAZ5edEf/AAADAAADAakAAAAOAZ5Aakf/AAADAAADAakAAAAXQZpFSahBbJlMCGf//p4QAAADAAADAz8AAAA3QZ5jRRUsI/8AAAMDgACBBHhVZNYisQxU1e3ZrlECdTKGf+ww/ud6feWoCmGBteAAAAj0J4QLuQAAABoBnoJ0R/8AAAWsW3mST3cpJIfH3vXrGBnYsAAAABkBnoRqR/8AAAMCGyKu6puCrWFla7Y3Qe2XAAAAF0GaiUmoQWyZTAhf//6MsAAAAwAAAwNDAAAAK0Gep0UVLCP/AAADA4u458IDcF6SKbRhHQ69vqUrBigAAAMAAAMDjwSqA2YAAAAaAZ7GdEf/AAAFrFt5kk93KSSHx9716xgZ2LEAAAAZAZ7Iakf/AAADAhsiruqbgq1hZWu2N0HtlwAAACtBms1JqEFsmUwIX//+jLAAAAsn2Z7qCnKIOhZpMxM4B6ATdQBDaQFGXpjtAAAAKUGe60UVLCP/AAADA4oDQlphfnRLYC6n65o4rIJFC57zscAAAbaaZYGzAAAAGgGfCnRH/wAABaxbeZJPdykkh8fe9esYGdixAAAAHQGfDGpH/wAAJ8Udzux6hi5uEyIQOv7CBLuQLsM/AAAAF0GbEUmoQWyZTAhf//6MsAAAAwAAAwNCAAAAIUGfL0UVLCP/AAADAADdbtng5Hm+nkfMAAADACU61KoLaAAAAA4Bn050R/8AAAMAAAMBqQAAAA4Bn1BqR/8AAAMAAAMBqQAAADpBm1VJqEFsmUwIV//+OEAAAA/Ut+CuHBgBKecgv3xQ6y+q0hOxwDwLydrCK6+5iNTcoDNR9Sm+fc5sAAAAK0Gfc0UVLCP/AAADAVnE5rwv+3VrYb4QRY8f+nppcAAAAwAAAwA8UEqgNmEAAAAOAZ+SdEf/AAADAAADAakAAAAZAZ+Uakf/AAAFq2Hq8mACfZl8Lt9niTDnLwAAABdBm5lJqEFsmUwIR//94QAAAwAAAwAxYAAAACtBn7dFFSwj/wAAAwFaCfZHgT44Yrb1roBQONpOeRoAAAMAAAMALzlMsDZgAAAAGgGf1nRH/wAAAwIaw4/vSROYZm3YmfUW2c2LAAAAGQGf2GpH/wAABath6vJgAn2ZfC7fZ4kw5y4AAAExZYiEACv//vZzfAprRzOVLgV292aj5dCS5fsQYPrQAAADAAADAABNxUTOiwpjxNkAAAMAXEAR4LAI8JULYSEfY8Bk4ExsQBEk/v/nklhxnG6bgEcT/BHfM+IJ1Z7eB9lrEFfDumQl7iBWC8UqjILcAB2rEm9Jx+kACbCA6OyeNMoYacW/jCyCPbZ/hAeGpa6zOKAbDGAPOdpI7LeHzagMf1dWJJbcAAAvh8RfQAdHXlA0tlQf5YkIRIvAQC3kPgIfabVo3/0io4hb+9TCI4wx8FFiH+yJOFdkKla7vM97lBcv3HVgS995EfD56TMZdVJAHX32zJGZO14hBrGS6j6aqIgAHW2142hRCar2vNsEFC118CuGhcv/DyRO7RGM/5K0BblIvOMcQAAAAwAAAwAANeAAABpvbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAJyQAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAGZl0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAJyQAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAlgAAAGQAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAACckAAACAAABAAAAABkRbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAAB9QBVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAYvG1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAGHxzdGJsAAAAmHN0c2QAAAAAAAAAAQAAAIhhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAlgBkABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAMmF2Y0MBZAAf/+EAGWdkAB+s2UCYM+XhAAADAAEAAAMAZA8YMZYBAAZo6+PLIsAAAAAYc3R0cwAAAAAAAAABAAAB9QAAAQAAAAAcc3RzcwAAAAAAAAADAAAAAQAAAPsAAAH1AAAPkGN0dHMAAAAAAAAB8AAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAACAAAAAAEAAAMAAAAAAQAAAQAAAAABAAACAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAH1AAAAAQAAB+hzdHN6AAAAAAAAAAAAAAH1AAAEugAAAJgAAAAkAAAAKAAAABYAAADlAAAAVgAAADYAAAA8AAAAzwAAAEgAAACrAAAAPAAAAD4AAAAfAAAAaAAAADIAAAA+AAAAowAAAFAAAAA+AAAANgAAAK8AAABUAAAAQQAAADYAAACfAAAAUwAAAEsAAAA8AAAAfAAAAD4AAAAxAAAAyAAAAEUAAAAyAAAALwAAAJEAAABHAAAAKwAAADEAAACqAAAATQAAACYAAAAqAAAAgQAAAC4AAAA+AAAAOAAAAGIAAAAwAAAAJwAAACYAAABCAAAAdgAAADsAAAA3AAAAHwAAAG8AAAB7AAAAJgAAAGMAAABrAAAAHQAAAFcAAABKAAAALQAAACIAAABnAAAAMgAAABsAAAAlAAAAZwAAACgAAAAYAAAAMQAAAE8AAAAqAAAAMwAAABgAAAA7AAAAJAAAAEAAAAAWAAAAcAAAADkAAAA3AAAAHgAAAEsAAAAmAAAAEgAAACMAAABiAAAALAAAACIAAAAbAAAAYAAAACwAAAAiAAAAHwAAAGwAAAAmAAAAMAAAABQAAACSAAAAPwAAACAAAAAlAAAAGwAAAEcAAAAcAAAAGwAAAGQAAAA6AAAAKwAAAB0AAABhAAAAJAAAADcAAAAUAAAASwAAADQAAAAeAAAAJgAAAFEAAAAmAAAAMQAAABUAAABJAAAATwAAACAAAAAcAAAAqAAAADQAAAAxAAAAKgAAACMAAABTAAAAJQAAAIQAAABLAAAAIAAAAB8AAAAbAAAAFgAAAD8AAAASAAAATQAAACMAAAAfAAAAEgAAAGEAAAAiAAAAHgAAABIAAAAjAAAAGgAAABQAAAASAAAAJAAAABgAAAASAAAAEgAAABsAAAAYAAAAEgAAABIAAAA8AAAAPAAAACIAAAAgAAAAYgAAACoAAAAhAAAAHwAAAD8AAAAnAAAAHwAAAB4AAAApAAAANAAAACAAAAAhAAAARgAAACYAAAAjAAAAGgAAABsAAAAhAAAAGwAAABsAAAAbAAAALAAAAB0AAAAfAAAAGwAAADAAAAAbAAAAHAAAABsAAAAsAAAAHQAAAB8AAAA+AAAAJgAAABsAAAAVAAAAYAAAADQAAAAlAAAAJgAAAD0AAAAsAAAAIAAAABoAAABIAAAAJgAAABIAAAAhAAAAMgAAAB8AAAASAAAAGwAAABsAAAAhAAAAGwAAABsAAABAAAAAJwAAAB0AAAAiAAAAVgAAACoAAAAxAAAAEgAAAEsAAAAkAAAAHwAAABQAAABOAAAAIwAAAB8AAAASAAAASAAAACQAAAAeAAAAEgAAAEUAAAAhAAAAGwAAAhoAAAB9AAAAMwAAACAAAAASAAAALgAAACcAAAASAAAAEgAAABsAAABOAAAAIAAAACIAAAAmAAAAJQAAABQAAAASAAAAGwAAAE4AAAAgAAAAIgAAAEQAAAA1AAAAIAAAABwAAABtAAAAOQAAABgAAAAwAAAAVgAAADEAAAAfAAAAEgAAAF0AAAAxAAAAIQAAABIAAAAbAAAAPgAAACAAAAAfAAAAUQAAAEMAAAAcAAAAIwAAAEEAAAAvAAAAEgAAAB8AAABeAAAAMQAAABQAAAAhAAAAYAAAADAAAAATAAAAIAAAAHUAAAAzAAAAHwAAACIAAABEAAAALQAAABsAAAAUAAAALwAAAC4AAAAUAAAAHAAAAGMAAAAxAAAAGwAAAB8AAAAiAAAAMwAAACEAAAAhAAAARAAAADQAAAAiAAAAHgAAACAAAAAxAAAAHgAAAB8AAABiAAAAQgAAABoAAAAuAAAASQAAADAAAAAgAAAAFAAAABsAAABJAAAAHwAAACAAAABWAAAALgAAAB8AAAAUAAAASgAAAC4AAAAfAAAAEwAAAFAAAAAwAAAALgAAABQAAAA/AAAAMgAAACAAAAAWAAAAeAAAAC8AAAAiAAAAEgAAAGMAAAAwAAAAMQAAABUAAAB+AAAAPwAAABUAAAAuAAAANgAAAD4AAAAuAAAAFAAAAFIAAAAuAAAATgAAADAAAAAeAAAAEgAAADoAAAAuAAAAHgAAABIAAACAAAAAQQAAAB8AAAAfAAAATAAAADEAAAATAAAAIQAAACMAAAAlAAAAOwAAABIAAABoAAAAMAAAAB4AAAAWAAAAUgAAAC0AAAAiAAAAEgAAAFcAAAAzAAAAJgAAABoAAABRAAAAQQAAABoAAAAhAAAAXwAAADIAAAASAAAAIQAAAFwAAAAyAAAAEgAAACAAAAA9AAAAQAAAACAAAAAiAAAASgAAAC4AAAAoAAAARwAAAEwAAAAhAAAAHgAAAGwAAAAwAAAAFAAAAC4AAAAjAAAAJQAAABIAAAASAAAAGwAAAEMAAAAgAAAAHwAAABsAAAAvAAAAIAAAAB8AAAB5AAAARAAAACQAAAAwAAAAGwAAACUAAAASAAAAEgAAAFsAAAAwAAAAHwAAABQAAACFAAAAMwAAACAAAAASAAAAIwAAACcAAAATAAAAEgAAABsAAAAlAAAAEgAAABIAAAAbAAAAOwAAAB4AAAAdAAAAGwAAAC8AAAAeAAAAHQAAAC8AAAAtAAAAHgAAACEAAAAbAAAAJQAAABIAAAASAAAAPgAAAC8AAAASAAAAHQAAABsAAAAvAAAAHgAAAB0AAAE1AAAAFHN0Y28AAAAAAAAAAQAAADAAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjI5LjEwMA==\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "observation = env.reset()\n",
        "while True:\n",
        "    env.render()\n",
        "    #your agent goes here\n",
        "    action = agent.forward(observation)\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    if done: \n",
        "      break;\n",
        "    if reward==500: # break condition added to prevent the video from not working if score==500\n",
        "      break;    \n",
        "env.close()\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
